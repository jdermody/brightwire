//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-37061995
// Cuda compilation tools, release 13.1, V13.1.115
// Based on NVVM 21.0.0
//

.version 9.1
.target sm_103
.address_size 64

	// .globl	IsFinite
// _ZZ13FindMinAndMaxE5block has been demoted
// _ZZ10FindStdDevE5block has been demoted
// _ZZ9SumValuesE5block has been demoted

.visible .entry IsFinite(
	.param .u64 .ptr .align 1 IsFinite_param_0,
	.param .u64 .ptr .align 1 IsFinite_param_1,
	.param .u32 IsFinite_param_2,
	.param .u32 IsFinite_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [IsFinite_param_0];
	ld.param.b64 	%rd4, [IsFinite_param_1];
	ld.param.b32 	%r6, [IsFinite_param_2];
	ld.param.b32 	%r7, [IsFinite_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r15, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r15, %r6;
	@%p1 bra 	$L__BB0_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB0_2:
	mul.lo.s32 	%r11, %r15, %r7;
	mul.wide.u32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r12, [%rd6];
	abs.ftz.f32 	%r13, %r12;
	setp.equ.ftz.f32 	%p2, %r13, 0f7F800000;
	selp.f32 	%r14, 0f3F800000, 0f00000000, %p2;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r15, %r15, %r3;
	setp.lt.u32 	%p3, %r15, %r6;
	@%p3 bra 	$L__BB0_2;
$L__BB0_3:
	ret;

}
	// .globl	Scale
.visible .entry Scale(
	.param .u64 .ptr .align 1 Scale_param_0,
	.param .u32 Scale_param_1,
	.param .f32 Scale_param_2,
	.param .u32 Scale_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [Scale_param_0];
	ld.param.b32 	%r6, [Scale_param_1];
	ld.param.b32 	%r7, [Scale_param_2];
	ld.param.b32 	%r8, [Scale_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r15, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r15, %r6;
	@%p1 bra 	$L__BB1_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB1_2:
	mul.lo.s32 	%r12, %r15, %r8;
	mul.wide.u32 	%rd3, %r12, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r13, [%rd4];
	mul.ftz.f32 	%r14, %r7, %r13;
	st.global.b32 	[%rd4], %r14;
	add.s32 	%r15, %r15, %r3;
	setp.lt.u32 	%p2, %r15, %r6;
	@%p2 bra 	$L__BB1_2;
$L__BB1_3:
	ret;

}
	// .globl	PointwiseMultiply
.visible .entry PointwiseMultiply(
	.param .u64 .ptr .align 1 PointwiseMultiply_param_0,
	.param .u64 .ptr .align 1 PointwiseMultiply_param_1,
	.param .u32 PointwiseMultiply_param_2,
	.param .u32 PointwiseMultiply_param_3,
	.param .u32 PointwiseMultiply_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [PointwiseMultiply_param_0];
	ld.param.b64 	%rd4, [PointwiseMultiply_param_1];
	ld.param.b32 	%r6, [PointwiseMultiply_param_2];
	ld.param.b32 	%r7, [PointwiseMultiply_param_3];
	ld.param.b32 	%r8, [PointwiseMultiply_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB2_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB2_2:
	mul.lo.s32 	%r12, %r17, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	mul.lo.s32 	%r14, %r17, %r8;
	mul.wide.u32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r15, [%rd8];
	mul.ftz.f32 	%r16, %r13, %r15;
	st.global.b32 	[%rd8], %r16;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB2_2;
$L__BB2_3:
	ret;

}
	// .globl	PointwiseDivide
.visible .entry PointwiseDivide(
	.param .u64 .ptr .align 1 PointwiseDivide_param_0,
	.param .u64 .ptr .align 1 PointwiseDivide_param_1,
	.param .u32 PointwiseDivide_param_2,
	.param .u32 PointwiseDivide_param_3,
	.param .u32 PointwiseDivide_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [PointwiseDivide_param_0];
	ld.param.b64 	%rd4, [PointwiseDivide_param_1];
	ld.param.b32 	%r6, [PointwiseDivide_param_2];
	ld.param.b32 	%r7, [PointwiseDivide_param_3];
	ld.param.b32 	%r8, [PointwiseDivide_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB3_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB3_2:
	mul.lo.s32 	%r12, %r17, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	mul.lo.s32 	%r14, %r17, %r8;
	mul.wide.u32 	%rd7, %r14, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r15, [%rd8];
	div.approx.ftz.f32 	%r16, %r13, %r15;
	st.global.b32 	[%rd8], %r16;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB3_2;
$L__BB3_3:
	ret;

}
	// .globl	Sqrt
.visible .entry Sqrt(
	.param .u64 .ptr .align 1 Sqrt_param_0,
	.param .u64 .ptr .align 1 Sqrt_param_1,
	.param .u32 Sqrt_param_2,
	.param .f32 Sqrt_param_3,
	.param .u32 Sqrt_param_4,
	.param .u32 Sqrt_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Sqrt_param_0];
	ld.param.b64 	%rd4, [Sqrt_param_1];
	ld.param.b32 	%r6, [Sqrt_param_2];
	ld.param.b32 	%r7, [Sqrt_param_3];
	ld.param.b32 	%r8, [Sqrt_param_4];
	ld.param.b32 	%r9, [Sqrt_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r18, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r18, %r6;
	@%p1 bra 	$L__BB4_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB4_2:
	mul.lo.s32 	%r13, %r18, %r8;
	mul.wide.u32 	%rd5, %r13, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r14, [%rd6];
	add.ftz.f32 	%r15, %r7, %r14;
	sqrt.approx.ftz.f32 	%r16, %r15;
	mul.lo.s32 	%r17, %r18, %r9;
	mul.wide.u32 	%rd7, %r17, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r16;
	add.s32 	%r18, %r18, %r3;
	setp.lt.u32 	%p2, %r18, %r6;
	@%p2 bra 	$L__BB4_2;
$L__BB4_3:
	ret;

}
	// .globl	AddInPlace
.visible .entry AddInPlace(
	.param .u64 .ptr .align 1 AddInPlace_param_0,
	.param .u64 .ptr .align 1 AddInPlace_param_1,
	.param .u32 AddInPlace_param_2,
	.param .f32 AddInPlace_param_3,
	.param .f32 AddInPlace_param_4,
	.param .u32 AddInPlace_param_5,
	.param .u32 AddInPlace_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [AddInPlace_param_0];
	ld.param.b64 	%rd4, [AddInPlace_param_1];
	ld.param.b32 	%r6, [AddInPlace_param_2];
	ld.param.b32 	%r7, [AddInPlace_param_3];
	ld.param.b32 	%r8, [AddInPlace_param_4];
	ld.param.b32 	%r9, [AddInPlace_param_5];
	ld.param.b32 	%r10, [AddInPlace_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r20, %r1, %r11, %r12;
	setp.ge.u32 	%p1, %r20, %r6;
	@%p1 bra 	$L__BB5_3;
	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB5_2:
	mul.lo.s32 	%r14, %r20, %r9;
	mul.wide.u32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.b32 	%r15, [%rd6];
	mul.lo.s32 	%r16, %r20, %r10;
	mul.wide.u32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.b32 	%r17, [%rd8];
	mul.ftz.f32 	%r18, %r8, %r17;
	fma.rn.ftz.f32 	%r19, %r7, %r15, %r18;
	st.global.b32 	[%rd6], %r19;
	add.s32 	%r20, %r20, %r3;
	setp.lt.u32 	%p2, %r20, %r6;
	@%p2 bra 	$L__BB5_2;
$L__BB5_3:
	ret;

}
	// .globl	SubtractInPlace
.visible .entry SubtractInPlace(
	.param .u64 .ptr .align 1 SubtractInPlace_param_0,
	.param .u64 .ptr .align 1 SubtractInPlace_param_1,
	.param .u32 SubtractInPlace_param_2,
	.param .f32 SubtractInPlace_param_3,
	.param .f32 SubtractInPlace_param_4,
	.param .u32 SubtractInPlace_param_5,
	.param .u32 SubtractInPlace_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [SubtractInPlace_param_0];
	ld.param.b64 	%rd4, [SubtractInPlace_param_1];
	ld.param.b32 	%r6, [SubtractInPlace_param_2];
	ld.param.b32 	%r7, [SubtractInPlace_param_3];
	ld.param.b32 	%r8, [SubtractInPlace_param_4];
	ld.param.b32 	%r9, [SubtractInPlace_param_5];
	ld.param.b32 	%r10, [SubtractInPlace_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r21, %r1, %r11, %r12;
	setp.ge.u32 	%p1, %r21, %r6;
	@%p1 bra 	$L__BB6_3;
	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB6_2:
	mul.lo.s32 	%r14, %r21, %r9;
	mul.wide.u32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.b32 	%r15, [%rd6];
	mul.ftz.f32 	%r16, %r7, %r15;
	mul.lo.s32 	%r17, %r21, %r10;
	mul.wide.u32 	%rd7, %r17, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.b32 	%r18, [%rd8];
	mul.ftz.f32 	%r19, %r8, %r18;
	sub.ftz.f32 	%r20, %r16, %r19;
	st.global.b32 	[%rd6], %r20;
	add.s32 	%r21, %r21, %r3;
	setp.lt.u32 	%p2, %r21, %r6;
	@%p2 bra 	$L__BB6_2;
$L__BB6_3:
	ret;

}
	// .globl	AddToEachRow
.visible .entry AddToEachRow(
	.param .u64 .ptr .align 1 AddToEachRow_param_0,
	.param .u64 .ptr .align 1 AddToEachRow_param_1,
	.param .u32 AddToEachRow_param_2,
	.param .u32 AddToEachRow_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [AddToEachRow_param_0];
	ld.param.b64 	%rd4, [AddToEachRow_param_1];
	ld.param.b32 	%r10, [AddToEachRow_param_2];
	ld.param.b32 	%r11, [AddToEachRow_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r23, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r23, %r10;
	@%p1 bra 	$L__BB7_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB7_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB7_5;
	mov.b32 	%r24, %r3;
$L__BB7_4:
	mul.wide.u32 	%rd5, %r24, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r19, [%rd6];
	mad.lo.s32 	%r20, %r24, %r10, %r23;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r21, [%rd8];
	add.ftz.f32 	%r22, %r19, %r21;
	st.global.b32 	[%rd8], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r11;
	@%p3 bra 	$L__BB7_4;
$L__BB7_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r10;
	@%p4 bra 	$L__BB7_2;
$L__BB7_6:
	ret;

}
	// .globl	AddToEachColumn
.visible .entry AddToEachColumn(
	.param .u64 .ptr .align 1 AddToEachColumn_param_0,
	.param .u64 .ptr .align 1 AddToEachColumn_param_1,
	.param .u32 AddToEachColumn_param_2,
	.param .u32 AddToEachColumn_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [AddToEachColumn_param_0];
	ld.param.b64 	%rd4, [AddToEachColumn_param_1];
	ld.param.b32 	%r11, [AddToEachColumn_param_2];
	ld.param.b32 	%r12, [AddToEachColumn_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r23, %r1, %r13, %r14;
	setp.ge.u32 	%p1, %r23, %r11;
	@%p1 bra 	$L__BB8_6;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r15, %r16, %r17;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r4, %r15, %r18;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r19;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB8_2:
	setp.ge.u32 	%p2, %r3, %r12;
	@%p2 bra 	$L__BB8_5;
	mul.wide.u32 	%rd5, %r23, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	mov.b32 	%r24, %r3;
$L__BB8_4:
	mad.lo.s32 	%r20, %r24, %r11, %r23;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r21, [%rd8];
	add.ftz.f32 	%r22, %r7, %r21;
	st.global.b32 	[%rd8], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r12;
	@%p3 bra 	$L__BB8_4;
$L__BB8_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r11;
	@%p4 bra 	$L__BB8_2;
$L__BB8_6:
	ret;

}
	// .globl	MultiplyByEachRow
.visible .entry MultiplyByEachRow(
	.param .u64 .ptr .align 1 MultiplyByEachRow_param_0,
	.param .u64 .ptr .align 1 MultiplyByEachRow_param_1,
	.param .u32 MultiplyByEachRow_param_2,
	.param .u32 MultiplyByEachRow_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [MultiplyByEachRow_param_0];
	ld.param.b64 	%rd4, [MultiplyByEachRow_param_1];
	ld.param.b32 	%r10, [MultiplyByEachRow_param_2];
	ld.param.b32 	%r11, [MultiplyByEachRow_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r23, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r23, %r10;
	@%p1 bra 	$L__BB9_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB9_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB9_5;
	mov.b32 	%r24, %r3;
$L__BB9_4:
	mul.wide.u32 	%rd5, %r24, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r19, [%rd6];
	mad.lo.s32 	%r20, %r24, %r10, %r23;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r21, [%rd8];
	mul.ftz.f32 	%r22, %r19, %r21;
	st.global.b32 	[%rd8], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r11;
	@%p3 bra 	$L__BB9_4;
$L__BB9_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r10;
	@%p4 bra 	$L__BB9_2;
$L__BB9_6:
	ret;

}
	// .globl	MultiplyByEachColumn
.visible .entry MultiplyByEachColumn(
	.param .u64 .ptr .align 1 MultiplyByEachColumn_param_0,
	.param .u64 .ptr .align 1 MultiplyByEachColumn_param_1,
	.param .u32 MultiplyByEachColumn_param_2,
	.param .u32 MultiplyByEachColumn_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [MultiplyByEachColumn_param_0];
	ld.param.b64 	%rd4, [MultiplyByEachColumn_param_1];
	ld.param.b32 	%r11, [MultiplyByEachColumn_param_2];
	ld.param.b32 	%r12, [MultiplyByEachColumn_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r23, %r1, %r13, %r14;
	setp.ge.u32 	%p1, %r23, %r11;
	@%p1 bra 	$L__BB10_6;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r15, %r16, %r17;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r4, %r15, %r18;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r19;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB10_2:
	setp.ge.u32 	%p2, %r3, %r12;
	@%p2 bra 	$L__BB10_5;
	mul.wide.u32 	%rd5, %r23, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	mov.b32 	%r24, %r3;
$L__BB10_4:
	mad.lo.s32 	%r20, %r24, %r11, %r23;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.b32 	%r21, [%rd8];
	mul.ftz.f32 	%r22, %r7, %r21;
	st.global.b32 	[%rd8], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r12;
	@%p3 bra 	$L__BB10_4;
$L__BB10_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r11;
	@%p4 bra 	$L__BB10_2;
$L__BB10_6:
	ret;

}
	// .globl	TanH
.visible .entry TanH(
	.param .u64 .ptr .align 1 TanH_param_0,
	.param .u64 .ptr .align 1 TanH_param_1,
	.param .u32 TanH_param_2,
	.param .u32 TanH_param_3,
	.param .u32 TanH_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [TanH_param_0];
	ld.param.b64 	%rd4, [TanH_param_1];
	ld.param.b32 	%r6, [TanH_param_2];
	ld.param.b32 	%r7, [TanH_param_3];
	ld.param.b32 	%r8, [TanH_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r16, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB11_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB11_2:
	mul.lo.s32 	%r12, %r16, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	tanh.approx.f32 	%r14, %r13;
	mul.lo.s32 	%r15, %r16, %r8;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB11_2;
$L__BB11_3:
	ret;

}
	// .globl	TanHDerivative
.visible .entry TanHDerivative(
	.param .u64 .ptr .align 1 TanHDerivative_param_0,
	.param .u64 .ptr .align 1 TanHDerivative_param_1,
	.param .u32 TanHDerivative_param_2,
	.param .u32 TanHDerivative_param_3,
	.param .u32 TanHDerivative_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [TanHDerivative_param_0];
	ld.param.b64 	%rd4, [TanHDerivative_param_1];
	ld.param.b32 	%r6, [TanHDerivative_param_2];
	ld.param.b32 	%r7, [TanHDerivative_param_3];
	ld.param.b32 	%r8, [TanHDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r19, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r19, %r6;
	@%p1 bra 	$L__BB12_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB12_2:
	mul.lo.s32 	%r12, %r19, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	tanh.approx.f32 	%r14, %r13;
	mul.ftz.f32 	%r15, %r14, %r14;
	mov.b32 	%r16, 0f3F800000;
	sub.ftz.f32 	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r19, %r8;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r17;
	add.s32 	%r19, %r19, %r3;
	setp.lt.u32 	%p2, %r19, %r6;
	@%p2 bra 	$L__BB12_2;
$L__BB12_3:
	ret;

}
	// .globl	Sigmoid
.visible .entry Sigmoid(
	.param .u64 .ptr .align 1 Sigmoid_param_0,
	.param .u64 .ptr .align 1 Sigmoid_param_1,
	.param .u32 Sigmoid_param_2,
	.param .u32 Sigmoid_param_3,
	.param .u32 Sigmoid_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Sigmoid_param_0];
	ld.param.b64 	%rd4, [Sigmoid_param_1];
	ld.param.b32 	%r6, [Sigmoid_param_2];
	ld.param.b32 	%r7, [Sigmoid_param_3];
	ld.param.b32 	%r8, [Sigmoid_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r19, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r19, %r6;
	@%p1 bra 	$L__BB13_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB13_2:
	mul.lo.s32 	%r12, %r19, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	mul.ftz.f32 	%r14, %r13, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%r15, %r14;
	add.ftz.f32 	%r16, %r15, 0f3F800000;
	rcp.approx.ftz.f32 	%r17, %r16;
	mul.lo.s32 	%r18, %r19, %r8;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r17;
	add.s32 	%r19, %r19, %r3;
	setp.lt.u32 	%p2, %r19, %r6;
	@%p2 bra 	$L__BB13_2;
$L__BB13_3:
	ret;

}
	// .globl	SigmoidDerivative
.visible .entry SigmoidDerivative(
	.param .u64 .ptr .align 1 SigmoidDerivative_param_0,
	.param .u64 .ptr .align 1 SigmoidDerivative_param_1,
	.param .u32 SigmoidDerivative_param_2,
	.param .u32 SigmoidDerivative_param_3,
	.param .u32 SigmoidDerivative_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [SigmoidDerivative_param_0];
	ld.param.b64 	%rd4, [SigmoidDerivative_param_1];
	ld.param.b32 	%r6, [SigmoidDerivative_param_2];
	ld.param.b32 	%r7, [SigmoidDerivative_param_3];
	ld.param.b32 	%r8, [SigmoidDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r22, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r22, %r6;
	@%p1 bra 	$L__BB14_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB14_2:
	mul.lo.s32 	%r12, %r22, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	mul.ftz.f32 	%r14, %r13, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%r15, %r14;
	add.ftz.f32 	%r16, %r15, 0f3F800000;
	rcp.approx.ftz.f32 	%r17, %r16;
	mov.b32 	%r18, 0f3F800000;
	sub.ftz.f32 	%r19, %r18, %r17;
	mul.ftz.f32 	%r20, %r17, %r19;
	mul.lo.s32 	%r21, %r22, %r8;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r20;
	add.s32 	%r22, %r22, %r3;
	setp.lt.u32 	%p2, %r22, %r6;
	@%p2 bra 	$L__BB14_2;
$L__BB14_3:
	ret;

}
	// .globl	RELU
.visible .entry RELU(
	.param .u64 .ptr .align 1 RELU_param_0,
	.param .u64 .ptr .align 1 RELU_param_1,
	.param .u32 RELU_param_2,
	.param .u32 RELU_param_3,
	.param .u32 RELU_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [RELU_param_0];
	ld.param.b64 	%rd4, [RELU_param_1];
	ld.param.b32 	%r6, [RELU_param_2];
	ld.param.b32 	%r7, [RELU_param_3];
	ld.param.b32 	%r8, [RELU_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r16, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB15_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB15_2:
	mul.lo.s32 	%r12, %r16, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	setp.le.ftz.f32 	%p2, %r13, 0f00000000;
	selp.f32 	%r14, 0f00000000, %r13, %p2;
	mul.lo.s32 	%r15, %r16, %r8;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p3, %r16, %r6;
	@%p3 bra 	$L__BB15_2;
$L__BB15_3:
	ret;

}
	// .globl	RELUDerivative
.visible .entry RELUDerivative(
	.param .u64 .ptr .align 1 RELUDerivative_param_0,
	.param .u64 .ptr .align 1 RELUDerivative_param_1,
	.param .u32 RELUDerivative_param_2,
	.param .u32 RELUDerivative_param_3,
	.param .u32 RELUDerivative_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [RELUDerivative_param_0];
	ld.param.b64 	%rd4, [RELUDerivative_param_1];
	ld.param.b32 	%r6, [RELUDerivative_param_2];
	ld.param.b32 	%r7, [RELUDerivative_param_3];
	ld.param.b32 	%r8, [RELUDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r16, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB16_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB16_2:
	mul.lo.s32 	%r12, %r16, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	setp.gtu.ftz.f32 	%p2, %r13, 0f00000000;
	selp.f32 	%r14, 0f3F800000, 0f00000000, %p2;
	mul.lo.s32 	%r15, %r16, %r8;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p3, %r16, %r6;
	@%p3 bra 	$L__BB16_2;
$L__BB16_3:
	ret;

}
	// .globl	LeakyRELU
.visible .entry LeakyRELU(
	.param .u64 .ptr .align 1 LeakyRELU_param_0,
	.param .u64 .ptr .align 1 LeakyRELU_param_1,
	.param .u32 LeakyRELU_param_2,
	.param .u32 LeakyRELU_param_3,
	.param .u32 LeakyRELU_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [LeakyRELU_param_0];
	ld.param.b64 	%rd4, [LeakyRELU_param_1];
	ld.param.b32 	%r6, [LeakyRELU_param_2];
	ld.param.b32 	%r7, [LeakyRELU_param_3];
	ld.param.b32 	%r8, [LeakyRELU_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB17_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB17_2:
	mul.lo.s32 	%r12, %r17, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	setp.le.ftz.f32 	%p2, %r13, 0f00000000;
	mul.ftz.f32 	%r14, %r13, 0f3C23D70A;
	selp.f32 	%r15, %r14, %r13, %p2;
	mul.lo.s32 	%r16, %r17, %r8;
	mul.wide.u32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r15;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p3, %r17, %r6;
	@%p3 bra 	$L__BB17_2;
$L__BB17_3:
	ret;

}
	// .globl	LeakyRELUDerivative
.visible .entry LeakyRELUDerivative(
	.param .u64 .ptr .align 1 LeakyRELUDerivative_param_0,
	.param .u64 .ptr .align 1 LeakyRELUDerivative_param_1,
	.param .u32 LeakyRELUDerivative_param_2,
	.param .u32 LeakyRELUDerivative_param_3,
	.param .u32 LeakyRELUDerivative_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [LeakyRELUDerivative_param_0];
	ld.param.b64 	%rd4, [LeakyRELUDerivative_param_1];
	ld.param.b32 	%r6, [LeakyRELUDerivative_param_2];
	ld.param.b32 	%r7, [LeakyRELUDerivative_param_3];
	ld.param.b32 	%r8, [LeakyRELUDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r16, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB18_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB18_2:
	mul.lo.s32 	%r12, %r16, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	setp.le.ftz.f32 	%p2, %r13, 0f00000000;
	selp.f32 	%r14, 0f3C23D70A, 0f3F800000, %p2;
	mul.lo.s32 	%r15, %r16, %r8;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p3, %r16, %r6;
	@%p3 bra 	$L__BB18_2;
$L__BB18_3:
	ret;

}
	// .globl	Reverse
.visible .entry Reverse(
	.param .u64 .ptr .align 1 Reverse_param_0,
	.param .u64 .ptr .align 1 Reverse_param_1,
	.param .u32 Reverse_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Reverse_param_0];
	ld.param.b64 	%rd4, [Reverse_param_1];
	ld.param.b32 	%r6, [Reverse_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r13, %r1, %r7, %r8;
	setp.ge.u32 	%p1, %r13, %r6;
	@%p1 bra 	$L__BB19_3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r9;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB19_2:
	mul.wide.u32 	%rd5, %r13, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r10, [%rd6];
	not.b32 	%r11, %r13;
	add.s32 	%r12, %r6, %r11;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r10;
	add.s32 	%r13, %r13, %r3;
	setp.lt.u32 	%p2, %r13, %r6;
	@%p2 bra 	$L__BB19_2;
$L__BB19_3:
	ret;

}
	// .globl	SumRows
.visible .entry SumRows(
	.param .u64 .ptr .align 1 SumRows_param_0,
	.param .u64 .ptr .align 1 SumRows_param_1,
	.param .u32 SumRows_param_2,
	.param .u32 SumRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd4, [SumRows_param_0];
	ld.param.b64 	%rd5, [SumRows_param_1];
	ld.param.b32 	%r10, [SumRows_param_2];
	ld.param.b32 	%r11, [SumRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r22, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r22, %r10;
	@%p1 bra 	$L__BB20_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
$L__BB20_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB20_5;
	mul.wide.u32 	%rd6, %r22, 4;
	add.s64 	%rd3, %rd2, %rd6;
	mov.b32 	%r23, %r3;
$L__BB20_4:
	mad.lo.s32 	%r19, %r23, %r10, %r22;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r20, [%rd8];
	atom.global.add.f32 	%r21, [%rd3], %r20;
	add.s32 	%r23, %r23, %r4;
	setp.lt.u32 	%p3, %r23, %r11;
	@%p3 bra 	$L__BB20_4;
$L__BB20_5:
	add.s32 	%r22, %r22, %r5;
	setp.lt.u32 	%p4, %r22, %r10;
	@%p4 bra 	$L__BB20_2;
$L__BB20_6:
	ret;

}
	// .globl	SumColumns
.visible .entry SumColumns(
	.param .u64 .ptr .align 1 SumColumns_param_0,
	.param .u64 .ptr .align 1 SumColumns_param_1,
	.param .u32 SumColumns_param_2,
	.param .u32 SumColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<24>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [SumColumns_param_0];
	ld.param.b64 	%rd4, [SumColumns_param_1];
	ld.param.b32 	%r10, [SumColumns_param_2];
	ld.param.b32 	%r11, [SumColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r22, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r22, %r10;
	@%p1 bra 	$L__BB21_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB21_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB21_5;
	mov.b32 	%r23, %r3;
$L__BB21_4:
	mul.wide.u32 	%rd5, %r23, 4;
	add.s64 	%rd6, %rd2, %rd5;
	mad.lo.s32 	%r19, %r23, %r10, %r22;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r20, [%rd8];
	atom.global.add.f32 	%r21, [%rd6], %r20;
	add.s32 	%r23, %r23, %r4;
	setp.lt.u32 	%p3, %r23, %r11;
	@%p3 bra 	$L__BB21_4;
$L__BB21_5:
	add.s32 	%r22, %r22, %r5;
	setp.lt.u32 	%p4, %r22, %r10;
	@%p4 bra 	$L__BB21_2;
$L__BB21_6:
	ret;

}
	// .globl	MemSet
.visible .entry MemSet(
	.param .u64 .ptr .align 1 MemSet_param_0,
	.param .f32 MemSet_param_1,
	.param .u32 MemSet_param_2,
	.param .u32 MemSet_param_3,
	.param .u32 MemSet_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [MemSet_param_0];
	ld.param.b32 	%r6, [MemSet_param_1];
	ld.param.b32 	%r7, [MemSet_param_2];
	ld.param.b32 	%r8, [MemSet_param_3];
	ld.param.b32 	%r9, [MemSet_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r14, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r14, %r7;
	@%p1 bra 	$L__BB22_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB22_2:
	mad.lo.s32 	%r13, %r14, %r9, %r8;
	mul.wide.u32 	%rd3, %r13, 4;
	add.s64 	%rd4, %rd1, %rd3;
	st.global.b32 	[%rd4], %r6;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r7;
	@%p2 bra 	$L__BB22_2;
$L__BB22_3:
	ret;

}
	// .globl	MemCpy
.visible .entry MemCpy(
	.param .u64 .ptr .align 1 MemCpy_param_0,
	.param .u64 .ptr .align 1 MemCpy_param_1,
	.param .u32 MemCpy_param_2,
	.param .u32 MemCpy_param_3,
	.param .u32 MemCpy_param_4,
	.param .u32 MemCpy_param_5,
	.param .u32 MemCpy_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [MemCpy_param_0];
	ld.param.b64 	%rd4, [MemCpy_param_1];
	ld.param.b32 	%r6, [MemCpy_param_2];
	ld.param.b32 	%r7, [MemCpy_param_3];
	ld.param.b32 	%r8, [MemCpy_param_4];
	ld.param.b32 	%r9, [MemCpy_param_5];
	ld.param.b32 	%r10, [MemCpy_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r17, %r1, %r11, %r12;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB23_3;
	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB23_2:
	mad.lo.s32 	%r14, %r17, %r10, %r8;
	mul.wide.u32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r15, [%rd6];
	mad.lo.s32 	%r16, %r17, %r9, %r7;
	mul.wide.u32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r15;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB23_2;
$L__BB23_3:
	ret;

}
	// .globl	FindMinAndMax
.visible .entry FindMinAndMax(
	.param .u64 .ptr .align 1 FindMinAndMax_param_0,
	.param .u32 FindMinAndMax_param_1,
	.param .u64 .ptr .align 1 FindMinAndMax_param_2,
	.param .u64 .ptr .align 1 FindMinAndMax_param_3,
	.param .u32 FindMinAndMax_param_4
)
{
	.reg .pred 	%p<46>;
	.reg .b32 	%r<147>;
	.reg .b64 	%rd<12>;
	// demoted variable
	.shared .align 4 .b8 _ZZ13FindMinAndMaxE5block[4096];
	ld.param.b64 	%rd1, [FindMinAndMax_param_0];
	ld.param.b32 	%r45, [FindMinAndMax_param_1];
	ld.param.b64 	%rd2, [FindMinAndMax_param_2];
	ld.param.b64 	%rd3, [FindMinAndMax_param_3];
	ld.param.b32 	%r46, [FindMinAndMax_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r48, %ntid.x;
	mad.lo.s32 	%r3, %r48, %r2, %r1;
	setp.le.u32 	%p1, %r45, %r3;
	mov.b32 	%r125, 0f00000000;
	@%p1 bra 	$L__BB24_2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.lo.s32 	%r49, %r46, %r3;
	mul.wide.u32 	%rd5, %r49, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.b32 	%r125, [%rd6];
$L__BB24_2:
	shl.b32 	%r50, %r1, 2;
	mov.b32 	%r51, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r52, %r51, %r50;
	st.shared.b32 	[%r52], %r125;
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB24_16;
	sub.s32 	%r6, %r45, %r3;
	setp.eq.s32 	%p3, %r6, 0;
	mov.b32 	%r146, 0f00800000;
	mov.b32 	%r145, 0f7F7FFFFF;
	@%p3 bra 	$L__BB24_15;
	min.u32 	%r7, %r6, 1024;
	and.b32 	%r8, %r7, 7;
	setp.lt.u32 	%p4, %r6, 8;
	mov.b32 	%r145, 0f7F7FFFFF;
	mov.b32 	%r146, 0f00800000;
	mov.b32 	%r137, 0;
	@%p4 bra 	$L__BB24_7;
	and.b32 	%r137, %r7, 2040;
	neg.s32 	%r127, %r137;
	mov.b32 	%r145, 0f7F7FFFFF;
	mov.b32 	%r146, 0f00800000;
	mov.b32 	%r126, 16;
$L__BB24_6:
	.pragma "nounroll";
	mov.b32 	%r62, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r63, %r62, %r126;
	ld.shared.b32 	%r64, [%r63+-16];
	setp.eq.s32 	%p5, %r126, 16;
	setp.gt.ftz.f32 	%p6, %r64, %r146;
	selp.f32 	%r66, %r64, %r146, %p6;
	selp.f32 	%r67, %r64, %r66, %p5;
	setp.lt.ftz.f32 	%p7, %r64, %r145;
	selp.f32 	%r68, %r64, %r145, %p7;
	selp.f32 	%r69, %r64, %r68, %p5;
	ld.shared.b32 	%r70, [%r63+-12];
	setp.gt.ftz.f32 	%p8, %r70, %r67;
	selp.f32 	%r71, %r70, %r67, %p8;
	setp.lt.ftz.f32 	%p9, %r70, %r69;
	selp.f32 	%r72, %r70, %r69, %p9;
	ld.shared.b32 	%r73, [%r63+-8];
	setp.gt.ftz.f32 	%p10, %r73, %r71;
	selp.f32 	%r74, %r73, %r71, %p10;
	setp.lt.ftz.f32 	%p11, %r73, %r72;
	selp.f32 	%r75, %r73, %r72, %p11;
	ld.shared.b32 	%r76, [%r63+-4];
	setp.gt.ftz.f32 	%p12, %r76, %r74;
	selp.f32 	%r77, %r76, %r74, %p12;
	setp.lt.ftz.f32 	%p13, %r76, %r75;
	selp.f32 	%r78, %r76, %r75, %p13;
	ld.shared.b32 	%r79, [%r63];
	setp.gt.ftz.f32 	%p14, %r79, %r77;
	selp.f32 	%r80, %r79, %r77, %p14;
	setp.lt.ftz.f32 	%p15, %r79, %r78;
	selp.f32 	%r81, %r79, %r78, %p15;
	ld.shared.b32 	%r82, [%r63+4];
	setp.gt.ftz.f32 	%p16, %r82, %r80;
	selp.f32 	%r83, %r82, %r80, %p16;
	setp.lt.ftz.f32 	%p17, %r82, %r81;
	selp.f32 	%r84, %r82, %r81, %p17;
	ld.shared.b32 	%r85, [%r63+8];
	setp.gt.ftz.f32 	%p18, %r85, %r83;
	selp.f32 	%r86, %r85, %r83, %p18;
	setp.lt.ftz.f32 	%p19, %r85, %r84;
	selp.f32 	%r87, %r85, %r84, %p19;
	ld.shared.b32 	%r88, [%r63+12];
	setp.gt.ftz.f32 	%p20, %r88, %r86;
	selp.f32 	%r146, %r88, %r86, %p20;
	setp.lt.ftz.f32 	%p21, %r88, %r87;
	selp.f32 	%r145, %r88, %r87, %p21;
	add.s32 	%r127, %r127, 8;
	add.s32 	%r126, %r126, 32;
	setp.ne.s32 	%p22, %r127, 0;
	@%p22 bra 	$L__BB24_6;
$L__BB24_7:
	setp.eq.s32 	%p23, %r8, 0;
	@%p23 bra 	$L__BB24_15;
	and.b32 	%r24, %r7, 3;
	setp.lt.u32 	%p24, %r8, 4;
	@%p24 bra 	$L__BB24_10;
	shl.b32 	%r90, %r137, 2;
	mov.b32 	%r91, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r92, %r91, %r90;
	ld.shared.b32 	%r93, [%r92];
	setp.eq.s32 	%p25, %r137, 0;
	setp.gt.ftz.f32 	%p26, %r93, %r146;
	selp.f32 	%r95, %r93, %r146, %p26;
	selp.f32 	%r96, %r93, %r95, %p25;
	setp.lt.ftz.f32 	%p27, %r93, %r145;
	selp.f32 	%r97, %r93, %r145, %p27;
	selp.f32 	%r98, %r93, %r97, %p25;
	ld.shared.b32 	%r99, [%r92+4];
	setp.gt.ftz.f32 	%p28, %r99, %r96;
	selp.f32 	%r100, %r99, %r96, %p28;
	setp.lt.ftz.f32 	%p29, %r99, %r98;
	selp.f32 	%r101, %r99, %r98, %p29;
	ld.shared.b32 	%r102, [%r92+8];
	setp.gt.ftz.f32 	%p30, %r102, %r100;
	selp.f32 	%r103, %r102, %r100, %p30;
	setp.lt.ftz.f32 	%p31, %r102, %r101;
	selp.f32 	%r104, %r102, %r101, %p31;
	ld.shared.b32 	%r105, [%r92+12];
	setp.gt.ftz.f32 	%p32, %r105, %r103;
	selp.f32 	%r146, %r105, %r103, %p32;
	setp.lt.ftz.f32 	%p33, %r105, %r104;
	selp.f32 	%r145, %r105, %r104, %p33;
	add.s32 	%r137, %r137, 4;
$L__BB24_10:
	setp.eq.s32 	%p34, %r24, 0;
	@%p34 bra 	$L__BB24_15;
	setp.eq.s32 	%p35, %r24, 1;
	@%p35 bra 	$L__BB24_13;
	shl.b32 	%r107, %r137, 2;
	mov.b32 	%r108, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r109, %r108, %r107;
	ld.shared.b32 	%r110, [%r109];
	setp.eq.s32 	%p36, %r137, 0;
	setp.gt.ftz.f32 	%p37, %r110, %r146;
	selp.f32 	%r112, %r110, %r146, %p37;
	selp.f32 	%r113, %r110, %r112, %p36;
	setp.lt.ftz.f32 	%p38, %r110, %r145;
	selp.f32 	%r114, %r110, %r145, %p38;
	selp.f32 	%r115, %r110, %r114, %p36;
	ld.shared.b32 	%r116, [%r109+4];
	setp.gt.ftz.f32 	%p39, %r116, %r113;
	selp.f32 	%r146, %r116, %r113, %p39;
	setp.lt.ftz.f32 	%p40, %r116, %r115;
	selp.f32 	%r145, %r116, %r115, %p40;
	add.s32 	%r137, %r137, 2;
$L__BB24_13:
	and.b32 	%r117, %r7, 1;
	setp.ne.b32 	%p41, %r117, 0;
	not.pred 	%p42, %p41;
	@%p42 bra 	$L__BB24_15;
	shl.b32 	%r118, %r137, 2;
	mov.b32 	%r119, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r120, %r119, %r118;
	ld.shared.b32 	%r121, [%r120];
	setp.eq.s32 	%p43, %r137, 0;
	setp.gt.ftz.f32 	%p44, %r121, %r146;
	selp.f32 	%r123, %r121, %r146, %p44;
	selp.f32 	%r146, %r121, %r123, %p43;
	setp.lt.ftz.f32 	%p45, %r121, %r145;
	selp.f32 	%r124, %r121, %r145, %p45;
	selp.f32 	%r145, %r121, %r124, %p43;
$L__BB24_15:
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.b32 	[%rd9], %r145;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.b32 	[%rd11], %r146;
$L__BB24_16:
	ret;

}
	// .globl	FindStdDev
.visible .entry FindStdDev(
	.param .u64 .ptr .align 1 FindStdDev_param_0,
	.param .u32 FindStdDev_param_1,
	.param .f32 FindStdDev_param_2,
	.param .u64 .ptr .align 1 FindStdDev_param_3,
	.param .u32 FindStdDev_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<114>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ10FindStdDevE5block[4096];
	ld.param.b64 	%rd1, [FindStdDev_param_0];
	ld.param.b32 	%r31, [FindStdDev_param_1];
	ld.param.b32 	%r32, [FindStdDev_param_2];
	ld.param.b64 	%rd2, [FindStdDev_param_3];
	ld.param.b32 	%r33, [FindStdDev_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r34, %ntid.x;
	mad.lo.s32 	%r3, %r34, %r2, %r1;
	setp.ge.u32 	%p1, %r3, %r31;
	@%p1 bra 	$L__BB25_2;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.lo.s32 	%r35, %r33, %r3;
	mul.wide.u32 	%rd4, %r35, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.b32 	%r36, [%rd5];
	shl.b32 	%r37, %r1, 2;
	mov.b32 	%r38, _ZZ10FindStdDevE5block;
	add.s32 	%r39, %r38, %r37;
	st.shared.b32 	[%r39], %r36;
$L__BB25_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB25_16;
	shl.b32 	%r41, %r2, 10;
	sub.s32 	%r4, %r31, %r41;
	setp.eq.s32 	%p3, %r4, 0;
	mov.b32 	%r113, 0f00000000;
	@%p3 bra 	$L__BB25_15;
	min.u32 	%r5, %r4, 1024;
	and.b32 	%r6, %r5, 7;
	setp.lt.u32 	%p4, %r4, 8;
	mov.b32 	%r113, 0f00000000;
	mov.b32 	%r108, 0;
	@%p4 bra 	$L__BB25_7;
	and.b32 	%r108, %r5, 2040;
	mov.b32 	%r47, _ZZ10FindStdDevE5block;
	add.s32 	%r101, %r47, 16;
	neg.s32 	%r102, %r108;
	mov.b32 	%r113, 0f00000000;
$L__BB25_6:
	.pragma "nounroll";
	ld.shared.b32 	%r48, [%r101+-16];
	sub.ftz.f32 	%r49, %r48, %r32;
	fma.rn.ftz.f32 	%r50, %r49, %r49, %r113;
	ld.shared.b32 	%r51, [%r101+-12];
	sub.ftz.f32 	%r52, %r51, %r32;
	fma.rn.ftz.f32 	%r53, %r52, %r52, %r50;
	ld.shared.b32 	%r54, [%r101+-8];
	sub.ftz.f32 	%r55, %r54, %r32;
	fma.rn.ftz.f32 	%r56, %r55, %r55, %r53;
	ld.shared.b32 	%r57, [%r101+-4];
	sub.ftz.f32 	%r58, %r57, %r32;
	fma.rn.ftz.f32 	%r59, %r58, %r58, %r56;
	ld.shared.b32 	%r60, [%r101];
	sub.ftz.f32 	%r61, %r60, %r32;
	fma.rn.ftz.f32 	%r62, %r61, %r61, %r59;
	ld.shared.b32 	%r63, [%r101+4];
	sub.ftz.f32 	%r64, %r63, %r32;
	fma.rn.ftz.f32 	%r65, %r64, %r64, %r62;
	ld.shared.b32 	%r66, [%r101+8];
	sub.ftz.f32 	%r67, %r66, %r32;
	fma.rn.ftz.f32 	%r68, %r67, %r67, %r65;
	ld.shared.b32 	%r69, [%r101+12];
	sub.ftz.f32 	%r70, %r69, %r32;
	fma.rn.ftz.f32 	%r113, %r70, %r70, %r68;
	add.s32 	%r102, %r102, 8;
	add.s32 	%r101, %r101, 32;
	setp.ne.s32 	%p5, %r102, 0;
	@%p5 bra 	$L__BB25_6;
$L__BB25_7:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB25_15;
	and.b32 	%r18, %r5, 3;
	setp.lt.u32 	%p7, %r6, 4;
	@%p7 bra 	$L__BB25_10;
	shl.b32 	%r72, %r108, 2;
	mov.b32 	%r73, _ZZ10FindStdDevE5block;
	add.s32 	%r74, %r73, %r72;
	ld.shared.b32 	%r75, [%r74];
	sub.ftz.f32 	%r76, %r75, %r32;
	fma.rn.ftz.f32 	%r77, %r76, %r76, %r113;
	ld.shared.b32 	%r78, [%r74+4];
	sub.ftz.f32 	%r79, %r78, %r32;
	fma.rn.ftz.f32 	%r80, %r79, %r79, %r77;
	ld.shared.b32 	%r81, [%r74+8];
	sub.ftz.f32 	%r82, %r81, %r32;
	fma.rn.ftz.f32 	%r83, %r82, %r82, %r80;
	ld.shared.b32 	%r84, [%r74+12];
	sub.ftz.f32 	%r85, %r84, %r32;
	fma.rn.ftz.f32 	%r113, %r85, %r85, %r83;
	add.s32 	%r108, %r108, 4;
$L__BB25_10:
	setp.eq.s32 	%p8, %r18, 0;
	@%p8 bra 	$L__BB25_15;
	setp.eq.s32 	%p9, %r18, 1;
	@%p9 bra 	$L__BB25_13;
	shl.b32 	%r87, %r108, 2;
	mov.b32 	%r88, _ZZ10FindStdDevE5block;
	add.s32 	%r89, %r88, %r87;
	ld.shared.b32 	%r90, [%r89];
	sub.ftz.f32 	%r91, %r90, %r32;
	fma.rn.ftz.f32 	%r92, %r91, %r91, %r113;
	ld.shared.b32 	%r93, [%r89+4];
	sub.ftz.f32 	%r94, %r93, %r32;
	fma.rn.ftz.f32 	%r113, %r94, %r94, %r92;
	add.s32 	%r108, %r108, 2;
$L__BB25_13:
	and.b32 	%r95, %r5, 1;
	setp.ne.b32 	%p10, %r95, 0;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB25_15;
	shl.b32 	%r96, %r108, 2;
	mov.b32 	%r97, _ZZ10FindStdDevE5block;
	add.s32 	%r98, %r97, %r96;
	ld.shared.b32 	%r99, [%r98];
	sub.ftz.f32 	%r100, %r99, %r32;
	fma.rn.ftz.f32 	%r113, %r100, %r100, %r113;
$L__BB25_15:
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.b32 	[%rd8], %r113;
$L__BB25_16:
	ret;

}
	// .globl	Constrain
.visible .entry Constrain(
	.param .u64 .ptr .align 1 Constrain_param_0,
	.param .u32 Constrain_param_1,
	.param .f32 Constrain_param_2,
	.param .f32 Constrain_param_3,
	.param .u32 Constrain_param_4
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd3, [Constrain_param_0];
	ld.param.b32 	%r7, [Constrain_param_1];
	ld.param.b32 	%r8, [Constrain_param_2];
	ld.param.b32 	%r9, [Constrain_param_3];
	ld.param.b32 	%r10, [Constrain_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r18, %r1, %r11, %r12;
	setp.ge.u32 	%p1, %r18, %r7;
	@%p1 bra 	$L__BB26_5;
	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd3;
$L__BB26_2:
	mul.lo.s32 	%r14, %r18, %r10;
	mul.wide.u32 	%rd4, %r14, 4;
	add.s64 	%rd2, %rd1, %rd4;
	ld.global.b32 	%r15, [%rd2];
	setp.lt.ftz.f32 	%p2, %r15, %r8;
	setp.eq.ftz.f32 	%p3, %r15, 0fFF800000;
	or.pred 	%p4, %p2, %p3;
	setp.gt.ftz.f32 	%p5, %r15, %r9;
	setp.eq.ftz.f32 	%p6, %r15, 0f7F800000;
	or.pred 	%p7, %p5, %p6;
	selp.f32 	%r16, %r9, %r8, %p7;
	or.pred 	%p8, %p4, %p7;
	abs.ftz.f32 	%r17, %r15;
	setp.nan.ftz.f32 	%p9, %r17, %r17;
	selp.f32 	%r5, 0f00000000, %r16, %p9;
	or.pred 	%p10, %p9, %p8;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB26_4;
	st.global.b32 	[%rd2], %r5;
$L__BB26_4:
	add.s32 	%r18, %r18, %r3;
	setp.lt.u32 	%p12, %r18, %r7;
	@%p12 bra 	$L__BB26_2;
$L__BB26_5:
	ret;

}
	// .globl	RoundInPlace
.visible .entry RoundInPlace(
	.param .u64 .ptr .align 1 RoundInPlace_param_0,
	.param .u32 RoundInPlace_param_1,
	.param .f32 RoundInPlace_param_2,
	.param .f32 RoundInPlace_param_3,
	.param .f32 RoundInPlace_param_4,
	.param .u32 RoundInPlace_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [RoundInPlace_param_0];
	ld.param.b32 	%r7, [RoundInPlace_param_1];
	ld.param.b32 	%r8, [RoundInPlace_param_2];
	ld.param.b32 	%r9, [RoundInPlace_param_3];
	ld.param.b32 	%r10, [RoundInPlace_param_4];
	ld.param.b32 	%r11, [RoundInPlace_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r18, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r18, %r7;
	@%p1 bra 	$L__BB27_3;
	mul.lo.s32 	%r3, %r11, %r11;
	mov.u32 	%r14, %nctaid.x;
	mul.lo.s32 	%r4, %r1, %r14;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB27_2:
	mul.lo.s32 	%r15, %r3, %r18;
	mul.wide.u32 	%rd3, %r15, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r16, [%rd4];
	setp.ltu.ftz.f32 	%p2, %r16, %r10;
	selp.f32 	%r17, %r8, %r9, %p2;
	st.global.b32 	[%rd4], %r17;
	add.s32 	%r18, %r18, %r4;
	setp.lt.u32 	%p3, %r18, %r7;
	@%p3 bra 	$L__BB27_2;
$L__BB27_3:
	ret;

}
	// .globl	Pow
.visible .entry Pow(
	.param .u64 .ptr .align 1 Pow_param_0,
	.param .u64 .ptr .align 1 Pow_param_1,
	.param .u32 Pow_param_2,
	.param .f32 Pow_param_3,
	.param .u32 Pow_param_4,
	.param .u32 Pow_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Pow_param_0];
	ld.param.b64 	%rd4, [Pow_param_1];
	ld.param.b32 	%r6, [Pow_param_2];
	ld.param.b32 	%r7, [Pow_param_3];
	ld.param.b32 	%r8, [Pow_param_4];
	ld.param.b32 	%r9, [Pow_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r19, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r19, %r6;
	@%p1 bra 	$L__BB28_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB28_2:
	mul.lo.s32 	%r13, %r19, %r8;
	mul.wide.u32 	%rd5, %r13, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r14, [%rd6];
	lg2.approx.ftz.f32 	%r15, %r14;
	mul.ftz.f32 	%r16, %r7, %r15;
	ex2.approx.ftz.f32 	%r17, %r16;
	mul.lo.s32 	%r18, %r19, %r9;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r17;
	add.s32 	%r19, %r19, %r3;
	setp.lt.u32 	%p2, %r19, %r6;
	@%p2 bra 	$L__BB28_2;
$L__BB28_3:
	ret;

}
	// .globl	Diagonal
.visible .entry Diagonal(
	.param .u64 .ptr .align 1 Diagonal_param_0,
	.param .u64 .ptr .align 1 Diagonal_param_1,
	.param .u32 Diagonal_param_2,
	.param .u32 Diagonal_param_3,
	.param .u32 Diagonal_param_4,
	.param .u32 Diagonal_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Diagonal_param_0];
	ld.param.b64 	%rd4, [Diagonal_param_1];
	ld.param.b32 	%r8, [Diagonal_param_2];
	ld.param.b32 	%r9, [Diagonal_param_3];
	ld.param.b32 	%r10, [Diagonal_param_4];
	ld.param.b32 	%r11, [Diagonal_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r19, %r1, %r12, %r13;
	min.u32 	%r14, %r8, %r9;
	setp.le.u32 	%p1, %r14, %r19;
	@%p1 bra 	$L__BB29_3;
	mul.lo.s32 	%r3, %r10, %r8;
	mov.u32 	%r15, %nctaid.x;
	mul.lo.s32 	%r4, %r1, %r15;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB29_2:
	mad.lo.s32 	%r16, %r3, %r19, %r19;
	mul.wide.u32 	%rd5, %r16, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r17, [%rd6];
	mul.lo.s32 	%r18, %r19, %r11;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r17;
	add.s32 	%r19, %r19, %r4;
	setp.lt.u32 	%p2, %r19, %r14;
	@%p2 bra 	$L__BB29_2;
$L__BB29_3:
	ret;

}
	// .globl	L1Regularisation
.visible .entry L1Regularisation(
	.param .u64 .ptr .align 1 L1Regularisation_param_0,
	.param .u32 L1Regularisation_param_1,
	.param .f32 L1Regularisation_param_2,
	.param .u32 L1Regularisation_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<20>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [L1Regularisation_param_0];
	ld.param.b32 	%r6, [L1Regularisation_param_1];
	ld.param.b32 	%r7, [L1Regularisation_param_2];
	ld.param.b32 	%r8, [L1Regularisation_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r19, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r19, %r6;
	@%p1 bra 	$L__BB30_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB30_2:
	mul.lo.s32 	%r12, %r19, %r8;
	mul.wide.u32 	%rd3, %r12, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r13, [%rd4];
	abs.ftz.f32 	%r14, %r13;
	setp.lt.ftz.f32 	%p2, %r14, %r7;
	sub.ftz.f32 	%r15, %r14, %r7;
	setp.gt.ftz.f32 	%p3, %r13, 0f00000000;
	neg.ftz.f32 	%r16, %r15;
	selp.f32 	%r17, %r15, %r16, %p3;
	selp.f32 	%r18, 0f00000000, %r17, %p2;
	st.global.b32 	[%rd4], %r18;
	add.s32 	%r19, %r19, %r3;
	setp.lt.u32 	%p4, %r19, %r6;
	@%p4 bra 	$L__BB30_2;
$L__BB30_3:
	ret;

}
	// .globl	PointwiseDivideRows
.visible .entry PointwiseDivideRows(
	.param .u64 .ptr .align 1 PointwiseDivideRows_param_0,
	.param .u64 .ptr .align 1 PointwiseDivideRows_param_1,
	.param .u32 PointwiseDivideRows_param_2,
	.param .u32 PointwiseDivideRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [PointwiseDivideRows_param_0];
	ld.param.b64 	%rd4, [PointwiseDivideRows_param_1];
	ld.param.b32 	%r11, [PointwiseDivideRows_param_2];
	ld.param.b32 	%r12, [PointwiseDivideRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r23, %r1, %r13, %r14;
	setp.ge.u32 	%p1, %r23, %r11;
	@%p1 bra 	$L__BB31_6;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r15, %r16, %r17;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r4, %r15, %r18;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r19;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB31_2:
	setp.ge.u32 	%p2, %r3, %r12;
	@%p2 bra 	$L__BB31_5;
	mul.wide.u32 	%rd5, %r23, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.b32 	%r7, [%rd6];
	mov.b32 	%r24, %r3;
$L__BB31_4:
	mad.lo.s32 	%r20, %r24, %r11, %r23;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.b32 	%r21, [%rd8];
	div.approx.ftz.f32 	%r22, %r21, %r7;
	st.global.b32 	[%rd8], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r12;
	@%p3 bra 	$L__BB31_4;
$L__BB31_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r11;
	@%p4 bra 	$L__BB31_2;
$L__BB31_6:
	ret;

}
	// .globl	PointwiseDivideColumns
.visible .entry PointwiseDivideColumns(
	.param .u64 .ptr .align 1 PointwiseDivideColumns_param_0,
	.param .u64 .ptr .align 1 PointwiseDivideColumns_param_1,
	.param .u32 PointwiseDivideColumns_param_2,
	.param .u32 PointwiseDivideColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [PointwiseDivideColumns_param_0];
	ld.param.b64 	%rd4, [PointwiseDivideColumns_param_1];
	ld.param.b32 	%r10, [PointwiseDivideColumns_param_2];
	ld.param.b32 	%r11, [PointwiseDivideColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r23, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r23, %r10;
	@%p1 bra 	$L__BB32_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB32_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB32_5;
	mov.b32 	%r24, %r3;
$L__BB32_4:
	mad.lo.s32 	%r19, %r24, %r10, %r23;
	mul.wide.u32 	%rd5, %r19, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.b32 	%r20, [%rd6];
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.b32 	%r21, [%rd8];
	div.approx.ftz.f32 	%r22, %r20, %r21;
	st.global.b32 	[%rd6], %r22;
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p3, %r24, %r11;
	@%p3 bra 	$L__BB32_4;
$L__BB32_5:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p4, %r23, %r10;
	@%p4 bra 	$L__BB32_2;
$L__BB32_6:
	ret;

}
	// .globl	SplitRows
.visible .entry SplitRows(
	.param .u64 .ptr .align 1 SplitRows_param_0,
	.param .u64 .ptr .align 1 SplitRows_param_1,
	.param .u64 .ptr .align 1 SplitRows_param_2,
	.param .u32 SplitRows_param_3,
	.param .u32 SplitRows_param_4,
	.param .u32 SplitRows_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<26>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [SplitRows_param_0];
	ld.param.b64 	%rd5, [SplitRows_param_1];
	ld.param.b64 	%rd6, [SplitRows_param_2];
	ld.param.b32 	%r12, [SplitRows_param_3];
	ld.param.b32 	%r13, [SplitRows_param_4];
	ld.param.b32 	%r14, [SplitRows_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r24, %r1, %r15, %r16;
	setp.ge.u32 	%p1, %r24, %r12;
	@%p1 bra 	$L__BB33_9;
	mov.u32 	%r17, %ntid.y;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %tid.y;
	mad.lo.s32 	%r3, %r17, %r18, %r19;
	mov.u32 	%r20, %nctaid.y;
	mul.lo.s32 	%r4, %r17, %r20;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r21;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
$L__BB33_2:
	setp.ge.u32 	%p2, %r3, %r13;
	@%p2 bra 	$L__BB33_8;
	mov.b32 	%r25, %r3;
$L__BB33_4:
	mad.lo.s32 	%r8, %r25, %r12, %r24;
	mul.wide.u32 	%rd7, %r8, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r9, [%rd8];
	setp.lt.u32 	%p3, %r25, %r14;
	@%p3 bra 	$L__BB33_6;
	sub.s32 	%r22, %r25, %r14;
	mad.lo.s32 	%r23, %r22, %r12, %r24;
	mul.wide.u32 	%rd9, %r23, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.b32 	[%rd10], %r9;
	bra.uni 	$L__BB33_7;
$L__BB33_6:
	add.s64 	%rd12, %rd3, %rd7;
	st.global.b32 	[%rd12], %r9;
$L__BB33_7:
	add.s32 	%r25, %r25, %r4;
	setp.lt.u32 	%p4, %r25, %r13;
	@%p4 bra 	$L__BB33_4;
$L__BB33_8:
	add.s32 	%r24, %r24, %r5;
	setp.lt.u32 	%p5, %r24, %r12;
	@%p5 bra 	$L__BB33_2;
$L__BB33_9:
	ret;

}
	// .globl	SplitColumns
.visible .entry SplitColumns(
	.param .u64 .ptr .align 1 SplitColumns_param_0,
	.param .u64 .ptr .align 1 SplitColumns_param_1,
	.param .u64 .ptr .align 1 SplitColumns_param_2,
	.param .u32 SplitColumns_param_3,
	.param .u32 SplitColumns_param_4,
	.param .u32 SplitColumns_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<28>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [SplitColumns_param_0];
	ld.param.b64 	%rd5, [SplitColumns_param_1];
	ld.param.b64 	%rd6, [SplitColumns_param_2];
	ld.param.b32 	%r13, [SplitColumns_param_3];
	ld.param.b32 	%r14, [SplitColumns_param_4];
	ld.param.b32 	%r15, [SplitColumns_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r26, %r1, %r16, %r17;
	setp.ge.u32 	%p1, %r26, %r13;
	@%p1 bra 	$L__BB34_9;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r3, %r18, %r19, %r20;
	sub.s32 	%r4, %r13, %r15;
	mov.u32 	%r21, %nctaid.y;
	mul.lo.s32 	%r5, %r18, %r21;
	mov.u32 	%r22, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r22;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
$L__BB34_2:
	setp.ge.u32 	%p2, %r3, %r14;
	@%p2 bra 	$L__BB34_8;
	sub.s32 	%r8, %r26, %r15;
	mov.b32 	%r27, %r3;
$L__BB34_4:
	setp.lt.u32 	%p3, %r26, %r15;
	mad.lo.s32 	%r23, %r27, %r13, %r26;
	mul.wide.u32 	%rd7, %r23, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r10, [%rd8];
	@%p3 bra 	$L__BB34_6;
	mad.lo.s32 	%r24, %r27, %r4, %r8;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.b32 	[%rd10], %r10;
	bra.uni 	$L__BB34_7;
$L__BB34_6:
	mad.lo.s32 	%r25, %r27, %r15, %r26;
	mul.wide.u32 	%rd11, %r25, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b32 	[%rd12], %r10;
$L__BB34_7:
	add.s32 	%r27, %r27, %r5;
	setp.lt.u32 	%p4, %r27, %r14;
	@%p4 bra 	$L__BB34_4;
$L__BB34_8:
	add.s32 	%r26, %r26, %r6;
	setp.lt.u32 	%p5, %r26, %r13;
	@%p5 bra 	$L__BB34_2;
$L__BB34_9:
	ret;

}
	// .globl	ConcatColumns
.visible .entry ConcatColumns(
	.param .u64 .ptr .align 1 ConcatColumns_param_0,
	.param .u64 .ptr .align 1 ConcatColumns_param_1,
	.param .u64 .ptr .align 1 ConcatColumns_param_2,
	.param .u32 ConcatColumns_param_3,
	.param .u32 ConcatColumns_param_4,
	.param .u32 ConcatColumns_param_5,
	.param .u32 ConcatColumns_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<31>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [ConcatColumns_param_0];
	ld.param.b64 	%rd5, [ConcatColumns_param_1];
	ld.param.b64 	%rd6, [ConcatColumns_param_2];
	ld.param.b32 	%r14, [ConcatColumns_param_3];
	ld.param.b32 	%r15, [ConcatColumns_param_4];
	ld.param.b32 	%r16, [ConcatColumns_param_5];
	ld.param.b32 	%r17, [ConcatColumns_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r28, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r28, %r14;
	@%p1 bra 	$L__BB35_9;
	mov.u32 	%r20, %ntid.y;
	mov.u32 	%r21, %ctaid.y;
	mov.u32 	%r22, %tid.y;
	mad.lo.s32 	%r3, %r20, %r21, %r22;
	mov.u32 	%r23, %nctaid.y;
	mul.lo.s32 	%r4, %r20, %r23;
	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r24;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB35_2:
	setp.ge.u32 	%p2, %r3, %r15;
	@%p2 bra 	$L__BB35_8;
	sub.s32 	%r7, %r28, %r16;
	mov.b32 	%r29, %r3;
$L__BB35_4:
	setp.lt.u32 	%p3, %r28, %r16;
	@%p3 bra 	$L__BB35_6;
	mad.lo.s32 	%r25, %r29, %r17, %r7;
	mul.wide.u32 	%rd7, %r25, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r30, [%rd8];
	bra.uni 	$L__BB35_7;
$L__BB35_6:
	mad.lo.s32 	%r26, %r29, %r16, %r28;
	mul.wide.u32 	%rd9, %r26, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b32 	%r30, [%rd10];
$L__BB35_7:
	mad.lo.s32 	%r27, %r29, %r14, %r28;
	mul.wide.u32 	%rd11, %r27, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b32 	[%rd12], %r30;
	add.s32 	%r29, %r29, %r4;
	setp.lt.u32 	%p4, %r29, %r15;
	@%p4 bra 	$L__BB35_4;
$L__BB35_8:
	add.s32 	%r28, %r28, %r5;
	setp.lt.u32 	%p5, %r28, %r14;
	@%p5 bra 	$L__BB35_2;
$L__BB35_9:
	ret;

}
	// .globl	ConcatRows
.visible .entry ConcatRows(
	.param .u64 .ptr .align 1 ConcatRows_param_0,
	.param .u64 .ptr .align 1 ConcatRows_param_1,
	.param .u64 .ptr .align 1 ConcatRows_param_2,
	.param .u32 ConcatRows_param_3,
	.param .u32 ConcatRows_param_4,
	.param .u32 ConcatRows_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [ConcatRows_param_0];
	ld.param.b64 	%rd5, [ConcatRows_param_1];
	ld.param.b64 	%rd6, [ConcatRows_param_2];
	ld.param.b32 	%r13, [ConcatRows_param_3];
	ld.param.b32 	%r14, [ConcatRows_param_4];
	ld.param.b32 	%r15, [ConcatRows_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r16, %ctaid.x;
	mov.u32 	%r17, %tid.x;
	mad.lo.s32 	%r27, %r1, %r16, %r17;
	setp.ge.u32 	%p1, %r27, %r13;
	@%p1 bra 	$L__BB36_9;
	mov.u32 	%r18, %ntid.y;
	mov.u32 	%r19, %ctaid.y;
	mov.u32 	%r20, %tid.y;
	mad.lo.s32 	%r3, %r18, %r19, %r20;
	mov.u32 	%r21, %nctaid.y;
	mul.lo.s32 	%r4, %r18, %r21;
	mov.u32 	%r22, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r22;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB36_2:
	setp.ge.u32 	%p2, %r3, %r14;
	@%p2 bra 	$L__BB36_8;
	mov.b32 	%r28, %r3;
$L__BB36_4:
	setp.lt.u32 	%p3, %r28, %r15;
	@%p3 bra 	$L__BB36_6;
	sub.s32 	%r23, %r28, %r15;
	mad.lo.s32 	%r24, %r23, %r13, %r27;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r29, [%rd8];
	bra.uni 	$L__BB36_7;
$L__BB36_6:
	mad.lo.s32 	%r25, %r28, %r13, %r27;
	mul.wide.u32 	%rd9, %r25, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b32 	%r29, [%rd10];
$L__BB36_7:
	mad.lo.s32 	%r26, %r28, %r13, %r27;
	mul.wide.u32 	%rd11, %r26, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b32 	[%rd12], %r29;
	add.s32 	%r28, %r28, %r4;
	setp.lt.u32 	%p4, %r28, %r14;
	@%p4 bra 	$L__BB36_4;
$L__BB36_8:
	add.s32 	%r27, %r27, %r5;
	setp.lt.u32 	%p5, %r27, %r13;
	@%p5 bra 	$L__BB36_2;
$L__BB36_9:
	ret;

}
	// .globl	EuclideanDistance
.visible .entry EuclideanDistance(
	.param .u64 .ptr .align 1 EuclideanDistance_param_0,
	.param .u64 .ptr .align 1 EuclideanDistance_param_1,
	.param .u64 .ptr .align 1 EuclideanDistance_param_2,
	.param .u32 EuclideanDistance_param_3,
	.param .u32 EuclideanDistance_param_4,
	.param .u32 EuclideanDistance_param_5,
	.param .u32 EuclideanDistance_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [EuclideanDistance_param_0];
	ld.param.b64 	%rd5, [EuclideanDistance_param_1];
	ld.param.b64 	%rd6, [EuclideanDistance_param_2];
	ld.param.b32 	%r6, [EuclideanDistance_param_3];
	ld.param.b32 	%r7, [EuclideanDistance_param_4];
	ld.param.b32 	%r8, [EuclideanDistance_param_5];
	ld.param.b32 	%r9, [EuclideanDistance_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r20, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r20, %r6;
	@%p1 bra 	$L__BB37_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB37_2:
	mul.lo.s32 	%r13, %r20, %r7;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r14, [%rd8];
	mul.lo.s32 	%r15, %r20, %r8;
	mul.wide.u32 	%rd9, %r15, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b32 	%r16, [%rd10];
	sub.ftz.f32 	%r17, %r14, %r16;
	mul.ftz.f32 	%r18, %r17, %r17;
	mul.lo.s32 	%r19, %r20, %r9;
	mul.wide.u32 	%rd11, %r19, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b32 	[%rd12], %r18;
	add.s32 	%r20, %r20, %r3;
	setp.lt.u32 	%p2, %r20, %r6;
	@%p2 bra 	$L__BB37_2;
$L__BB37_3:
	ret;

}
	// .globl	ManhattanDistance
.visible .entry ManhattanDistance(
	.param .u64 .ptr .align 1 ManhattanDistance_param_0,
	.param .u64 .ptr .align 1 ManhattanDistance_param_1,
	.param .u64 .ptr .align 1 ManhattanDistance_param_2,
	.param .u32 ManhattanDistance_param_3,
	.param .u32 ManhattanDistance_param_4,
	.param .u32 ManhattanDistance_param_5,
	.param .u32 ManhattanDistance_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<21>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [ManhattanDistance_param_0];
	ld.param.b64 	%rd5, [ManhattanDistance_param_1];
	ld.param.b64 	%rd6, [ManhattanDistance_param_2];
	ld.param.b32 	%r6, [ManhattanDistance_param_3];
	ld.param.b32 	%r7, [ManhattanDistance_param_4];
	ld.param.b32 	%r8, [ManhattanDistance_param_5];
	ld.param.b32 	%r9, [ManhattanDistance_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r20, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r20, %r6;
	@%p1 bra 	$L__BB38_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB38_2:
	mul.lo.s32 	%r13, %r20, %r7;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r14, [%rd8];
	mul.lo.s32 	%r15, %r20, %r8;
	mul.wide.u32 	%rd9, %r15, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b32 	%r16, [%rd10];
	sub.ftz.f32 	%r17, %r14, %r16;
	abs.ftz.f32 	%r18, %r17;
	mul.lo.s32 	%r19, %r20, %r9;
	mul.wide.u32 	%rd11, %r19, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b32 	[%rd12], %r18;
	add.s32 	%r20, %r20, %r3;
	setp.lt.u32 	%p2, %r20, %r6;
	@%p2 bra 	$L__BB38_2;
$L__BB38_3:
	ret;

}
	// .globl	CosineDistance
.visible .entry CosineDistance(
	.param .u64 .ptr .align 1 CosineDistance_param_0,
	.param .u64 .ptr .align 1 CosineDistance_param_1,
	.param .u64 .ptr .align 1 CosineDistance_param_2,
	.param .u64 .ptr .align 1 CosineDistance_param_3,
	.param .u64 .ptr .align 1 CosineDistance_param_4,
	.param .u32 CosineDistance_param_5,
	.param .u32 CosineDistance_param_6,
	.param .u32 CosineDistance_param_7
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<15>;

	ld.param.b64 	%rd6, [CosineDistance_param_0];
	ld.param.b64 	%rd7, [CosineDistance_param_1];
	ld.param.b64 	%rd8, [CosineDistance_param_2];
	ld.param.b64 	%rd9, [CosineDistance_param_3];
	ld.param.b64 	%rd10, [CosineDistance_param_4];
	ld.param.b32 	%r6, [CosineDistance_param_5];
	ld.param.b32 	%r7, [CosineDistance_param_6];
	ld.param.b32 	%r8, [CosineDistance_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r22, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r22, %r6;
	@%p1 bra 	$L__BB39_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd9;
	cvta.to.global.u64 	%rd5, %rd10;
$L__BB39_2:
	mul.lo.s32 	%r12, %r22, %r7;
	mul.wide.u32 	%rd11, %r12, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.nc.b32 	%r13, [%rd12];
	mul.lo.s32 	%r14, %r22, %r8;
	mul.wide.u32 	%rd13, %r14, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.b32 	%r15, [%rd14];
	mul.ftz.f32 	%r16, %r13, %r13;
	atom.global.add.f32 	%r17, [%rd3], %r16;
	mul.ftz.f32 	%r18, %r13, %r15;
	atom.global.add.f32 	%r19, [%rd4], %r18;
	mul.ftz.f32 	%r20, %r15, %r15;
	atom.global.add.f32 	%r21, [%rd5], %r20;
	add.s32 	%r22, %r22, %r3;
	setp.lt.u32 	%p2, %r22, %r6;
	@%p2 bra 	$L__BB39_2;
$L__BB39_3:
	ret;

}
	// .globl	Abs
.visible .entry Abs(
	.param .u64 .ptr .align 1 Abs_param_0,
	.param .u64 .ptr .align 1 Abs_param_1,
	.param .u32 Abs_param_2,
	.param .u32 Abs_param_3,
	.param .u32 Abs_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Abs_param_0];
	ld.param.b64 	%rd4, [Abs_param_1];
	ld.param.b32 	%r6, [Abs_param_2];
	ld.param.b32 	%r7, [Abs_param_3];
	ld.param.b32 	%r8, [Abs_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r16, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB40_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB40_2:
	mul.lo.s32 	%r12, %r16, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	abs.ftz.f32 	%r14, %r13;
	mul.lo.s32 	%r15, %r16, %r8;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r14;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB40_2;
$L__BB40_3:
	ret;

}
	// .globl	Log
.visible .entry Log(
	.param .u64 .ptr .align 1 Log_param_0,
	.param .u64 .ptr .align 1 Log_param_1,
	.param .u32 Log_param_2,
	.param .u32 Log_param_3,
	.param .u32 Log_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Log_param_0];
	ld.param.b64 	%rd4, [Log_param_1];
	ld.param.b32 	%r6, [Log_param_2];
	ld.param.b32 	%r7, [Log_param_3];
	ld.param.b32 	%r8, [Log_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB41_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB41_2:
	mul.lo.s32 	%r12, %r17, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	lg2.approx.ftz.f32 	%r14, %r13;
	mul.ftz.f32 	%r15, %r14, 0f3F317218;
	mul.lo.s32 	%r16, %r17, %r8;
	mul.wide.u32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r15;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB41_2;
$L__BB41_3:
	ret;

}
	// .globl	Exp
.visible .entry Exp(
	.param .u64 .ptr .align 1 Exp_param_0,
	.param .u64 .ptr .align 1 Exp_param_1,
	.param .u32 Exp_param_2,
	.param .u32 Exp_param_3,
	.param .u32 Exp_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [Exp_param_0];
	ld.param.b64 	%rd4, [Exp_param_1];
	ld.param.b32 	%r6, [Exp_param_2];
	ld.param.b32 	%r7, [Exp_param_3];
	ld.param.b32 	%r8, [Exp_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB42_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB42_2:
	mul.lo.s32 	%r12, %r17, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	mul.ftz.f32 	%r14, %r13, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r15, %r14;
	mul.lo.s32 	%r16, %r17, %r8;
	mul.wide.u32 	%rd7, %r16, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r15;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB42_2;
$L__BB42_3:
	ret;

}
	// .globl	Normalise
.visible .entry Normalise(
	.param .u64 .ptr .align 1 Normalise_param_0,
	.param .u32 Normalise_param_1,
	.param .f32 Normalise_param_2,
	.param .f32 Normalise_param_3,
	.param .u32 Normalise_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [Normalise_param_0];
	ld.param.b32 	%r6, [Normalise_param_1];
	ld.param.b32 	%r7, [Normalise_param_2];
	ld.param.b32 	%r8, [Normalise_param_3];
	ld.param.b32 	%r9, [Normalise_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r17, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB43_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB43_2:
	mul.lo.s32 	%r13, %r17, %r9;
	mul.wide.u32 	%rd3, %r13, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r14, [%rd4];
	sub.ftz.f32 	%r15, %r14, %r7;
	div.approx.ftz.f32 	%r16, %r15, %r8;
	st.global.b32 	[%rd4], %r16;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB43_2;
$L__BB43_3:
	ret;

}
	// .globl	SoftmaxVector
.visible .entry SoftmaxVector(
	.param .u64 .ptr .align 1 SoftmaxVector_param_0,
	.param .u64 .ptr .align 1 SoftmaxVector_param_1,
	.param .u32 SoftmaxVector_param_2,
	.param .f32 SoftmaxVector_param_3,
	.param .u32 SoftmaxVector_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<18>;
	.reg .b64 	%rd<9>;

	ld.param.b64 	%rd3, [SoftmaxVector_param_0];
	ld.param.b64 	%rd4, [SoftmaxVector_param_1];
	ld.param.b32 	%r6, [SoftmaxVector_param_2];
	ld.param.b32 	%r7, [SoftmaxVector_param_3];
	ld.param.b32 	%r8, [SoftmaxVector_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r17, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r17, %r6;
	@%p1 bra 	$L__BB44_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB44_2:
	mul.lo.s32 	%r12, %r17, %r8;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b32 	%r13, [%rd6];
	sub.ftz.f32 	%r14, %r13, %r7;
	mul.ftz.f32 	%r15, %r14, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%r16, %r15;
	mul.wide.u32 	%rd7, %r17, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.b32 	[%rd8], %r16;
	add.s32 	%r17, %r17, %r3;
	setp.lt.u32 	%p2, %r17, %r6;
	@%p2 bra 	$L__BB44_2;
$L__BB44_3:
	ret;

}
	// .globl	VectorAddInPlace
.visible .entry VectorAddInPlace(
	.param .u64 .ptr .align 1 VectorAddInPlace_param_0,
	.param .u32 VectorAddInPlace_param_1,
	.param .f32 VectorAddInPlace_param_2,
	.param .u32 VectorAddInPlace_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .b64 	%rd<5>;

	ld.param.b64 	%rd2, [VectorAddInPlace_param_0];
	ld.param.b32 	%r6, [VectorAddInPlace_param_1];
	ld.param.b32 	%r7, [VectorAddInPlace_param_2];
	ld.param.b32 	%r8, [VectorAddInPlace_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r15, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r15, %r6;
	@%p1 bra 	$L__BB45_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB45_2:
	mul.lo.s32 	%r12, %r15, %r8;
	mul.wide.u32 	%rd3, %r12, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r13, [%rd4];
	add.ftz.f32 	%r14, %r7, %r13;
	st.global.b32 	[%rd4], %r14;
	add.s32 	%r15, %r15, %r3;
	setp.lt.u32 	%p2, %r15, %r6;
	@%p2 bra 	$L__BB45_2;
$L__BB45_3:
	ret;

}
	// .globl	VectorCopyRandom
.visible .entry VectorCopyRandom(
	.param .u64 .ptr .align 1 VectorCopyRandom_param_0,
	.param .u64 .ptr .align 1 VectorCopyRandom_param_1,
	.param .u64 .ptr .align 1 VectorCopyRandom_param_2,
	.param .u32 VectorCopyRandom_param_3,
	.param .u32 VectorCopyRandom_param_4,
	.param .u32 VectorCopyRandom_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [VectorCopyRandom_param_0];
	ld.param.b64 	%rd5, [VectorCopyRandom_param_1];
	ld.param.b64 	%rd6, [VectorCopyRandom_param_2];
	ld.param.b32 	%r6, [VectorCopyRandom_param_3];
	ld.param.b32 	%r7, [VectorCopyRandom_param_4];
	ld.param.b32 	%r8, [VectorCopyRandom_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r18, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r18, %r6;
	@%p1 bra 	$L__BB46_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
$L__BB46_2:
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r12, [%rd8];
	mul.lo.s32 	%r13, %r12, %r7;
	mul.wide.u32 	%rd9, %r13, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b32 	%r14, [%rd10];
	mul.lo.s32 	%r15, %r18, %r8;
	mul.wide.u32 	%rd11, %r15, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.b32 	%r16, [%rd12];
	add.ftz.f32 	%r17, %r14, %r16;
	st.global.b32 	[%rd12], %r17;
	add.s32 	%r18, %r18, %r3;
	setp.lt.u32 	%p2, %r18, %r6;
	@%p2 bra 	$L__BB46_2;
$L__BB46_3:
	ret;

}
	// .globl	CopyToMatrixRows
.visible .entry CopyToMatrixRows(
	.param .u64 .ptr .align 1 CopyToMatrixRows_param_0,
	.param .u64 .ptr .align 1 CopyToMatrixRows_param_1,
	.param .u32 CopyToMatrixRows_param_2,
	.param .u32 CopyToMatrixRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd4, [CopyToMatrixRows_param_0];
	ld.param.b64 	%rd5, [CopyToMatrixRows_param_1];
	ld.param.b32 	%r10, [CopyToMatrixRows_param_2];
	ld.param.b32 	%r11, [CopyToMatrixRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r21, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r21, %r10;
	@%p1 bra 	$L__BB47_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
$L__BB47_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB47_5;
	mul.wide.u32 	%rd6, %r21, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.b64 	%rd8, [%rd7];
	cvta.to.global.u64 	%rd3, %rd8;
	mov.b32 	%r22, %r3;
$L__BB47_4:
	mul.wide.u32 	%rd9, %r22, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.b32 	%r19, [%rd10];
	mad.lo.s32 	%r20, %r22, %r10, %r21;
	mul.wide.u32 	%rd11, %r20, 4;
	add.s64 	%rd12, %rd2, %rd11;
	st.global.b32 	[%rd12], %r19;
	add.s32 	%r22, %r22, %r4;
	setp.lt.u32 	%p3, %r22, %r11;
	@%p3 bra 	$L__BB47_4;
$L__BB47_5:
	add.s32 	%r21, %r21, %r5;
	setp.lt.u32 	%p4, %r21, %r10;
	@%p4 bra 	$L__BB47_2;
$L__BB47_6:
	ret;

}
	// .globl	CopyToMatrixColumns
.visible .entry CopyToMatrixColumns(
	.param .u64 .ptr .align 1 CopyToMatrixColumns_param_0,
	.param .u64 .ptr .align 1 CopyToMatrixColumns_param_1,
	.param .u32 CopyToMatrixColumns_param_2,
	.param .u32 CopyToMatrixColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<13>;

	ld.param.b64 	%rd3, [CopyToMatrixColumns_param_0];
	ld.param.b64 	%rd4, [CopyToMatrixColumns_param_1];
	ld.param.b32 	%r10, [CopyToMatrixColumns_param_2];
	ld.param.b32 	%r11, [CopyToMatrixColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r21, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r21, %r10;
	@%p1 bra 	$L__BB48_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB48_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB48_5;
	mul.wide.u32 	%rd9, %r21, 4;
	mov.b32 	%r22, %r3;
$L__BB48_4:
	mul.wide.u32 	%rd5, %r22, 8;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.b64 	%rd7, [%rd6];
	cvta.to.global.u64 	%rd8, %rd7;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.b32 	%r19, [%rd10];
	mad.lo.s32 	%r20, %r22, %r10, %r21;
	mul.wide.u32 	%rd11, %r20, 4;
	add.s64 	%rd12, %rd2, %rd11;
	st.global.b32 	[%rd12], %r19;
	add.s32 	%r22, %r22, %r4;
	setp.lt.u32 	%p3, %r22, %r11;
	@%p3 bra 	$L__BB48_4;
$L__BB48_5:
	add.s32 	%r21, %r21, %r5;
	setp.lt.u32 	%p4, %r21, %r10;
	@%p4 bra 	$L__BB48_2;
$L__BB48_6:
	ret;

}
	// .globl	TensorAddPadding
.visible .entry TensorAddPadding(
	.param .u32 TensorAddPadding_param_0,
	.param .u64 .ptr .align 1 TensorAddPadding_param_1,
	.param .u64 .ptr .align 1 TensorAddPadding_param_2,
	.param .u32 TensorAddPadding_param_3,
	.param .u32 TensorAddPadding_param_4,
	.param .u32 TensorAddPadding_param_5,
	.param .u32 TensorAddPadding_param_6,
	.param .u32 TensorAddPadding_param_7,
	.param .u32 TensorAddPadding_param_8,
	.param .u32 TensorAddPadding_param_9
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<19>;

	ld.param.b32 	%r18, [TensorAddPadding_param_0];
	ld.param.b64 	%rd3, [TensorAddPadding_param_1];
	ld.param.b64 	%rd4, [TensorAddPadding_param_2];
	ld.param.b32 	%r19, [TensorAddPadding_param_3];
	ld.param.b32 	%r20, [TensorAddPadding_param_4];
	ld.param.b32 	%r21, [TensorAddPadding_param_5];
	ld.param.b32 	%r22, [TensorAddPadding_param_7];
	ld.param.b32 	%r23, [TensorAddPadding_param_8];
	ld.param.b32 	%r24, [TensorAddPadding_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r25, %ctaid.x;
	mov.u32 	%r26, %tid.x;
	mad.lo.s32 	%r43, %r1, %r25, %r26;
	setp.ge.u32 	%p1, %r43, %r18;
	@%p1 bra 	$L__BB49_5;
	sub.s32 	%r3, %r22, %r24;
	sub.s32 	%r4, %r23, %r24;
	mul.lo.s32 	%r5, %r20, %r19;
	mul.lo.s32 	%r6, %r5, %r21;
	mul.lo.s32 	%r7, %r23, %r22;
	mul.lo.s32 	%r8, %r7, %r21;
	mov.u32 	%r27, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r27;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB49_2:
	div.u32 	%r29, %r43, %r22;
	mul.lo.s32 	%r30, %r29, %r22;
	sub.s32 	%r11, %r43, %r30;
	div.u32 	%r31, %r29, %r23;
	mul.lo.s32 	%r32, %r31, %r23;
	sub.s32 	%r12, %r29, %r32;
	div.u32 	%r14, %r31, %r21;
	mul.lo.s32 	%r33, %r14, %r21;
	sub.s32 	%r13, %r31, %r33;
	setp.ge.u32 	%p2, %r11, %r3;
	min.u32 	%r34, %r11, %r12;
	setp.lt.u32 	%p3, %r34, %r24;
	setp.ge.u32 	%p4, %r12, %r4;
	or.pred 	%p5, %p2, %p4;
	mov.b32 	%r44, 0f00000000;
	or.pred 	%p6, %p5, %p3;
	@%p6 bra 	$L__BB49_4;
	mul.lo.s32 	%r35, %r6, %r14;
	cvt.u64.u32 	%rd5, %r35;
	mul.lo.s32 	%r36, %r13, %r5;
	cvt.u64.u32 	%rd6, %r36;
	sub.s32 	%r37, %r12, %r24;
	sub.s32 	%r38, %r11, %r24;
	mad.lo.s32 	%r39, %r37, %r19, %r38;
	cvt.u64.u32 	%rd7, %r39;
	add.s64 	%rd8, %rd7, %rd6;
	add.s64 	%rd9, %rd8, %rd5;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.b32 	%r44, [%rd11];
$L__BB49_4:
	mul.lo.s32 	%r40, %r8, %r14;
	cvt.u64.u32 	%rd12, %r40;
	mul.lo.s32 	%r41, %r13, %r7;
	cvt.u64.u32 	%rd13, %r41;
	mad.lo.s32 	%r42, %r12, %r22, %r11;
	cvt.u64.u32 	%rd14, %r42;
	add.s64 	%rd15, %rd13, %rd14;
	add.s64 	%rd16, %rd15, %rd12;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	st.global.b32 	[%rd18], %r44;
	add.s32 	%r43, %r43, %r9;
	setp.lt.u32 	%p7, %r43, %r18;
	@%p7 bra 	$L__BB49_2;
$L__BB49_5:
	ret;

}
	// .globl	TensorRemovePadding
.visible .entry TensorRemovePadding(
	.param .u32 TensorRemovePadding_param_0,
	.param .u64 .ptr .align 1 TensorRemovePadding_param_1,
	.param .u64 .ptr .align 1 TensorRemovePadding_param_2,
	.param .u32 TensorRemovePadding_param_3,
	.param .u32 TensorRemovePadding_param_4,
	.param .u32 TensorRemovePadding_param_5,
	.param .u32 TensorRemovePadding_param_6,
	.param .u32 TensorRemovePadding_param_7,
	.param .u32 TensorRemovePadding_param_8,
	.param .u32 TensorRemovePadding_param_9
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<41>;
	.reg .b64 	%rd<19>;

	ld.param.b32 	%r15, [TensorRemovePadding_param_0];
	ld.param.b64 	%rd3, [TensorRemovePadding_param_1];
	ld.param.b64 	%rd4, [TensorRemovePadding_param_2];
	ld.param.b32 	%r16, [TensorRemovePadding_param_3];
	ld.param.b32 	%r17, [TensorRemovePadding_param_4];
	ld.param.b32 	%r18, [TensorRemovePadding_param_5];
	ld.param.b32 	%r19, [TensorRemovePadding_param_7];
	ld.param.b32 	%r20, [TensorRemovePadding_param_8];
	ld.param.b32 	%r21, [TensorRemovePadding_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r40, %r1, %r22, %r23;
	setp.ge.u32 	%p1, %r40, %r15;
	@%p1 bra 	$L__BB50_5;
	sub.s32 	%r3, %r16, %r21;
	sub.s32 	%r4, %r17, %r21;
	mul.lo.s32 	%r5, %r17, %r16;
	mul.lo.s32 	%r6, %r5, %r18;
	mul.lo.s32 	%r7, %r20, %r19;
	mul.lo.s32 	%r8, %r7, %r18;
	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r24;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB50_2:
	div.u32 	%r12, %r40, %r16;
	mul.lo.s32 	%r25, %r12, %r16;
	sub.s32 	%r11, %r40, %r25;
	rem.u32 	%r13, %r12, %r17;
	setp.ge.u32 	%p2, %r11, %r3;
	min.u32 	%r26, %r11, %r13;
	setp.lt.u32 	%p3, %r26, %r21;
	setp.ge.u32 	%p4, %r13, %r4;
	or.pred 	%p5, %p2, %p4;
	or.pred 	%p6, %p5, %p3;
	@%p6 bra 	$L__BB50_4;
	div.u32 	%r27, %r12, %r17;
	div.u32 	%r28, %r27, %r18;
	mul.lo.s32 	%r29, %r28, %r18;
	sub.s32 	%r30, %r27, %r29;
	mul.lo.s32 	%r31, %r6, %r28;
	cvt.u64.u32 	%rd5, %r31;
	mul.lo.s32 	%r32, %r30, %r5;
	cvt.u64.u32 	%rd6, %r32;
	mad.lo.s32 	%r33, %r13, %r16, %r11;
	cvt.u64.u32 	%rd7, %r33;
	add.s64 	%rd8, %rd6, %rd7;
	add.s64 	%rd9, %rd8, %rd5;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.b32 	%r34, [%rd11];
	mul.lo.s32 	%r35, %r8, %r28;
	cvt.u64.u32 	%rd12, %r35;
	mul.lo.s32 	%r36, %r30, %r7;
	cvt.u64.u32 	%rd13, %r36;
	sub.s32 	%r37, %r13, %r21;
	sub.s32 	%r38, %r11, %r21;
	mad.lo.s32 	%r39, %r37, %r19, %r38;
	cvt.u64.u32 	%rd14, %r39;
	add.s64 	%rd15, %rd13, %rd14;
	add.s64 	%rd16, %rd15, %rd12;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	st.global.b32 	[%rd18], %r34;
$L__BB50_4:
	add.s32 	%r40, %r40, %r9;
	setp.lt.u32 	%p7, %r40, %r15;
	@%p7 bra 	$L__BB50_2;
$L__BB50_5:
	ret;

}
	// .globl	TensorIm2Col
.visible .entry TensorIm2Col(
	.param .u32 TensorIm2Col_param_0,
	.param .u64 .ptr .align 1 TensorIm2Col_param_1,
	.param .u64 .ptr .align 1 TensorIm2Col_param_2,
	.param .u64 .ptr .align 1 TensorIm2Col_param_3,
	.param .u64 .ptr .align 1 TensorIm2Col_param_4,
	.param .u32 TensorIm2Col_param_5,
	.param .u32 TensorIm2Col_param_6,
	.param .u32 TensorIm2Col_param_7,
	.param .u32 TensorIm2Col_param_8,
	.param .u32 TensorIm2Col_param_9,
	.param .u32 TensorIm2Col_param_10,
	.param .u32 TensorIm2Col_param_11,
	.param .u32 TensorIm2Col_param_12,
	.param .u32 TensorIm2Col_param_13,
	.param .u32 TensorIm2Col_param_14,
	.param .u32 TensorIm2Col_param_15
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<24>;

	ld.param.b32 	%r9, [TensorIm2Col_param_0];
	ld.param.b64 	%rd5, [TensorIm2Col_param_1];
	ld.param.b64 	%rd6, [TensorIm2Col_param_2];
	ld.param.b64 	%rd7, [TensorIm2Col_param_3];
	ld.param.b64 	%rd8, [TensorIm2Col_param_4];
	ld.param.b32 	%r10, [TensorIm2Col_param_5];
	ld.param.b32 	%r11, [TensorIm2Col_param_6];
	ld.param.b32 	%r12, [TensorIm2Col_param_7];
	ld.param.b32 	%r13, [TensorIm2Col_param_9];
	ld.param.b32 	%r14, [TensorIm2Col_param_10];
	ld.param.b32 	%r15, [TensorIm2Col_param_11];
	ld.param.b32 	%r16, [TensorIm2Col_param_12];
	ld.param.b32 	%r17, [TensorIm2Col_param_13];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r47, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r47, %r9;
	@%p1 bra 	$L__BB51_3;
	mul.lo.s32 	%r3, %r14, %r13;
	mul.lo.s32 	%r4, %r11, %r10;
	mul.lo.s32 	%r5, %r4, %r12;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r20;
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd5;
	cvta.to.global.u64 	%rd4, %rd6;
$L__BB51_2:
	div.u32 	%r21, %r47, %r16;
	mul.lo.s32 	%r22, %r21, %r16;
	sub.s32 	%r23, %r47, %r22;
	div.u32 	%r24, %r21, %r17;
	mul.lo.s32 	%r25, %r24, %r17;
	sub.s32 	%r26, %r21, %r25;
	div.u32 	%r27, %r24, %r12;
	mul.lo.s32 	%r28, %r27, %r12;
	sub.s32 	%r29, %r24, %r28;
	div.u32 	%r30, %r27, %r15;
	mul.lo.s32 	%r31, %r30, %r15;
	sub.s32 	%r32, %r27, %r31;
	mul.wide.u32 	%rd9, %r32, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.nc.b32 	%r33, [%rd10];
	cvt.rzi.ftz.u32.f32 	%r34, %r33;
	add.s64 	%rd11, %rd2, %rd9;
	ld.global.nc.b32 	%r35, [%rd11];
	cvt.rzi.ftz.u32.f32 	%r36, %r35;
	mad.lo.s32 	%r37, %r29, %r16, %r23;
	mad.lo.s32 	%r38, %r37, %r17, %r26;
	mul.lo.s32 	%r39, %r3, %r30;
	cvt.u64.u32 	%rd12, %r39;
	mul.lo.s32 	%r40, %r5, %r30;
	cvt.u64.u32 	%rd13, %r40;
	mul.lo.s32 	%r41, %r29, %r4;
	cvt.u64.u32 	%rd14, %r41;
	add.s64 	%rd15, %rd13, %rd14;
	add.s32 	%r42, %r34, %r23;
	add.s32 	%r43, %r36, %r26;
	mad.lo.s32 	%r44, %r42, %r10, %r43;
	cvt.u64.u32 	%rd16, %r44;
	add.s64 	%rd17, %rd15, %rd16;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.b32 	%r45, [%rd19];
	mad.lo.s32 	%r46, %r38, %r13, %r32;
	cvt.u64.u32 	%rd20, %r46;
	add.s64 	%rd21, %rd20, %rd12;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	st.global.b32 	[%rd23], %r45;
	add.s32 	%r47, %r47, %r6;
	setp.lt.u32 	%p2, %r47, %r9;
	@%p2 bra 	$L__BB51_2;
$L__BB51_3:
	ret;

}
	// .globl	TensorReverseIm2Col
.visible .entry TensorReverseIm2Col(
	.param .u32 TensorReverseIm2Col_param_0,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_1,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_2,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_3,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_4,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_5,
	.param .u32 TensorReverseIm2Col_param_6,
	.param .u32 TensorReverseIm2Col_param_7,
	.param .u32 TensorReverseIm2Col_param_8,
	.param .u32 TensorReverseIm2Col_param_9,
	.param .u32 TensorReverseIm2Col_param_10,
	.param .u32 TensorReverseIm2Col_param_11,
	.param .u32 TensorReverseIm2Col_param_12,
	.param .u32 TensorReverseIm2Col_param_13,
	.param .u32 TensorReverseIm2Col_param_14,
	.param .u32 TensorReverseIm2Col_param_15,
	.param .u32 TensorReverseIm2Col_param_16,
	.param .u32 TensorReverseIm2Col_param_17
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<37>;

	ld.param.b32 	%r20, [TensorReverseIm2Col_param_0];
	ld.param.b64 	%rd4, [TensorReverseIm2Col_param_1];
	ld.param.b64 	%rd5, [TensorReverseIm2Col_param_2];
	ld.param.b64 	%rd6, [TensorReverseIm2Col_param_3];
	ld.param.b32 	%r21, [TensorReverseIm2Col_param_6];
	ld.param.b32 	%r22, [TensorReverseIm2Col_param_7];
	ld.param.b32 	%r23, [TensorReverseIm2Col_param_8];
	ld.param.b32 	%r25, [TensorReverseIm2Col_param_11];
	ld.param.b32 	%r26, [TensorReverseIm2Col_param_12];
	ld.param.b32 	%r29, [TensorReverseIm2Col_param_15];
	ld.param.b32 	%r30, [TensorReverseIm2Col_param_16];
	ld.param.b32 	%r31, [TensorReverseIm2Col_param_17];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	mad.lo.s32 	%r68, %r1, %r32, %r33;
	setp.ge.u32 	%p1, %r68, %r20;
	@%p1 bra 	$L__BB52_5;
	mul.lo.s32 	%r3, %r22, %r21;
	mul.lo.s32 	%r4, %r3, %r23;
	mul.lo.s32 	%r5, %r26, %r25;
	mul.lo.s32 	%r6, %r5, %r31;
	mul.lo.s32 	%r7, %r30, %r29;
	mul.lo.s32 	%r8, %r7, %r31;
	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r34;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB52_2:
	ld.param.b32 	%r67, [TensorReverseIm2Col_param_14];
	ld.param.b32 	%r66, [TensorReverseIm2Col_param_13];
	ld.param.b32 	%r65, [TensorReverseIm2Col_param_10];
	ld.param.b64 	%rd36, [TensorReverseIm2Col_param_5];
	ld.param.b64 	%rd35, [TensorReverseIm2Col_param_4];
	div.u32 	%r11, %r68, %r31;
	div.u32 	%r12, %r11, %r25;
	div.u32 	%r35, %r12, %r26;
	div.u32 	%r36, %r35, %r65;
	mul.lo.s32 	%r37, %r36, %r65;
	sub.s32 	%r38, %r35, %r37;
	div.u32 	%r14, %r36, %r23;
	mul.lo.s32 	%r39, %r14, %r23;
	sub.s32 	%r13, %r36, %r39;
	cvta.to.global.u64 	%rd9, %rd35;
	mul.wide.u32 	%rd10, %r38, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.nc.b32 	%r40, [%rd11];
	cvt.rzi.ftz.u32.f32 	%r15, %r40;
	cvta.to.global.u64 	%rd12, %rd36;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.nc.b32 	%r41, [%rd13];
	cvt.rzi.ftz.u32.f32 	%r16, %r41;
	div.u32 	%r17, %r15, %r66;
	div.u32 	%r42, %r16, %r67;
	setp.ge.u32 	%p2, %r17, %r22;
	setp.ge.u32 	%p3, %r42, %r21;
	or.pred 	%p4, %p2, %p3;
	@%p4 bra 	$L__BB52_4;
	rem.u32 	%r43, %r68, %r31;
	rem.u32 	%r44, %r11, %r25;
	rem.u32 	%r45, %r12, %r26;
	mul.lo.s32 	%r46, %r5, %r43;
	cvt.u64.u32 	%rd14, %r46;
	mul.lo.s32 	%r47, %r6, %r13;
	cvt.u64.u32 	%rd15, %r47;
	mad.lo.s32 	%r48, %r17, %r21, %r42;
	cvt.u64.u32 	%rd16, %r48;
	mul.lo.s32 	%r49, %r3, %r13;
	cvt.u64.u32 	%rd17, %r49;
	mul.lo.s32 	%r50, %r4, %r14;
	cvt.u64.u32 	%rd18, %r50;
	add.s64 	%rd19, %rd18, %rd17;
	add.s64 	%rd20, %rd19, %rd16;
	shl.b64 	%rd21, %rd20, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.b32 	%r51, [%rd22];
	not.b32 	%r52, %r44;
	add.s32 	%r53, %r25, %r52;
	not.b32 	%r54, %r45;
	add.s32 	%r55, %r26, %r54;
	mad.lo.s32 	%r56, %r53, %r26, %r55;
	add.s32 	%r57, %r15, %r44;
	add.s32 	%r58, %r16, %r45;
	mad.lo.s32 	%r59, %r57, %r29, %r58;
	cvt.u64.u32 	%rd23, %r56;
	add.s64 	%rd24, %rd23, %rd14;
	add.s64 	%rd25, %rd24, %rd15;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd27, %rd2, %rd26;
	ld.global.nc.b32 	%r60, [%rd27];
	mul.ftz.f32 	%r61, %r51, %r60;
	cvt.u64.u32 	%rd28, %r59;
	mul.lo.s32 	%r62, %r7, %r43;
	cvt.u64.u32 	%rd29, %r62;
	mul.lo.s32 	%r63, %r8, %r14;
	cvt.u64.u32 	%rd30, %r63;
	add.s64 	%rd31, %rd30, %rd29;
	add.s64 	%rd32, %rd31, %rd28;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd3, %rd33;
	atom.global.add.f32 	%r64, [%rd34], %r61;
$L__BB52_4:
	add.s32 	%r68, %r68, %r9;
	setp.lt.u32 	%p5, %r68, %r20;
	@%p5 bra 	$L__BB52_2;
$L__BB52_5:
	ret;

}
	// .globl	SoftmaxDerivative
.visible .entry SoftmaxDerivative(
	.param .u64 .ptr .align 1 SoftmaxDerivative_param_0,
	.param .u64 .ptr .align 1 SoftmaxDerivative_param_1,
	.param .u32 SoftmaxDerivative_param_2,
	.param .u32 SoftmaxDerivative_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<11>;

	ld.param.b64 	%rd5, [SoftmaxDerivative_param_0];
	ld.param.b64 	%rd4, [SoftmaxDerivative_param_1];
	ld.param.b32 	%r16, [SoftmaxDerivative_param_2];
	ld.param.b32 	%r17, [SoftmaxDerivative_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r31, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r31, %r16;
	@%p1 bra 	$L__BB53_9;
	mov.u32 	%r20, %ntid.y;
	mov.u32 	%r21, %ctaid.y;
	mov.u32 	%r22, %tid.y;
	mad.lo.s32 	%r3, %r20, %r21, %r22;
	mov.u32 	%r23, %nctaid.y;
	mul.lo.s32 	%r4, %r20, %r23;
	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r24;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB53_2:
	setp.ge.u32 	%p2, %r3, %r16;
	@%p2 bra 	$L__BB53_8;
	mul.lo.s32 	%r25, %r31, %r17;
	mul.wide.u32 	%rd6, %r25, 4;
	add.s64 	%rd3, %rd1, %rd6;
	mov.b32 	%r32, %r3;
$L__BB53_4:
	setp.ne.s32 	%p3, %r31, %r32;
	@%p3 bra 	$L__BB53_6;
	ld.global.nc.b32 	%r33, [%rd3];
	mov.b32 	%r28, 0f3F800000;
	sub.ftz.f32 	%r34, %r28, %r33;
	bra.uni 	$L__BB53_7;
$L__BB53_6:
	ld.global.nc.b32 	%r26, [%rd3];
	neg.ftz.f32 	%r34, %r26;
	mul.lo.s32 	%r27, %r32, %r17;
	mul.wide.u32 	%rd7, %r27, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.b32 	%r33, [%rd8];
$L__BB53_7:
	mul.ftz.f32 	%r29, %r33, %r34;
	mad.lo.s32 	%r30, %r32, %r16, %r31;
	mul.wide.u32 	%rd9, %r30, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.b32 	[%rd10], %r29;
	add.s32 	%r32, %r32, %r4;
	setp.lt.u32 	%p4, %r32, %r16;
	@%p4 bra 	$L__BB53_4;
$L__BB53_8:
	add.s32 	%r31, %r31, %r5;
	setp.lt.u32 	%p5, %r31, %r16;
	@%p5 bra 	$L__BB53_2;
$L__BB53_9:
	ret;

}
	// .globl	RotateInPlace
.visible .entry RotateInPlace(
	.param .u64 .ptr .align 1 RotateInPlace_param_0,
	.param .u32 RotateInPlace_param_1,
	.param .u32 RotateInPlace_param_2,
	.param .u32 RotateInPlace_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<19>;
	.reg .b64 	%rd<7>;

	ld.param.b64 	%rd2, [RotateInPlace_param_0];
	ld.param.b32 	%r6, [RotateInPlace_param_1];
	ld.param.b32 	%r7, [RotateInPlace_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r18, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r18, %r6;
	@%p1 bra 	$L__BB54_3;
	cvta.to.global.u64 	%rd1, %rd2;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
$L__BB54_2:
	rem.u32 	%r11, %r18, %r7;
	sub.s32 	%r12, %r18, %r11;
	not.b32 	%r13, %r11;
	add.s32 	%r14, %r7, %r13;
	add.s32 	%r15, %r14, %r12;
	mul.wide.u32 	%rd3, %r15, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.b32 	%r16, [%rd4];
	mul.wide.u32 	%rd5, %r18, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.b32 	%r17, [%rd6];
	st.global.b32 	[%rd4], %r17;
	st.global.b32 	[%rd6], %r16;
	add.s32 	%r18, %r18, %r3;
	setp.lt.u32 	%p2, %r18, %r6;
	@%p2 bra 	$L__BB54_2;
$L__BB54_3:
	ret;

}
	// .globl	TensorMaxPool
.visible .entry TensorMaxPool(
	.param .u32 TensorMaxPool_param_0,
	.param .u64 .ptr .align 1 TensorMaxPool_param_1,
	.param .u64 .ptr .align 1 TensorMaxPool_param_2,
	.param .u64 .ptr .align 1 TensorMaxPool_param_3,
	.param .u64 .ptr .align 1 TensorMaxPool_param_4,
	.param .u64 .ptr .align 1 TensorMaxPool_param_5,
	.param .u32 TensorMaxPool_param_6,
	.param .u32 TensorMaxPool_param_7,
	.param .u32 TensorMaxPool_param_8,
	.param .u32 TensorMaxPool_param_9,
	.param .u32 TensorMaxPool_param_10,
	.param .u32 TensorMaxPool_param_11,
	.param .u32 TensorMaxPool_param_12,
	.param .u32 TensorMaxPool_param_13,
	.param .u32 TensorMaxPool_param_14,
	.param .u32 TensorMaxPool_param_15,
	.param .u32 TensorMaxPool_param_16,
	.param .u32 TensorMaxPool_param_17
)
{
	.reg .pred 	%p<121>;
	.reg .b32 	%r<481>;
	.reg .b64 	%rd<190>;

	ld.param.b32 	%r126, [TensorMaxPool_param_0];
	ld.param.b64 	%rd9, [TensorMaxPool_param_1];
	ld.param.b64 	%rd10, [TensorMaxPool_param_2];
	ld.param.b64 	%rd8, [TensorMaxPool_param_3];
	ld.param.b64 	%rd11, [TensorMaxPool_param_4];
	ld.param.b64 	%rd12, [TensorMaxPool_param_5];
	ld.param.b32 	%r127, [TensorMaxPool_param_6];
	ld.param.b32 	%r129, [TensorMaxPool_param_8];
	ld.param.b32 	%r131, [TensorMaxPool_param_11];
	ld.param.b32 	%r132, [TensorMaxPool_param_12];
	ld.param.b32 	%r134, [TensorMaxPool_param_14];
	ld.param.b32 	%r135, [TensorMaxPool_param_15];
	ld.param.b32 	%r136, [TensorMaxPool_param_16];
	ld.param.b32 	%r137, [TensorMaxPool_param_17];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd12;
	cvta.to.global.u64 	%rd5, %rd11;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r138, %ctaid.x;
	mov.u32 	%r139, %tid.x;
	mad.lo.s32 	%r2, %r1, %r138, %r139;
	setp.ge.u32 	%p1, %r2, %r126;
	@%p1 bra 	$L__BB55_40;
	ld.param.b32 	%r415, [TensorMaxPool_param_13];
	ld.param.b32 	%r408, [TensorMaxPool_param_7];
	mul.lo.s32 	%r3, %r132, %r131;
	mul.lo.s32 	%r4, %r129, %r408;
	setp.ne.s32 	%p2, %r415, 0;
	mov.u32 	%r140, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r140;
	@%p2 bra 	$L__BB55_5;
	setp.eq.s32 	%p118, %r137, 0;
	@%p118 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_4;
$L__BB55_3:
	div.u32 	%r397, %r2, %r127;
	mul.lo.s32 	%r398, %r397, %r127;
	sub.s32 	%r399, %r2, %r398;
	mul.wide.u32 	%rd181, %r399, 4;
	add.s64 	%rd182, %rd5, %rd181;
	ld.global.nc.b32 	%r400, [%rd182];
	cvt.rzi.ftz.u32.f32 	%r401, %r400;
	add.s64 	%rd183, %rd4, %rd181;
	ld.global.nc.b32 	%r402, [%rd183];
	cvt.rzi.ftz.u32.f32 	%r403, %r402;
	div.u32 	%r404, %r401, %r135;
	div.u32 	%r405, %r403, %r136;
	mul.lo.s32 	%r406, %r3, %r397;
	cvt.u64.u32 	%rd184, %r406;
	mad.lo.s32 	%r407, %r404, %r131, %r405;
	cvt.u64.u32 	%rd185, %r407;
	add.s64 	%rd186, %rd184, %rd185;
	shl.b64 	%rd187, %rd186, 2;
	add.s64 	%rd188, %rd2, %rd187;
	st.global.b32 	[%rd188], 0;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p120, %r2, %r126;
	@%p120 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_40;
$L__BB55_4:
	div.u32 	%r386, %r2, %r127;
	mul.lo.s32 	%r387, %r386, %r127;
	sub.s32 	%r388, %r2, %r387;
	mul.wide.u32 	%rd172, %r388, 4;
	add.s64 	%rd173, %rd5, %rd172;
	ld.global.nc.b32 	%r389, [%rd173];
	cvt.rzi.ftz.u32.f32 	%r390, %r389;
	add.s64 	%rd174, %rd4, %rd172;
	ld.global.nc.b32 	%r391, [%rd174];
	cvt.rzi.ftz.u32.f32 	%r392, %r391;
	div.u32 	%r393, %r390, %r135;
	div.u32 	%r394, %r392, %r136;
	mul.lo.s32 	%r395, %r3, %r386;
	cvt.u64.u32 	%rd175, %r395;
	mad.lo.s32 	%r396, %r393, %r131, %r394;
	cvt.u64.u32 	%rd176, %r396;
	add.s64 	%rd177, %rd175, %rd176;
	shl.b64 	%rd178, %rd177, 2;
	add.s64 	%rd179, %rd3, %rd178;
	st.global.b32 	[%rd179], -1082130432;
	add.s64 	%rd180, %rd2, %rd178;
	st.global.b32 	[%rd180], 0;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p119, %r2, %r126;
	@%p119 bra 	$L__BB55_4;
	bra.uni 	$L__BB55_40;
$L__BB55_5:
	setp.ne.s32 	%p3, %r134, 0;
	@%p3 bra 	$L__BB55_9;
	setp.eq.s32 	%p115, %r137, 0;
	@%p115 bra 	$L__BB55_7;
	bra.uni 	$L__BB55_8;
$L__BB55_7:
	div.u32 	%r375, %r2, %r127;
	mul.lo.s32 	%r376, %r375, %r127;
	sub.s32 	%r377, %r2, %r376;
	mul.wide.u32 	%rd164, %r377, 4;
	add.s64 	%rd165, %rd5, %rd164;
	ld.global.nc.b32 	%r378, [%rd165];
	cvt.rzi.ftz.u32.f32 	%r379, %r378;
	add.s64 	%rd166, %rd4, %rd164;
	ld.global.nc.b32 	%r380, [%rd166];
	cvt.rzi.ftz.u32.f32 	%r381, %r380;
	div.u32 	%r382, %r379, %r135;
	div.u32 	%r383, %r381, %r136;
	mul.lo.s32 	%r384, %r3, %r375;
	cvt.u64.u32 	%rd167, %r384;
	mad.lo.s32 	%r385, %r382, %r131, %r383;
	cvt.u64.u32 	%rd168, %r385;
	add.s64 	%rd169, %rd167, %rd168;
	shl.b64 	%rd170, %rd169, 2;
	add.s64 	%rd171, %rd2, %rd170;
	st.global.b32 	[%rd171], 0;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p117, %r2, %r126;
	@%p117 bra 	$L__BB55_7;
	bra.uni 	$L__BB55_40;
$L__BB55_8:
	div.u32 	%r364, %r2, %r127;
	mul.lo.s32 	%r365, %r364, %r127;
	sub.s32 	%r366, %r2, %r365;
	mul.wide.u32 	%rd155, %r366, 4;
	add.s64 	%rd156, %rd5, %rd155;
	ld.global.nc.b32 	%r367, [%rd156];
	cvt.rzi.ftz.u32.f32 	%r368, %r367;
	add.s64 	%rd157, %rd4, %rd155;
	ld.global.nc.b32 	%r369, [%rd157];
	cvt.rzi.ftz.u32.f32 	%r370, %r369;
	div.u32 	%r371, %r368, %r135;
	div.u32 	%r372, %r370, %r136;
	mul.lo.s32 	%r373, %r3, %r364;
	cvt.u64.u32 	%rd158, %r373;
	mad.lo.s32 	%r374, %r371, %r131, %r372;
	cvt.u64.u32 	%rd159, %r374;
	add.s64 	%rd160, %rd158, %rd159;
	shl.b64 	%rd161, %rd160, 2;
	add.s64 	%rd162, %rd3, %rd161;
	st.global.b32 	[%rd162], -1082130432;
	add.s64 	%rd163, %rd2, %rd161;
	st.global.b32 	[%rd163], 0;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p116, %r2, %r126;
	@%p116 bra 	$L__BB55_8;
	bra.uni 	$L__BB55_40;
$L__BB55_9:
	setp.ne.s32 	%p4, %r137, 0;
	@%p4 bra 	$L__BB55_25;
	and.b32 	%r14, %r134, 7;
	and.b32 	%r15, %r134, -8;
$L__BB55_11:
	ld.param.b32 	%r413, [TensorMaxPool_param_9];
	div.u32 	%r256, %r2, %r127;
	mul.lo.s32 	%r257, %r256, %r127;
	sub.s32 	%r258, %r2, %r257;
	div.u32 	%r18, %r256, %r413;
	mul.lo.s32 	%r259, %r18, %r413;
	sub.s32 	%r17, %r256, %r259;
	mul.wide.u32 	%rd85, %r258, 4;
	add.s64 	%rd86, %rd5, %rd85;
	ld.global.nc.b32 	%r260, [%rd86];
	cvt.rzi.ftz.u32.f32 	%r19, %r260;
	add.s64 	%rd87, %rd4, %rd85;
	ld.global.nc.b32 	%r261, [%rd87];
	cvt.rzi.ftz.u32.f32 	%r20, %r261;
	mul.lo.s32 	%r262, %r4, %r413;
	mul.lo.s32 	%r263, %r262, %r18;
	cvt.u64.u32 	%rd88, %r263;
	mul.lo.s32 	%r264, %r4, %r17;
	cvt.u64.u32 	%rd89, %r264;
	add.s64 	%rd6, %rd88, %rd89;
	mov.b32 	%r450, 0f00000000;
	mov.b32 	%r451, -1;
	mov.b32 	%r423, 0;
	mov.b32 	%r424, %r423;
$L__BB55_12:
	ld.param.b32 	%r410, [TensorMaxPool_param_7];
	setp.lt.u32 	%p60, %r134, 8;
	add.s32 	%r268, %r423, %r19;
	mul.lo.s32 	%r25, %r268, %r410;
	add.s32 	%r26, %r25, %r20;
	mov.b32 	%r440, 0;
	mov.b32 	%r435, %r424;
	@%p60 bra 	$L__BB55_15;
	and.b32 	%r269, %r134, -8;
	neg.s32 	%r428, %r269;
	add.s32 	%r270, %r20, %r25;
	add.s32 	%r427, %r270, 3;
	mov.b32 	%r435, %r424;
$L__BB55_14:
	.pragma "nounroll";
	add.s32 	%r271, %r427, -3;
	cvt.u64.u32 	%rd90, %r271;
	add.s64 	%rd91, %rd6, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.nc.b32 	%r272, [%rd93];
	setp.lt.s32 	%p61, %r451, 0;
	setp.gt.ftz.f32 	%p62, %r272, %r450;
	or.pred 	%p63, %p61, %p62;
	selp.f32 	%r274, %r272, %r450, %p63;
	selp.b32 	%r275, %r435, %r451, %p63;
	add.s32 	%r276, %r435, 1;
	add.s32 	%r277, %r427, -2;
	cvt.u64.u32 	%rd94, %r277;
	add.s64 	%rd95, %rd6, %rd94;
	shl.b64 	%rd96, %rd95, 2;
	add.s64 	%rd97, %rd1, %rd96;
	ld.global.nc.b32 	%r278, [%rd97];
	setp.lt.s32 	%p64, %r275, 0;
	setp.gt.ftz.f32 	%p65, %r278, %r274;
	or.pred 	%p66, %p64, %p65;
	selp.f32 	%r280, %r278, %r274, %p66;
	selp.b32 	%r281, %r276, %r275, %p66;
	add.s32 	%r282, %r427, -1;
	cvt.u64.u32 	%rd98, %r282;
	add.s64 	%rd99, %rd6, %rd98;
	shl.b64 	%rd100, %rd99, 2;
	add.s64 	%rd101, %rd1, %rd100;
	ld.global.nc.b32 	%r283, [%rd101];
	setp.lt.s32 	%p67, %r281, 0;
	setp.gt.ftz.f32 	%p68, %r283, %r280;
	or.pred 	%p69, %p67, %p68;
	selp.f32 	%r285, %r283, %r280, %p69;
	add.s32 	%r286, %r435, 2;
	selp.b32 	%r287, %r286, %r281, %p69;
	add.s32 	%r289, %r427, 4;
	cvt.u64.u32 	%rd102, %r427;
	add.s64 	%rd103, %rd6, %rd102;
	shl.b64 	%rd104, %rd103, 2;
	add.s64 	%rd105, %rd1, %rd104;
	ld.global.nc.b32 	%r290, [%rd105];
	setp.lt.s32 	%p70, %r287, 0;
	setp.gt.ftz.f32 	%p71, %r290, %r285;
	or.pred 	%p72, %p70, %p71;
	selp.f32 	%r292, %r290, %r285, %p72;
	add.s32 	%r293, %r435, 3;
	selp.b32 	%r294, %r293, %r287, %p72;
	add.s32 	%r296, %r427, 1;
	cvt.u64.u32 	%rd106, %r296;
	add.s64 	%rd107, %rd6, %rd106;
	shl.b64 	%rd108, %rd107, 2;
	add.s64 	%rd109, %rd1, %rd108;
	ld.global.nc.b32 	%r297, [%rd109];
	setp.lt.s32 	%p73, %r294, 0;
	setp.gt.ftz.f32 	%p74, %r297, %r292;
	or.pred 	%p75, %p73, %p74;
	selp.f32 	%r299, %r297, %r292, %p75;
	add.s32 	%r300, %r435, 4;
	selp.b32 	%r301, %r300, %r294, %p75;
	add.s32 	%r303, %r427, 2;
	cvt.u64.u32 	%rd110, %r303;
	add.s64 	%rd111, %rd6, %rd110;
	shl.b64 	%rd112, %rd111, 2;
	add.s64 	%rd113, %rd1, %rd112;
	ld.global.nc.b32 	%r304, [%rd113];
	setp.lt.s32 	%p76, %r301, 0;
	setp.gt.ftz.f32 	%p77, %r304, %r299;
	or.pred 	%p78, %p76, %p77;
	selp.f32 	%r306, %r304, %r299, %p78;
	add.s32 	%r307, %r435, 5;
	selp.b32 	%r308, %r307, %r301, %p78;
	add.s32 	%r310, %r427, 3;
	cvt.u64.u32 	%rd114, %r310;
	add.s64 	%rd115, %rd6, %rd114;
	shl.b64 	%rd116, %rd115, 2;
	add.s64 	%rd117, %rd1, %rd116;
	ld.global.nc.b32 	%r311, [%rd117];
	setp.lt.s32 	%p79, %r308, 0;
	setp.gt.ftz.f32 	%p80, %r311, %r306;
	or.pred 	%p81, %p79, %p80;
	selp.f32 	%r313, %r311, %r306, %p81;
	add.s32 	%r314, %r435, 6;
	selp.b32 	%r315, %r314, %r308, %p81;
	cvt.u64.u32 	%rd118, %r289;
	add.s64 	%rd119, %rd6, %rd118;
	shl.b64 	%rd120, %rd119, 2;
	add.s64 	%rd121, %rd1, %rd120;
	ld.global.nc.b32 	%r316, [%rd121];
	setp.lt.s32 	%p82, %r315, 0;
	setp.gt.ftz.f32 	%p83, %r316, %r313;
	or.pred 	%p84, %p82, %p83;
	selp.f32 	%r450, %r316, %r313, %p84;
	add.s32 	%r318, %r435, 7;
	selp.b32 	%r451, %r318, %r315, %p84;
	add.s32 	%r435, %r435, 8;
	add.s32 	%r428, %r428, 8;
	add.s32 	%r427, %r427, 8;
	setp.eq.s32 	%p85, %r428, 0;
	mov.b32 	%r440, %r15;
	@%p85 bra 	$L__BB55_15;
	bra.uni 	$L__BB55_14;
$L__BB55_15:
	setp.eq.s32 	%p86, %r14, 0;
	@%p86 bra 	$L__BB55_23;
	add.s32 	%r321, %r14, -1;
	setp.lt.u32 	%p87, %r321, 3;
	@%p87 bra 	$L__BB55_18;
	add.s32 	%r322, %r26, %r440;
	cvt.u64.u32 	%rd122, %r322;
	add.s64 	%rd123, %rd6, %rd122;
	shl.b64 	%rd124, %rd123, 2;
	add.s64 	%rd125, %rd1, %rd124;
	ld.global.nc.b32 	%r323, [%rd125];
	setp.lt.s32 	%p88, %r451, 0;
	setp.gt.ftz.f32 	%p89, %r323, %r450;
	or.pred 	%p90, %p88, %p89;
	selp.f32 	%r325, %r323, %r450, %p90;
	selp.b32 	%r326, %r435, %r451, %p90;
	add.s32 	%r327, %r435, 1;
	add.s32 	%r328, %r322, 1;
	cvt.u64.u32 	%rd126, %r328;
	add.s64 	%rd127, %rd6, %rd126;
	shl.b64 	%rd128, %rd127, 2;
	add.s64 	%rd129, %rd1, %rd128;
	ld.global.nc.b32 	%r329, [%rd129];
	setp.lt.s32 	%p91, %r326, 0;
	setp.gt.ftz.f32 	%p92, %r329, %r325;
	or.pred 	%p93, %p91, %p92;
	selp.f32 	%r331, %r329, %r325, %p93;
	selp.b32 	%r332, %r327, %r326, %p93;
	add.s32 	%r333, %r435, 2;
	add.s32 	%r334, %r322, 2;
	cvt.u64.u32 	%rd130, %r334;
	add.s64 	%rd131, %rd6, %rd130;
	shl.b64 	%rd132, %rd131, 2;
	add.s64 	%rd133, %rd1, %rd132;
	ld.global.nc.b32 	%r335, [%rd133];
	setp.lt.s32 	%p94, %r332, 0;
	setp.gt.ftz.f32 	%p95, %r335, %r331;
	or.pred 	%p96, %p94, %p95;
	selp.f32 	%r337, %r335, %r331, %p96;
	selp.b32 	%r338, %r333, %r332, %p96;
	add.s32 	%r339, %r435, 3;
	add.s32 	%r340, %r322, 3;
	cvt.u64.u32 	%rd134, %r340;
	add.s64 	%rd135, %rd6, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.nc.b32 	%r341, [%rd137];
	setp.lt.s32 	%p97, %r338, 0;
	setp.gt.ftz.f32 	%p98, %r341, %r337;
	or.pred 	%p99, %p97, %p98;
	selp.f32 	%r450, %r341, %r337, %p99;
	selp.b32 	%r451, %r339, %r338, %p99;
	add.s32 	%r435, %r435, 4;
	add.s32 	%r440, %r440, 4;
$L__BB55_18:
	and.b32 	%r345, %r134, 3;
	setp.eq.s32 	%p100, %r345, 0;
	@%p100 bra 	$L__BB55_23;
	setp.eq.s32 	%p101, %r345, 1;
	@%p101 bra 	$L__BB55_21;
	add.s32 	%r346, %r26, %r440;
	cvt.u64.u32 	%rd138, %r346;
	add.s64 	%rd139, %rd6, %rd138;
	shl.b64 	%rd140, %rd139, 2;
	add.s64 	%rd141, %rd1, %rd140;
	ld.global.nc.b32 	%r347, [%rd141];
	setp.lt.s32 	%p102, %r451, 0;
	setp.gt.ftz.f32 	%p103, %r347, %r450;
	or.pred 	%p104, %p102, %p103;
	selp.f32 	%r349, %r347, %r450, %p104;
	selp.b32 	%r350, %r435, %r451, %p104;
	add.s32 	%r351, %r435, 1;
	add.s32 	%r352, %r346, 1;
	cvt.u64.u32 	%rd142, %r352;
	add.s64 	%rd143, %rd6, %rd142;
	shl.b64 	%rd144, %rd143, 2;
	add.s64 	%rd145, %rd1, %rd144;
	ld.global.nc.b32 	%r353, [%rd145];
	setp.lt.s32 	%p105, %r350, 0;
	setp.gt.ftz.f32 	%p106, %r353, %r349;
	or.pred 	%p107, %p105, %p106;
	selp.f32 	%r450, %r353, %r349, %p107;
	selp.b32 	%r451, %r351, %r350, %p107;
	add.s32 	%r435, %r435, 2;
	add.s32 	%r440, %r440, 2;
$L__BB55_21:
	and.b32 	%r355, %r134, 1;
	setp.ne.b32 	%p108, %r355, 0;
	not.pred 	%p109, %p108;
	@%p109 bra 	$L__BB55_23;
	add.s32 	%r356, %r26, %r440;
	cvt.u64.u32 	%rd146, %r356;
	add.s64 	%rd147, %rd6, %rd146;
	shl.b64 	%rd148, %rd147, 2;
	add.s64 	%rd149, %rd1, %rd148;
	ld.global.nc.b32 	%r357, [%rd149];
	setp.lt.s32 	%p110, %r451, 0;
	setp.gt.ftz.f32 	%p111, %r357, %r450;
	or.pred 	%p112, %p110, %p111;
	selp.f32 	%r450, %r357, %r450, %p112;
	selp.b32 	%r451, %r435, %r451, %p112;
$L__BB55_23:
	ld.param.b32 	%r417, [TensorMaxPool_param_13];
	add.s32 	%r423, %r423, 1;
	setp.ne.s32 	%p113, %r423, %r417;
	add.s32 	%r424, %r134, %r424;
	@%p113 bra 	$L__BB55_12;
	ld.param.b32 	%r414, [TensorMaxPool_param_9];
	div.u32 	%r359, %r19, %r135;
	div.u32 	%r360, %r20, %r136;
	mad.lo.s32 	%r361, %r359, %r131, %r360;
	cvt.u64.u32 	%rd150, %r361;
	mad.lo.s32 	%r362, %r18, %r414, %r17;
	mul.lo.s32 	%r363, %r3, %r362;
	cvt.u64.u32 	%rd151, %r363;
	add.s64 	%rd152, %rd151, %rd150;
	shl.b64 	%rd153, %rd152, 2;
	add.s64 	%rd154, %rd2, %rd153;
	st.global.b32 	[%rd154], %r450;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p114, %r2, %r126;
	@%p114 bra 	$L__BB55_11;
	bra.uni 	$L__BB55_40;
$L__BB55_25:
	and.b32 	%r73, %r134, 7;
	and.b32 	%r74, %r134, -8;
$L__BB55_26:
	ld.param.b32 	%r411, [TensorMaxPool_param_9];
	div.u32 	%r144, %r2, %r127;
	mul.lo.s32 	%r145, %r144, %r127;
	sub.s32 	%r146, %r2, %r145;
	div.u32 	%r77, %r144, %r411;
	mul.lo.s32 	%r147, %r77, %r411;
	sub.s32 	%r76, %r144, %r147;
	mul.wide.u32 	%rd13, %r146, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.nc.b32 	%r148, [%rd14];
	cvt.rzi.ftz.u32.f32 	%r78, %r148;
	add.s64 	%rd15, %rd4, %rd13;
	ld.global.nc.b32 	%r149, [%rd15];
	cvt.rzi.ftz.u32.f32 	%r79, %r149;
	mul.lo.s32 	%r150, %r4, %r411;
	mul.lo.s32 	%r151, %r150, %r77;
	cvt.u64.u32 	%rd16, %r151;
	mul.lo.s32 	%r152, %r4, %r76;
	cvt.u64.u32 	%rd17, %r152;
	add.s64 	%rd7, %rd16, %rd17;
	mov.b32 	%r479, 0f00000000;
	mov.b32 	%r480, -1;
	mov.b32 	%r453, 0;
	mov.b32 	%r454, %r453;
$L__BB55_27:
	ld.param.b32 	%r409, [TensorMaxPool_param_7];
	setp.lt.u32 	%p5, %r134, 8;
	add.s32 	%r156, %r453, %r78;
	mad.lo.s32 	%r84, %r156, %r409, %r79;
	mov.b32 	%r469, 0;
	mov.b32 	%r464, %r454;
	@%p5 bra 	$L__BB55_30;
	mov.b32 	%r457, 0;
	mov.b32 	%r464, %r454;
$L__BB55_29:
	.pragma "nounroll";
	add.s32 	%r158, %r84, %r457;
	cvt.u64.u32 	%rd18, %r158;
	add.s64 	%rd19, %rd7, %rd18;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.b32 	%r159, [%rd21];
	setp.lt.s32 	%p6, %r480, 0;
	setp.gt.ftz.f32 	%p7, %r159, %r479;
	or.pred 	%p8, %p6, %p7;
	selp.f32 	%r161, %r159, %r479, %p8;
	selp.b32 	%r162, %r464, %r480, %p8;
	add.s32 	%r163, %r464, 1;
	add.s32 	%r164, %r158, 1;
	cvt.u64.u32 	%rd22, %r164;
	add.s64 	%rd23, %rd7, %rd22;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.b32 	%r165, [%rd25];
	setp.lt.s32 	%p9, %r162, 0;
	setp.gt.ftz.f32 	%p10, %r165, %r161;
	or.pred 	%p11, %p9, %p10;
	selp.f32 	%r167, %r165, %r161, %p11;
	selp.b32 	%r168, %r163, %r162, %p11;
	add.s32 	%r169, %r464, 2;
	add.s32 	%r170, %r158, 2;
	cvt.u64.u32 	%rd26, %r170;
	add.s64 	%rd27, %rd7, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.b32 	%r171, [%rd29];
	setp.lt.s32 	%p12, %r168, 0;
	setp.gt.ftz.f32 	%p13, %r171, %r167;
	or.pred 	%p14, %p12, %p13;
	selp.f32 	%r173, %r171, %r167, %p14;
	selp.b32 	%r174, %r169, %r168, %p14;
	add.s32 	%r176, %r464, 3;
	add.s32 	%r177, %r158, 3;
	cvt.u64.u32 	%rd30, %r177;
	add.s64 	%rd31, %rd7, %rd30;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.b32 	%r178, [%rd33];
	setp.lt.s32 	%p15, %r174, 0;
	setp.gt.ftz.f32 	%p16, %r178, %r173;
	or.pred 	%p17, %p15, %p16;
	selp.f32 	%r180, %r178, %r173, %p17;
	selp.b32 	%r181, %r176, %r174, %p17;
	add.s32 	%r183, %r464, 4;
	add.s32 	%r184, %r158, 4;
	cvt.u64.u32 	%rd34, %r184;
	add.s64 	%rd35, %rd7, %rd34;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.nc.b32 	%r185, [%rd37];
	setp.lt.s32 	%p18, %r181, 0;
	setp.gt.ftz.f32 	%p19, %r185, %r180;
	or.pred 	%p20, %p18, %p19;
	selp.f32 	%r187, %r185, %r180, %p20;
	selp.b32 	%r188, %r183, %r181, %p20;
	add.s32 	%r190, %r464, 5;
	add.s32 	%r191, %r158, 5;
	cvt.u64.u32 	%rd38, %r191;
	add.s64 	%rd39, %rd7, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.b32 	%r192, [%rd41];
	setp.lt.s32 	%p21, %r188, 0;
	setp.gt.ftz.f32 	%p22, %r192, %r187;
	or.pred 	%p23, %p21, %p22;
	selp.f32 	%r194, %r192, %r187, %p23;
	selp.b32 	%r195, %r190, %r188, %p23;
	add.s32 	%r197, %r464, 6;
	add.s32 	%r198, %r158, 6;
	cvt.u64.u32 	%rd42, %r198;
	add.s64 	%rd43, %rd7, %rd42;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.b32 	%r199, [%rd45];
	setp.lt.s32 	%p24, %r195, 0;
	setp.gt.ftz.f32 	%p25, %r199, %r194;
	or.pred 	%p26, %p24, %p25;
	selp.f32 	%r201, %r199, %r194, %p26;
	selp.b32 	%r202, %r197, %r195, %p26;
	add.s32 	%r203, %r464, 7;
	add.s32 	%r204, %r158, 7;
	cvt.u64.u32 	%rd46, %r204;
	add.s64 	%rd47, %rd7, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	ld.global.nc.b32 	%r205, [%rd49];
	setp.lt.s32 	%p27, %r202, 0;
	setp.gt.ftz.f32 	%p28, %r205, %r201;
	or.pred 	%p29, %p27, %p28;
	selp.f32 	%r479, %r205, %r201, %p29;
	selp.b32 	%r480, %r203, %r202, %p29;
	add.s32 	%r464, %r464, 8;
	add.s32 	%r457, %r457, 8;
	setp.ne.s32 	%p30, %r457, %r74;
	mov.b32 	%r469, %r74;
	@%p30 bra 	$L__BB55_29;
$L__BB55_30:
	setp.eq.s32 	%p31, %r73, 0;
	@%p31 bra 	$L__BB55_38;
	add.s32 	%r209, %r73, -1;
	setp.lt.u32 	%p32, %r209, 3;
	@%p32 bra 	$L__BB55_33;
	add.s32 	%r210, %r84, %r469;
	cvt.u64.u32 	%rd50, %r210;
	add.s64 	%rd51, %rd7, %rd50;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd53, %rd1, %rd52;
	ld.global.nc.b32 	%r211, [%rd53];
	setp.lt.s32 	%p33, %r480, 0;
	setp.gt.ftz.f32 	%p34, %r211, %r479;
	or.pred 	%p35, %p33, %p34;
	selp.f32 	%r213, %r211, %r479, %p35;
	selp.b32 	%r214, %r464, %r480, %p35;
	add.s32 	%r215, %r464, 1;
	add.s32 	%r216, %r210, 1;
	cvt.u64.u32 	%rd54, %r216;
	add.s64 	%rd55, %rd7, %rd54;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.nc.b32 	%r217, [%rd57];
	setp.lt.s32 	%p36, %r214, 0;
	setp.gt.ftz.f32 	%p37, %r217, %r213;
	or.pred 	%p38, %p36, %p37;
	selp.f32 	%r219, %r217, %r213, %p38;
	selp.b32 	%r220, %r215, %r214, %p38;
	add.s32 	%r221, %r464, 2;
	add.s32 	%r222, %r210, 2;
	cvt.u64.u32 	%rd58, %r222;
	add.s64 	%rd59, %rd7, %rd58;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd1, %rd60;
	ld.global.nc.b32 	%r223, [%rd61];
	setp.lt.s32 	%p39, %r220, 0;
	setp.gt.ftz.f32 	%p40, %r223, %r219;
	or.pred 	%p41, %p39, %p40;
	selp.f32 	%r225, %r223, %r219, %p41;
	selp.b32 	%r226, %r221, %r220, %p41;
	add.s32 	%r227, %r464, 3;
	add.s32 	%r228, %r210, 3;
	cvt.u64.u32 	%rd62, %r228;
	add.s64 	%rd63, %rd7, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	ld.global.nc.b32 	%r229, [%rd65];
	setp.lt.s32 	%p42, %r226, 0;
	setp.gt.ftz.f32 	%p43, %r229, %r225;
	or.pred 	%p44, %p42, %p43;
	selp.f32 	%r479, %r229, %r225, %p44;
	selp.b32 	%r480, %r227, %r226, %p44;
	add.s32 	%r464, %r464, 4;
	add.s32 	%r469, %r469, 4;
$L__BB55_33:
	and.b32 	%r233, %r134, 3;
	setp.eq.s32 	%p45, %r233, 0;
	@%p45 bra 	$L__BB55_38;
	setp.eq.s32 	%p46, %r233, 1;
	@%p46 bra 	$L__BB55_36;
	add.s32 	%r234, %r84, %r469;
	cvt.u64.u32 	%rd66, %r234;
	add.s64 	%rd67, %rd7, %rd66;
	shl.b64 	%rd68, %rd67, 2;
	add.s64 	%rd69, %rd1, %rd68;
	ld.global.nc.b32 	%r235, [%rd69];
	setp.lt.s32 	%p47, %r480, 0;
	setp.gt.ftz.f32 	%p48, %r235, %r479;
	or.pred 	%p49, %p47, %p48;
	selp.f32 	%r237, %r235, %r479, %p49;
	selp.b32 	%r238, %r464, %r480, %p49;
	add.s32 	%r239, %r464, 1;
	add.s32 	%r240, %r234, 1;
	cvt.u64.u32 	%rd70, %r240;
	add.s64 	%rd71, %rd7, %rd70;
	shl.b64 	%rd72, %rd71, 2;
	add.s64 	%rd73, %rd1, %rd72;
	ld.global.nc.b32 	%r241, [%rd73];
	setp.lt.s32 	%p50, %r238, 0;
	setp.gt.ftz.f32 	%p51, %r241, %r237;
	or.pred 	%p52, %p50, %p51;
	selp.f32 	%r479, %r241, %r237, %p52;
	selp.b32 	%r480, %r239, %r238, %p52;
	add.s32 	%r464, %r464, 2;
	add.s32 	%r469, %r469, 2;
$L__BB55_36:
	and.b32 	%r243, %r134, 1;
	setp.ne.b32 	%p53, %r243, 0;
	not.pred 	%p54, %p53;
	@%p54 bra 	$L__BB55_38;
	add.s32 	%r244, %r84, %r469;
	cvt.u64.u32 	%rd74, %r244;
	add.s64 	%rd75, %rd7, %rd74;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd77, %rd1, %rd76;
	ld.global.nc.b32 	%r245, [%rd77];
	setp.lt.s32 	%p55, %r480, 0;
	setp.gt.ftz.f32 	%p56, %r245, %r479;
	or.pred 	%p57, %p55, %p56;
	selp.f32 	%r479, %r245, %r479, %p57;
	selp.b32 	%r480, %r464, %r480, %p57;
$L__BB55_38:
	ld.param.b32 	%r416, [TensorMaxPool_param_13];
	add.s32 	%r453, %r453, 1;
	setp.ne.s32 	%p58, %r453, %r416;
	add.s32 	%r454, %r134, %r454;
	@%p58 bra 	$L__BB55_27;
	ld.param.b32 	%r412, [TensorMaxPool_param_9];
	ld.param.b64 	%rd189, [TensorMaxPool_param_3];
	cvt.rn.f32.s32 	%r247, %r480;
	div.u32 	%r248, %r78, %r135;
	div.u32 	%r249, %r79, %r136;
	mad.lo.s32 	%r250, %r248, %r131, %r249;
	cvt.u64.u32 	%rd78, %r250;
	mad.lo.s32 	%r251, %r77, %r412, %r76;
	mul.lo.s32 	%r252, %r3, %r251;
	cvt.u64.u32 	%rd79, %r252;
	add.s64 	%rd80, %rd79, %rd78;
	cvta.to.global.u64 	%rd81, %rd189;
	shl.b64 	%rd82, %rd80, 2;
	add.s64 	%rd83, %rd81, %rd82;
	st.global.b32 	[%rd83], %r247;
	add.s64 	%rd84, %rd2, %rd82;
	st.global.b32 	[%rd84], %r479;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p59, %r2, %r126;
	@%p59 bra 	$L__BB55_26;
$L__BB55_40:
	ret;

}
	// .globl	TensorReverseMaxPool
.visible .entry TensorReverseMaxPool(
	.param .u32 TensorReverseMaxPool_param_0,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_1,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_2,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_3,
	.param .u32 TensorReverseMaxPool_param_4,
	.param .u32 TensorReverseMaxPool_param_5,
	.param .u32 TensorReverseMaxPool_param_6,
	.param .u32 TensorReverseMaxPool_param_7,
	.param .u32 TensorReverseMaxPool_param_8,
	.param .u32 TensorReverseMaxPool_param_9,
	.param .u32 TensorReverseMaxPool_param_10,
	.param .u32 TensorReverseMaxPool_param_11,
	.param .u32 TensorReverseMaxPool_param_12,
	.param .u32 TensorReverseMaxPool_param_13
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<20>;

	ld.param.b32 	%r9, [TensorReverseMaxPool_param_0];
	ld.param.b64 	%rd4, [TensorReverseMaxPool_param_1];
	ld.param.b64 	%rd5, [TensorReverseMaxPool_param_2];
	ld.param.b64 	%rd6, [TensorReverseMaxPool_param_3];
	ld.param.b32 	%r10, [TensorReverseMaxPool_param_4];
	ld.param.b32 	%r11, [TensorReverseMaxPool_param_5];
	ld.param.b32 	%r12, [TensorReverseMaxPool_param_6];
	ld.param.b32 	%r13, [TensorReverseMaxPool_param_8];
	ld.param.b32 	%r14, [TensorReverseMaxPool_param_9];
	ld.param.b32 	%r15, [TensorReverseMaxPool_param_11];
	ld.param.b32 	%r16, [TensorReverseMaxPool_param_12];
	ld.param.b32 	%r17, [TensorReverseMaxPool_param_13];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r45, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r45, %r9;
	@%p1 bra 	$L__BB56_3;
	mul.lo.s32 	%r3, %r11, %r10;
	mul.lo.s32 	%r20, %r13, %r12;
	mul.lo.s32 	%r4, %r20, %r14;
	mul.lo.s32 	%r5, %r14, %r13;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r21;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB56_2:
	div.u32 	%r22, %r45, %r10;
	mul.lo.s32 	%r23, %r22, %r10;
	sub.s32 	%r24, %r45, %r23;
	div.u32 	%r25, %r22, %r11;
	mul.lo.s32 	%r26, %r25, %r11;
	sub.s32 	%r27, %r22, %r26;
	div.u32 	%r28, %r25, %r12;
	mul.lo.s32 	%r29, %r28, %r12;
	sub.s32 	%r30, %r25, %r29;
	mul.lo.s32 	%r31, %r3, %r25;
	cvt.u64.u32 	%rd7, %r31;
	mul.lo.s32 	%r32, %r4, %r28;
	cvt.u64.u32 	%rd8, %r32;
	mul.lo.s32 	%r33, %r5, %r30;
	cvt.u64.u32 	%rd9, %r33;
	add.s64 	%rd10, %rd8, %rd9;
	mad.lo.s32 	%r34, %r27, %r10, %r24;
	cvt.u64.u32 	%rd11, %r34;
	add.s64 	%rd12, %rd7, %rd11;
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.b32 	%r35, [%rd14];
	add.s64 	%rd15, %rd2, %rd13;
	ld.global.nc.b32 	%r36, [%rd15];
	cvt.rzi.ftz.s32.f32 	%r37, %r36;
	max.s32 	%r38, %r37, 0;
	div.u32 	%r39, %r38, %r15;
	mad.lo.s32 	%r40, %r27, %r16, %r39;
	mul.lo.s32 	%r41, %r39, %r15;
	sub.s32 	%r42, %r38, %r41;
	mad.lo.s32 	%r43, %r24, %r17, %r42;
	mad.lo.s32 	%r44, %r40, %r13, %r43;
	cvt.u64.u32 	%rd16, %r44;
	add.s64 	%rd17, %rd10, %rd16;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd3, %rd18;
	st.global.b32 	[%rd19], %r35;
	add.s32 	%r45, %r45, %r6;
	setp.lt.u32 	%p2, %r45, %r9;
	@%p2 bra 	$L__BB56_2;
$L__BB56_3:
	ret;

}
	// .globl	CalculateMultiDistances
.visible .entry CalculateMultiDistances(
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_0,
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_1,
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_2,
	.param .u32 CalculateMultiDistances_param_3,
	.param .u32 CalculateMultiDistances_param_4,
	.param .u32 CalculateMultiDistances_param_5,
	.param .u32 CalculateMultiDistances_param_6
)
{
	.reg .pred 	%p<19>;
	.reg .b32 	%r<69>;
	.reg .b64 	%rd<33>;

	ld.param.b64 	%rd6, [CalculateMultiDistances_param_0];
	ld.param.b64 	%rd7, [CalculateMultiDistances_param_1];
	ld.param.b64 	%rd8, [CalculateMultiDistances_param_2];
	ld.param.b32 	%r31, [CalculateMultiDistances_param_3];
	ld.param.b32 	%r32, [CalculateMultiDistances_param_4];
	ld.param.b32 	%r33, [CalculateMultiDistances_param_5];
	ld.param.b32 	%r34, [CalculateMultiDistances_param_6];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r35, %ctaid.x;
	mov.u32 	%r36, %tid.x;
	mad.lo.s32 	%r62, %r1, %r35, %r36;
	setp.ge.u32 	%p1, %r62, %r33;
	@%p1 bra 	$L__BB57_18;
	mov.u32 	%r37, %ntid.y;
	mov.u32 	%r38, %ctaid.y;
	mov.u32 	%r39, %tid.y;
	mad.lo.s32 	%r3, %r37, %r38, %r39;
	mov.u32 	%r40, %ntid.z;
	mov.u32 	%r41, %ctaid.z;
	mov.u32 	%r42, %tid.z;
	mad.lo.s32 	%r4, %r40, %r41, %r42;
	mov.u32 	%r43, %nctaid.z;
	mul.lo.s32 	%r5, %r40, %r43;
	mov.u32 	%r44, %nctaid.y;
	mul.lo.s32 	%r6, %r37, %r44;
	mov.u32 	%r45, %nctaid.x;
	mul.lo.s32 	%r7, %r1, %r45;
	setp.eq.s32 	%p2, %r34, 0;
	@%p2 bra 	$L__BB57_10;
	setp.ne.s32 	%p3, %r34, 2;
	@%p3 bra 	$L__BB57_3;
	bra.uni 	$L__BB57_16;
$L__BB57_3:
	setp.ge.u32 	%p14, %r3, %r32;
	@%p14 bra 	$L__BB57_9;
	mov.b32 	%r66, %r3;
$L__BB57_5:
	setp.ge.u32 	%p15, %r4, %r31;
	@%p15 bra 	$L__BB57_8;
	mad.lo.s32 	%r67, %r66, %r31, %r4;
	mov.b32 	%r68, %r4;
$L__BB57_7:
	mul.wide.u32 	%rd31, %r67, 4;
	add.s64 	%rd32, %rd1, %rd31;
	atom.global.add.f32 	%r58, [%rd32], 0f00000000;
	add.s32 	%r68, %r68, %r5;
	add.s32 	%r67, %r67, %r5;
	setp.lt.u32 	%p16, %r68, %r31;
	@%p16 bra 	$L__BB57_7;
$L__BB57_8:
	add.s32 	%r66, %r66, %r6;
	setp.lt.u32 	%p17, %r66, %r32;
	@%p17 bra 	$L__BB57_5;
$L__BB57_9:
	add.s32 	%r62, %r62, %r7;
	setp.lt.u32 	%p18, %r62, %r33;
	@%p18 bra 	$L__BB57_3;
	bra.uni 	$L__BB57_18;
$L__BB57_10:
	setp.ge.u32 	%p9, %r3, %r32;
	mov.b32 	%r60, %r3;
	@%p9 bra 	$L__BB57_11;
	bra.uni 	$L__BB57_12;
$L__BB57_11:
	add.s32 	%r62, %r62, %r7;
	setp.lt.u32 	%p13, %r62, %r33;
	@%p13 bra 	$L__BB57_10;
	bra.uni 	$L__BB57_18;
$L__BB57_12:
	setp.ge.u32 	%p10, %r4, %r31;
	@%p10 bra 	$L__BB57_15;
	mul.wide.u32 	%rd20, %r60, 8;
	add.s64 	%rd21, %rd3, %rd20;
	ld.global.nc.b64 	%rd22, [%rd21];
	mul.wide.u32 	%rd23, %r62, 4;
	add.s64 	%rd4, %rd22, %rd23;
	mul.lo.s32 	%r11, %r60, %r31;
	mov.b32 	%r61, %r4;
$L__BB57_14:
	mul.wide.u32 	%rd24, %r61, 8;
	add.s64 	%rd25, %rd2, %rd24;
	ld.global.nc.b64 	%rd26, [%rd25];
	add.s64 	%rd28, %rd26, %rd23;
	ld.b32 	%r52, [%rd28];
	ld.b32 	%r53, [%rd4];
	sub.ftz.f32 	%r54, %r53, %r52;
	mul.ftz.f32 	%r55, %r54, %r54;
	add.s32 	%r56, %r61, %r11;
	mul.wide.u32 	%rd29, %r56, 4;
	add.s64 	%rd30, %rd1, %rd29;
	atom.global.add.f32 	%r57, [%rd30], %r55;
	add.s32 	%r61, %r61, %r5;
	setp.lt.u32 	%p11, %r61, %r31;
	@%p11 bra 	$L__BB57_14;
$L__BB57_15:
	add.s32 	%r60, %r60, %r6;
	setp.lt.u32 	%p12, %r60, %r32;
	@%p12 bra 	$L__BB57_12;
	bra.uni 	$L__BB57_11;
$L__BB57_16:
	setp.ge.u32 	%p4, %r3, %r32;
	mov.b32 	%r63, %r3;
	@%p4 bra 	$L__BB57_17;
	bra.uni 	$L__BB57_19;
$L__BB57_17:
	add.s32 	%r62, %r62, %r7;
	setp.lt.u32 	%p8, %r62, %r33;
	@%p8 bra 	$L__BB57_16;
$L__BB57_18:
	ret;
$L__BB57_19:
	setp.ge.u32 	%p5, %r4, %r31;
	@%p5 bra 	$L__BB57_22;
	mul.wide.u32 	%rd9, %r63, 8;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.b64 	%rd11, [%rd10];
	mul.wide.u32 	%rd12, %r62, 4;
	add.s64 	%rd5, %rd11, %rd12;
	mul.lo.s32 	%r18, %r63, %r31;
	mov.b32 	%r64, %r4;
$L__BB57_21:
	mul.wide.u32 	%rd13, %r64, 8;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.b64 	%rd15, [%rd14];
	add.s64 	%rd17, %rd15, %rd12;
	ld.b32 	%r46, [%rd17];
	ld.b32 	%r47, [%rd5];
	sub.ftz.f32 	%r48, %r47, %r46;
	abs.ftz.f32 	%r49, %r48;
	add.s32 	%r50, %r64, %r18;
	mul.wide.u32 	%rd18, %r50, 4;
	add.s64 	%rd19, %rd1, %rd18;
	atom.global.add.f32 	%r51, [%rd19], %r49;
	add.s32 	%r64, %r64, %r5;
	setp.lt.u32 	%p6, %r64, %r31;
	@%p6 bra 	$L__BB57_21;
$L__BB57_22:
	add.s32 	%r63, %r63, %r6;
	setp.lt.u32 	%p7, %r63, %r32;
	@%p7 bra 	$L__BB57_19;
	bra.uni 	$L__BB57_17;

}
	// .globl	CalculateDistances
.visible .entry CalculateDistances(
	.param .u64 .ptr .align 1 CalculateDistances_param_0,
	.param .u64 .ptr .align 1 CalculateDistances_param_1,
	.param .u64 .ptr .align 1 CalculateDistances_param_2,
	.param .u32 CalculateDistances_param_3,
	.param .u32 CalculateDistances_param_4,
	.param .u32 CalculateDistances_param_5
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<27>;

	ld.param.b64 	%rd4, [CalculateDistances_param_0];
	ld.param.b64 	%rd5, [CalculateDistances_param_1];
	ld.param.b64 	%rd6, [CalculateDistances_param_2];
	ld.param.b32 	%r20, [CalculateDistances_param_3];
	ld.param.b32 	%r21, [CalculateDistances_param_4];
	ld.param.b32 	%r22, [CalculateDistances_param_5];
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd4;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r41, %r1, %r23, %r24;
	setp.ge.u32 	%p1, %r41, %r21;
	@%p1 bra 	$L__BB58_15;
	mov.u32 	%r25, %ntid.y;
	mov.u32 	%r26, %ctaid.y;
	mov.u32 	%r27, %tid.y;
	mad.lo.s32 	%r3, %r25, %r26, %r27;
	mov.u32 	%r28, %nctaid.y;
	mul.lo.s32 	%r4, %r25, %r28;
	mov.u32 	%r29, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r29;
	setp.eq.s32 	%p2, %r22, 0;
	@%p2 bra 	$L__BB58_7;
	setp.ne.s32 	%p3, %r22, 2;
	@%p3 bra 	$L__BB58_11;
$L__BB58_3:
	setp.ge.u32 	%p4, %r3, %r20;
	@%p4 bra 	$L__BB58_6;
	mul.wide.u32 	%rd7, %r41, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.nc.b32 	%r12, [%rd8];
	mov.b32 	%r42, %r3;
$L__BB58_5:
	mul.wide.u32 	%rd9, %r42, 8;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.b64 	%rd11, [%rd10];
	add.s64 	%rd13, %rd11, %rd7;
	ld.b32 	%r30, [%rd13];
	sub.ftz.f32 	%r31, %r12, %r30;
	abs.ftz.f32 	%r32, %r31;
	mul.wide.u32 	%rd14, %r42, 4;
	add.s64 	%rd15, %rd1, %rd14;
	atom.global.add.f32 	%r33, [%rd15], %r32;
	add.s32 	%r42, %r42, %r4;
	setp.lt.u32 	%p5, %r42, %r20;
	@%p5 bra 	$L__BB58_5;
$L__BB58_6:
	add.s32 	%r41, %r41, %r5;
	setp.lt.u32 	%p6, %r41, %r21;
	@%p6 bra 	$L__BB58_3;
	bra.uni 	$L__BB58_15;
$L__BB58_7:
	setp.ge.u32 	%p7, %r3, %r20;
	@%p7 bra 	$L__BB58_10;
	mul.wide.u32 	%rd16, %r41, 4;
	add.s64 	%rd17, %rd3, %rd16;
	ld.global.nc.b32 	%r7, [%rd17];
	mov.b32 	%r40, %r3;
$L__BB58_9:
	mul.wide.u32 	%rd18, %r40, 8;
	add.s64 	%rd19, %rd2, %rd18;
	ld.global.nc.b64 	%rd20, [%rd19];
	add.s64 	%rd22, %rd20, %rd16;
	ld.b32 	%r34, [%rd22];
	sub.ftz.f32 	%r35, %r7, %r34;
	mul.ftz.f32 	%r36, %r35, %r35;
	mul.wide.u32 	%rd23, %r40, 4;
	add.s64 	%rd24, %rd1, %rd23;
	atom.global.add.f32 	%r37, [%rd24], %r36;
	add.s32 	%r40, %r40, %r4;
	setp.lt.u32 	%p8, %r40, %r20;
	@%p8 bra 	$L__BB58_9;
$L__BB58_10:
	add.s32 	%r41, %r41, %r5;
	setp.lt.u32 	%p9, %r41, %r21;
	@%p9 bra 	$L__BB58_7;
	bra.uni 	$L__BB58_15;
$L__BB58_11:
	setp.ge.u32 	%p10, %r3, %r20;
	@%p10 bra 	$L__BB58_14;
	mov.b32 	%r44, %r3;
$L__BB58_13:
	mul.wide.u32 	%rd25, %r44, 4;
	add.s64 	%rd26, %rd1, %rd25;
	atom.global.add.f32 	%r38, [%rd26], 0f00000000;
	add.s32 	%r44, %r44, %r4;
	setp.lt.u32 	%p11, %r44, %r20;
	@%p11 bra 	$L__BB58_13;
$L__BB58_14:
	add.s32 	%r41, %r41, %r5;
	setp.lt.u32 	%p12, %r41, %r21;
	@%p12 bra 	$L__BB58_11;
$L__BB58_15:
	ret;

}
	// .globl	CosineMultiDistance
.visible .entry CosineMultiDistance(
	.param .u64 .ptr .align 1 CosineMultiDistance_param_0,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_1,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_2,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_3,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_4,
	.param .u32 CosineMultiDistance_param_5,
	.param .u32 CosineMultiDistance_param_6,
	.param .u32 CosineMultiDistance_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<43>;
	.reg .b64 	%rd<25>;

	ld.param.b64 	%rd7, [CosineMultiDistance_param_0];
	ld.param.b64 	%rd8, [CosineMultiDistance_param_1];
	ld.param.b64 	%rd9, [CosineMultiDistance_param_2];
	ld.param.b64 	%rd10, [CosineMultiDistance_param_3];
	ld.param.b64 	%rd11, [CosineMultiDistance_param_4];
	ld.param.b32 	%r17, [CosineMultiDistance_param_5];
	ld.param.b32 	%r18, [CosineMultiDistance_param_6];
	ld.param.b32 	%r19, [CosineMultiDistance_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r39, %r1, %r20, %r21;
	setp.ge.u32 	%p1, %r39, %r19;
	@%p1 bra 	$L__BB59_9;
	mov.u32 	%r22, %ntid.y;
	mov.u32 	%r23, %ctaid.y;
	mov.u32 	%r24, %tid.y;
	mad.lo.s32 	%r3, %r22, %r23, %r24;
	mov.u32 	%r25, %ntid.z;
	mov.u32 	%r26, %ctaid.z;
	mov.u32 	%r27, %tid.z;
	mad.lo.s32 	%r4, %r25, %r26, %r27;
	mov.u32 	%r28, %nctaid.z;
	mul.lo.s32 	%r5, %r25, %r28;
	mov.u32 	%r29, %nctaid.y;
	mul.lo.s32 	%r6, %r22, %r29;
	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r7, %r1, %r30;
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd9;
	cvta.to.global.u64 	%rd4, %rd10;
	cvta.to.global.u64 	%rd5, %rd11;
$L__BB59_2:
	setp.ge.u32 	%p2, %r3, %r18;
	@%p2 bra 	$L__BB59_8;
	mul.wide.u32 	%rd15, %r39, 4;
	mov.b32 	%r40, %r3;
$L__BB59_4:
	setp.ge.u32 	%p3, %r4, %r17;
	@%p3 bra 	$L__BB59_7;
	mul.wide.u32 	%rd12, %r40, 8;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.b64 	%rd14, [%rd13];
	add.s64 	%rd6, %rd14, %rd15;
	mad.lo.s32 	%r41, %r40, %r17, %r4;
	mov.b32 	%r42, %r4;
$L__BB59_6:
	ld.b32 	%r31, [%rd6];
	mul.wide.u32 	%rd16, %r42, 8;
	add.s64 	%rd17, %rd2, %rd16;
	ld.global.nc.b64 	%rd18, [%rd17];
	add.s64 	%rd20, %rd18, %rd15;
	ld.b32 	%r32, [%rd20];
	mul.wide.u32 	%rd21, %r41, 4;
	add.s64 	%rd22, %rd3, %rd21;
	mul.ftz.f32 	%r33, %r31, %r31;
	atom.global.add.f32 	%r34, [%rd22], %r33;
	add.s64 	%rd23, %rd4, %rd21;
	mul.ftz.f32 	%r35, %r31, %r32;
	atom.global.add.f32 	%r36, [%rd23], %r35;
	add.s64 	%rd24, %rd5, %rd21;
	mul.ftz.f32 	%r37, %r32, %r32;
	atom.global.add.f32 	%r38, [%rd24], %r37;
	add.s32 	%r42, %r42, %r5;
	add.s32 	%r41, %r41, %r5;
	setp.lt.u32 	%p4, %r42, %r17;
	@%p4 bra 	$L__BB59_6;
$L__BB59_7:
	add.s32 	%r40, %r40, %r6;
	setp.lt.u32 	%p5, %r40, %r18;
	@%p5 bra 	$L__BB59_4;
$L__BB59_8:
	add.s32 	%r39, %r39, %r7;
	setp.lt.u32 	%p6, %r39, %r19;
	@%p6 bra 	$L__BB59_2;
$L__BB59_9:
	ret;

}
	// .globl	CosineDistances
.visible .entry CosineDistances(
	.param .u64 .ptr .align 1 CosineDistances_param_0,
	.param .u64 .ptr .align 1 CosineDistances_param_1,
	.param .u64 .ptr .align 1 CosineDistances_param_2,
	.param .u64 .ptr .align 1 CosineDistances_param_3,
	.param .u64 .ptr .align 1 CosineDistances_param_4,
	.param .u32 CosineDistances_param_5,
	.param .u32 CosineDistances_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<29>;
	.reg .b64 	%rd<22>;

	ld.param.b64 	%rd6, [CosineDistances_param_0];
	ld.param.b64 	%rd7, [CosineDistances_param_1];
	ld.param.b64 	%rd8, [CosineDistances_param_2];
	ld.param.b64 	%rd9, [CosineDistances_param_3];
	ld.param.b64 	%rd10, [CosineDistances_param_4];
	ld.param.b32 	%r12, [CosineDistances_param_5];
	ld.param.b32 	%r13, [CosineDistances_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r27, %r1, %r14, %r15;
	setp.ge.u32 	%p1, %r27, %r13;
	@%p1 bra 	$L__BB60_6;
	mov.u32 	%r16, %ntid.y;
	mov.u32 	%r17, %ctaid.y;
	mov.u32 	%r18, %tid.y;
	mad.lo.s32 	%r3, %r16, %r17, %r18;
	mov.u32 	%r19, %nctaid.y;
	mul.lo.s32 	%r4, %r16, %r19;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r20;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd9;
	cvta.to.global.u64 	%rd5, %rd10;
$L__BB60_2:
	setp.ge.u32 	%p2, %r3, %r12;
	@%p2 bra 	$L__BB60_5;
	mul.wide.u32 	%rd11, %r27, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.nc.b32 	%r7, [%rd12];
	mul.ftz.f32 	%r8, %r7, %r7;
	mov.b32 	%r28, %r3;
$L__BB60_4:
	mul.wide.u32 	%rd13, %r28, 8;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.b64 	%rd15, [%rd14];
	add.s64 	%rd17, %rd15, %rd11;
	ld.b32 	%r21, [%rd17];
	mul.wide.u32 	%rd18, %r28, 4;
	add.s64 	%rd19, %rd3, %rd18;
	atom.global.add.f32 	%r22, [%rd19], %r8;
	add.s64 	%rd20, %rd4, %rd18;
	mul.ftz.f32 	%r23, %r7, %r21;
	atom.global.add.f32 	%r24, [%rd20], %r23;
	add.s64 	%rd21, %rd5, %rd18;
	mul.ftz.f32 	%r25, %r21, %r21;
	atom.global.add.f32 	%r26, [%rd21], %r25;
	add.s32 	%r28, %r28, %r4;
	setp.lt.u32 	%p3, %r28, %r12;
	@%p3 bra 	$L__BB60_4;
$L__BB60_5:
	add.s32 	%r27, %r27, %r5;
	setp.lt.u32 	%p4, %r27, %r13;
	@%p4 bra 	$L__BB60_2;
$L__BB60_6:
	ret;

}
	// .globl	SumValues
.visible .entry SumValues(
	.param .u64 .ptr .align 1 SumValues_param_0,
	.param .u32 SumValues_param_1,
	.param .u64 .ptr .align 1 SumValues_param_2,
	.param .u32 SumValues_param_3
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<135>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ9SumValuesE5block[4096];
	ld.param.b64 	%rd1, [SumValues_param_0];
	ld.param.b32 	%r39, [SumValues_param_1];
	ld.param.b64 	%rd2, [SumValues_param_2];
	ld.param.b32 	%r40, [SumValues_param_3];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r41, %ntid.x;
	mad.lo.s32 	%r3, %r41, %r2, %r1;
	setp.ge.u32 	%p1, %r3, %r39;
	@%p1 bra 	$L__BB61_2;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.lo.s32 	%r42, %r40, %r3;
	mul.wide.u32 	%rd4, %r42, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.b32 	%r43, [%rd5];
	shl.b32 	%r44, %r1, 2;
	mov.b32 	%r45, _ZZ9SumValuesE5block;
	add.s32 	%r46, %r45, %r44;
	st.shared.b32 	[%r46], %r43;
$L__BB61_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB61_17;
	shl.b32 	%r48, %r2, 10;
	sub.s32 	%r4, %r39, %r48;
	setp.eq.s32 	%p3, %r4, 0;
	mov.b32 	%r134, 0f00000000;
	@%p3 bra 	$L__BB61_16;
	min.u32 	%r5, %r4, 1024;
	and.b32 	%r6, %r5, 15;
	setp.lt.u32 	%p4, %r4, 16;
	mov.b32 	%r134, 0f00000000;
	mov.b32 	%r126, 0;
	@%p4 bra 	$L__BB61_7;
	and.b32 	%r126, %r5, 2032;
	mov.b32 	%r54, _ZZ9SumValuesE5block;
	add.s32 	%r119, %r54, 32;
	neg.s32 	%r120, %r126;
	mov.b32 	%r134, 0f00000000;
$L__BB61_6:
	.pragma "nounroll";
	ld.shared.b32 	%r55, [%r119+-32];
	add.ftz.f32 	%r56, %r134, %r55;
	ld.shared.b32 	%r57, [%r119+-28];
	add.ftz.f32 	%r58, %r56, %r57;
	ld.shared.b32 	%r59, [%r119+-24];
	add.ftz.f32 	%r60, %r58, %r59;
	ld.shared.b32 	%r61, [%r119+-20];
	add.ftz.f32 	%r62, %r60, %r61;
	ld.shared.b32 	%r63, [%r119+-16];
	add.ftz.f32 	%r64, %r62, %r63;
	ld.shared.b32 	%r65, [%r119+-12];
	add.ftz.f32 	%r66, %r64, %r65;
	ld.shared.b32 	%r67, [%r119+-8];
	add.ftz.f32 	%r68, %r66, %r67;
	ld.shared.b32 	%r69, [%r119+-4];
	add.ftz.f32 	%r70, %r68, %r69;
	ld.shared.b32 	%r71, [%r119];
	add.ftz.f32 	%r72, %r70, %r71;
	ld.shared.b32 	%r73, [%r119+4];
	add.ftz.f32 	%r74, %r72, %r73;
	ld.shared.b32 	%r75, [%r119+8];
	add.ftz.f32 	%r76, %r74, %r75;
	ld.shared.b32 	%r77, [%r119+12];
	add.ftz.f32 	%r78, %r76, %r77;
	ld.shared.b32 	%r79, [%r119+16];
	add.ftz.f32 	%r80, %r78, %r79;
	ld.shared.b32 	%r81, [%r119+20];
	add.ftz.f32 	%r82, %r80, %r81;
	ld.shared.b32 	%r83, [%r119+24];
	add.ftz.f32 	%r84, %r82, %r83;
	ld.shared.b32 	%r85, [%r119+28];
	add.ftz.f32 	%r134, %r84, %r85;
	add.s32 	%r120, %r120, 16;
	add.s32 	%r119, %r119, 64;
	setp.ne.s32 	%p5, %r120, 0;
	@%p5 bra 	$L__BB61_6;
$L__BB61_7:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB61_16;
	and.b32 	%r18, %r5, 7;
	setp.lt.u32 	%p7, %r6, 8;
	@%p7 bra 	$L__BB61_10;
	shl.b32 	%r87, %r126, 2;
	mov.b32 	%r88, _ZZ9SumValuesE5block;
	add.s32 	%r89, %r88, %r87;
	ld.shared.b32 	%r90, [%r89];
	add.ftz.f32 	%r91, %r134, %r90;
	ld.shared.b32 	%r92, [%r89+4];
	add.ftz.f32 	%r93, %r91, %r92;
	ld.shared.b32 	%r94, [%r89+8];
	add.ftz.f32 	%r95, %r93, %r94;
	ld.shared.b32 	%r96, [%r89+12];
	add.ftz.f32 	%r97, %r95, %r96;
	ld.shared.b32 	%r98, [%r89+16];
	add.ftz.f32 	%r99, %r97, %r98;
	ld.shared.b32 	%r100, [%r89+20];
	add.ftz.f32 	%r101, %r99, %r100;
	ld.shared.b32 	%r102, [%r89+24];
	add.ftz.f32 	%r103, %r101, %r102;
	ld.shared.b32 	%r104, [%r89+28];
	add.ftz.f32 	%r134, %r103, %r104;
	add.s32 	%r126, %r126, 8;
$L__BB61_10:
	setp.eq.s32 	%p8, %r18, 0;
	@%p8 bra 	$L__BB61_16;
	and.b32 	%r24, %r5, 3;
	setp.lt.u32 	%p9, %r18, 4;
	@%p9 bra 	$L__BB61_13;
	shl.b32 	%r106, %r126, 2;
	mov.b32 	%r107, _ZZ9SumValuesE5block;
	add.s32 	%r108, %r107, %r106;
	ld.shared.b32 	%r109, [%r108];
	add.ftz.f32 	%r110, %r134, %r109;
	ld.shared.b32 	%r111, [%r108+4];
	add.ftz.f32 	%r112, %r110, %r111;
	ld.shared.b32 	%r113, [%r108+8];
	add.ftz.f32 	%r114, %r112, %r113;
	ld.shared.b32 	%r115, [%r108+12];
	add.ftz.f32 	%r134, %r114, %r115;
	add.s32 	%r126, %r126, 4;
$L__BB61_13:
	setp.eq.s32 	%p10, %r24, 0;
	@%p10 bra 	$L__BB61_16;
	shl.b32 	%r116, %r126, 2;
	mov.b32 	%r117, _ZZ9SumValuesE5block;
	add.s32 	%r132, %r117, %r116;
	neg.s32 	%r131, %r24;
$L__BB61_15:
	.pragma "nounroll";
	ld.shared.b32 	%r118, [%r132];
	add.ftz.f32 	%r134, %r134, %r118;
	add.s32 	%r132, %r132, 4;
	add.s32 	%r131, %r131, 1;
	setp.ne.s32 	%p11, %r131, 0;
	@%p11 bra 	$L__BB61_15;
$L__BB61_16:
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.b32 	[%rd8], %r134;
$L__BB61_17:
	ret;

}
