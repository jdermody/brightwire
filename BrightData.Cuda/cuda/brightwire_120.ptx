//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36037853
// Cuda compilation tools, release 12.9, V12.9.86
// Based on NVVM 20.0.0
//

.version 8.8
.target sm_100
.address_size 64

	// .globl	IsFinite
// _ZZ13FindMinAndMaxE5block has been demoted
// _ZZ10FindStdDevE5block has been demoted
// _ZZ9SumValuesE5block has been demoted

.visible .entry IsFinite(
	.param .u64 .ptr .align 1 IsFinite_param_0,
	.param .u64 .ptr .align 1 IsFinite_param_1,
	.param .u32 IsFinite_param_2,
	.param .u32 IsFinite_param_3
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [IsFinite_param_0];
	ld.param.u64 	%rd4, [IsFinite_param_1];
	ld.param.u32 	%r6, [IsFinite_param_2];
	ld.param.u32 	%r7, [IsFinite_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB0_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB0_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	abs.ftz.f32 	%f2, %f1;
	setp.equ.ftz.f32 	%p2, %f2, 0f7F800000;
	selp.f32 	%f3, 0f3F800000, 0f00000000, %p2;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p3, %r12, %r6;
	@%p3 bra 	$L__BB0_2;
$L__BB0_3:
	ret;

}
	// .globl	Scale
.visible .entry Scale(
	.param .u64 .ptr .align 1 Scale_param_0,
	.param .u32 Scale_param_1,
	.param .f32 Scale_param_2,
	.param .u32 Scale_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [Scale_param_0];
	ld.param.u32 	%r6, [Scale_param_1];
	ld.param.f32 	%f1, [Scale_param_2];
	ld.param.u32 	%r7, [Scale_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB1_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB1_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd3, %r11, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f2, [%rd4];
	mul.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd4], %f3;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p2, %r12, %r6;
	@%p2 bra 	$L__BB1_2;
$L__BB1_3:
	ret;

}
	// .globl	PointwiseMultiply
.visible .entry PointwiseMultiply(
	.param .u64 .ptr .align 1 PointwiseMultiply_param_0,
	.param .u64 .ptr .align 1 PointwiseMultiply_param_1,
	.param .u32 PointwiseMultiply_param_2,
	.param .u32 PointwiseMultiply_param_3,
	.param .u32 PointwiseMultiply_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [PointwiseMultiply_param_0];
	ld.param.u64 	%rd4, [PointwiseMultiply_param_1];
	ld.param.u32 	%r6, [PointwiseMultiply_param_2];
	ld.param.u32 	%r7, [PointwiseMultiply_param_3];
	ld.param.u32 	%r8, [PointwiseMultiply_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB2_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB2_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	mul.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB2_2;
$L__BB2_3:
	ret;

}
	// .globl	PointwiseDivide
.visible .entry PointwiseDivide(
	.param .u64 .ptr .align 1 PointwiseDivide_param_0,
	.param .u64 .ptr .align 1 PointwiseDivide_param_1,
	.param .u32 PointwiseDivide_param_2,
	.param .u32 PointwiseDivide_param_3,
	.param .u32 PointwiseDivide_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [PointwiseDivide_param_0];
	ld.param.u64 	%rd4, [PointwiseDivide_param_1];
	ld.param.u32 	%r6, [PointwiseDivide_param_2];
	ld.param.u32 	%r7, [PointwiseDivide_param_3];
	ld.param.u32 	%r8, [PointwiseDivide_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB3_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB3_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	div.approx.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB3_2;
$L__BB3_3:
	ret;

}
	// .globl	Sqrt
.visible .entry Sqrt(
	.param .u64 .ptr .align 1 Sqrt_param_0,
	.param .u64 .ptr .align 1 Sqrt_param_1,
	.param .u32 Sqrt_param_2,
	.param .f32 Sqrt_param_3,
	.param .u32 Sqrt_param_4,
	.param .u32 Sqrt_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Sqrt_param_0];
	ld.param.u64 	%rd4, [Sqrt_param_1];
	ld.param.u32 	%r6, [Sqrt_param_2];
	ld.param.f32 	%f1, [Sqrt_param_3];
	ld.param.u32 	%r7, [Sqrt_param_4];
	ld.param.u32 	%r8, [Sqrt_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB4_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB4_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	add.ftz.f32 	%f3, %f1, %f2;
	sqrt.approx.ftz.f32 	%f4, %f3;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f4;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB4_2;
$L__BB4_3:
	ret;

}
	// .globl	AddInPlace
.visible .entry AddInPlace(
	.param .u64 .ptr .align 1 AddInPlace_param_0,
	.param .u64 .ptr .align 1 AddInPlace_param_1,
	.param .u32 AddInPlace_param_2,
	.param .f32 AddInPlace_param_3,
	.param .f32 AddInPlace_param_4,
	.param .u32 AddInPlace_param_5,
	.param .u32 AddInPlace_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<7>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [AddInPlace_param_0];
	ld.param.u64 	%rd4, [AddInPlace_param_1];
	ld.param.u32 	%r6, [AddInPlace_param_2];
	ld.param.f32 	%f1, [AddInPlace_param_3];
	ld.param.f32 	%f2, [AddInPlace_param_4];
	ld.param.u32 	%r7, [AddInPlace_param_5];
	ld.param.u32 	%r8, [AddInPlace_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB5_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB5_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f3, [%rd6];
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.f32 	%f4, [%rd8];
	mul.ftz.f32 	%f5, %f2, %f4;
	fma.rn.ftz.f32 	%f6, %f1, %f3, %f5;
	st.global.f32 	[%rd6], %f6;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB5_2;
$L__BB5_3:
	ret;

}
	// .globl	SubtractInPlace
.visible .entry SubtractInPlace(
	.param .u64 .ptr .align 1 SubtractInPlace_param_0,
	.param .u64 .ptr .align 1 SubtractInPlace_param_1,
	.param .u32 SubtractInPlace_param_2,
	.param .f32 SubtractInPlace_param_3,
	.param .f32 SubtractInPlace_param_4,
	.param .u32 SubtractInPlace_param_5,
	.param .u32 SubtractInPlace_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<8>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [SubtractInPlace_param_0];
	ld.param.u64 	%rd4, [SubtractInPlace_param_1];
	ld.param.u32 	%r6, [SubtractInPlace_param_2];
	ld.param.f32 	%f1, [SubtractInPlace_param_3];
	ld.param.f32 	%f2, [SubtractInPlace_param_4];
	ld.param.u32 	%r7, [SubtractInPlace_param_5];
	ld.param.u32 	%r8, [SubtractInPlace_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB6_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB6_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f3, [%rd6];
	mul.ftz.f32 	%f4, %f1, %f3;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.f32 	%f5, [%rd8];
	mul.ftz.f32 	%f6, %f2, %f5;
	sub.ftz.f32 	%f7, %f4, %f6;
	st.global.f32 	[%rd6], %f7;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB6_2;
$L__BB6_3:
	ret;

}
	// .globl	AddToEachRow
.visible .entry AddToEachRow(
	.param .u64 .ptr .align 1 AddToEachRow_param_0,
	.param .u64 .ptr .align 1 AddToEachRow_param_1,
	.param .u32 AddToEachRow_param_2,
	.param .u32 AddToEachRow_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [AddToEachRow_param_0];
	ld.param.u64 	%rd4, [AddToEachRow_param_1];
	ld.param.u32 	%r10, [AddToEachRow_param_2];
	ld.param.u32 	%r11, [AddToEachRow_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB7_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB7_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB7_5;
	mov.u32 	%r21, %r3;
$L__BB7_4:
	mul.wide.u32 	%rd5, %r21, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	add.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB7_4;
$L__BB7_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB7_2;
$L__BB7_6:
	ret;

}
	// .globl	AddToEachColumn
.visible .entry AddToEachColumn(
	.param .u64 .ptr .align 1 AddToEachColumn_param_0,
	.param .u64 .ptr .align 1 AddToEachColumn_param_1,
	.param .u32 AddToEachColumn_param_2,
	.param .u32 AddToEachColumn_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [AddToEachColumn_param_0];
	ld.param.u64 	%rd4, [AddToEachColumn_param_1];
	ld.param.u32 	%r10, [AddToEachColumn_param_2];
	ld.param.u32 	%r11, [AddToEachColumn_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB8_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB8_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB8_5;
	mul.wide.u32 	%rd5, %r20, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mov.u32 	%r21, %r3;
$L__BB8_4:
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	add.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB8_4;
$L__BB8_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB8_2;
$L__BB8_6:
	ret;

}
	// .globl	MultiplyByEachRow
.visible .entry MultiplyByEachRow(
	.param .u64 .ptr .align 1 MultiplyByEachRow_param_0,
	.param .u64 .ptr .align 1 MultiplyByEachRow_param_1,
	.param .u32 MultiplyByEachRow_param_2,
	.param .u32 MultiplyByEachRow_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [MultiplyByEachRow_param_0];
	ld.param.u64 	%rd4, [MultiplyByEachRow_param_1];
	ld.param.u32 	%r10, [MultiplyByEachRow_param_2];
	ld.param.u32 	%r11, [MultiplyByEachRow_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB9_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB9_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB9_5;
	mov.u32 	%r21, %r3;
$L__BB9_4:
	mul.wide.u32 	%rd5, %r21, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	mul.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB9_4;
$L__BB9_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB9_2;
$L__BB9_6:
	ret;

}
	// .globl	MultiplyByEachColumn
.visible .entry MultiplyByEachColumn(
	.param .u64 .ptr .align 1 MultiplyByEachColumn_param_0,
	.param .u64 .ptr .align 1 MultiplyByEachColumn_param_1,
	.param .u32 MultiplyByEachColumn_param_2,
	.param .u32 MultiplyByEachColumn_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [MultiplyByEachColumn_param_0];
	ld.param.u64 	%rd4, [MultiplyByEachColumn_param_1];
	ld.param.u32 	%r10, [MultiplyByEachColumn_param_2];
	ld.param.u32 	%r11, [MultiplyByEachColumn_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB10_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB10_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB10_5;
	mul.wide.u32 	%rd5, %r20, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mov.u32 	%r21, %r3;
$L__BB10_4:
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.f32 	%f2, [%rd8];
	mul.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB10_4;
$L__BB10_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB10_2;
$L__BB10_6:
	ret;

}
	// .globl	TanH
.visible .entry TanH(
	.param .u64 .ptr .align 1 TanH_param_0,
	.param .u64 .ptr .align 1 TanH_param_1,
	.param .u32 TanH_param_2,
	.param .u32 TanH_param_3,
	.param .u32 TanH_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [TanH_param_0];
	ld.param.u64 	%rd4, [TanH_param_1];
	ld.param.u32 	%r6, [TanH_param_2];
	ld.param.u32 	%r7, [TanH_param_3];
	ld.param.u32 	%r8, [TanH_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB11_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB11_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	tanh.approx.f32 	%f2, %f1;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f2;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB11_2;
$L__BB11_3:
	ret;

}
	// .globl	TanHDerivative
.visible .entry TanHDerivative(
	.param .u64 .ptr .align 1 TanHDerivative_param_0,
	.param .u64 .ptr .align 1 TanHDerivative_param_1,
	.param .u32 TanHDerivative_param_2,
	.param .u32 TanHDerivative_param_3,
	.param .u32 TanHDerivative_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [TanHDerivative_param_0];
	ld.param.u64 	%rd4, [TanHDerivative_param_1];
	ld.param.u32 	%r6, [TanHDerivative_param_2];
	ld.param.u32 	%r7, [TanHDerivative_param_3];
	ld.param.u32 	%r8, [TanHDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB12_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB12_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	tanh.approx.f32 	%f2, %f1;
	mul.ftz.f32 	%f3, %f2, %f2;
	mov.f32 	%f4, 0f3F800000;
	sub.ftz.f32 	%f5, %f4, %f3;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f5;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB12_2;
$L__BB12_3:
	ret;

}
	// .globl	Sigmoid
.visible .entry Sigmoid(
	.param .u64 .ptr .align 1 Sigmoid_param_0,
	.param .u64 .ptr .align 1 Sigmoid_param_1,
	.param .u32 Sigmoid_param_2,
	.param .u32 Sigmoid_param_3,
	.param .u32 Sigmoid_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Sigmoid_param_0];
	ld.param.u64 	%rd4, [Sigmoid_param_1];
	ld.param.u32 	%r6, [Sigmoid_param_2];
	ld.param.u32 	%r7, [Sigmoid_param_3];
	ld.param.u32 	%r8, [Sigmoid_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB13_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB13_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f2, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f3, %f2;
	add.ftz.f32 	%f4, %f3, 0f3F800000;
	rcp.approx.ftz.f32 	%f5, %f4;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f5;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB13_2;
$L__BB13_3:
	ret;

}
	// .globl	SigmoidDerivative
.visible .entry SigmoidDerivative(
	.param .u64 .ptr .align 1 SigmoidDerivative_param_0,
	.param .u64 .ptr .align 1 SigmoidDerivative_param_1,
	.param .u32 SigmoidDerivative_param_2,
	.param .u32 SigmoidDerivative_param_3,
	.param .u32 SigmoidDerivative_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [SigmoidDerivative_param_0];
	ld.param.u64 	%rd4, [SigmoidDerivative_param_1];
	ld.param.u32 	%r6, [SigmoidDerivative_param_2];
	ld.param.u32 	%r7, [SigmoidDerivative_param_3];
	ld.param.u32 	%r8, [SigmoidDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB14_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB14_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f2, %f1, 0fBFB8AA3B;
	ex2.approx.ftz.f32 	%f3, %f2;
	add.ftz.f32 	%f4, %f3, 0f3F800000;
	rcp.approx.ftz.f32 	%f5, %f4;
	mov.f32 	%f6, 0f3F800000;
	sub.ftz.f32 	%f7, %f6, %f5;
	mul.ftz.f32 	%f8, %f5, %f7;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f8;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB14_2;
$L__BB14_3:
	ret;

}
	// .globl	RELU
.visible .entry RELU(
	.param .u64 .ptr .align 1 RELU_param_0,
	.param .u64 .ptr .align 1 RELU_param_1,
	.param .u32 RELU_param_2,
	.param .u32 RELU_param_3,
	.param .u32 RELU_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [RELU_param_0];
	ld.param.u64 	%rd4, [RELU_param_1];
	ld.param.u32 	%r6, [RELU_param_2];
	ld.param.u32 	%r7, [RELU_param_3];
	ld.param.u32 	%r8, [RELU_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB15_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB15_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	setp.le.ftz.f32 	%p2, %f1, 0f00000000;
	selp.f32 	%f2, 0f00000000, %f1, %p2;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f2;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p3, %r14, %r6;
	@%p3 bra 	$L__BB15_2;
$L__BB15_3:
	ret;

}
	// .globl	RELUDerivative
.visible .entry RELUDerivative(
	.param .u64 .ptr .align 1 RELUDerivative_param_0,
	.param .u64 .ptr .align 1 RELUDerivative_param_1,
	.param .u32 RELUDerivative_param_2,
	.param .u32 RELUDerivative_param_3,
	.param .u32 RELUDerivative_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [RELUDerivative_param_0];
	ld.param.u64 	%rd4, [RELUDerivative_param_1];
	ld.param.u32 	%r6, [RELUDerivative_param_2];
	ld.param.u32 	%r7, [RELUDerivative_param_3];
	ld.param.u32 	%r8, [RELUDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB16_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB16_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	setp.gtu.ftz.f32 	%p2, %f1, 0f00000000;
	selp.f32 	%f2, 0f3F800000, 0f00000000, %p2;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f2;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p3, %r14, %r6;
	@%p3 bra 	$L__BB16_2;
$L__BB16_3:
	ret;

}
	// .globl	LeakyRELU
.visible .entry LeakyRELU(
	.param .u64 .ptr .align 1 LeakyRELU_param_0,
	.param .u64 .ptr .align 1 LeakyRELU_param_1,
	.param .u32 LeakyRELU_param_2,
	.param .u32 LeakyRELU_param_3,
	.param .u32 LeakyRELU_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [LeakyRELU_param_0];
	ld.param.u64 	%rd4, [LeakyRELU_param_1];
	ld.param.u32 	%r6, [LeakyRELU_param_2];
	ld.param.u32 	%r7, [LeakyRELU_param_3];
	ld.param.u32 	%r8, [LeakyRELU_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB17_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB17_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	setp.le.ftz.f32 	%p2, %f1, 0f00000000;
	mul.ftz.f32 	%f2, %f1, 0f3C23D70A;
	selp.f32 	%f3, %f2, %f1, %p2;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p3, %r14, %r6;
	@%p3 bra 	$L__BB17_2;
$L__BB17_3:
	ret;

}
	// .globl	LeakyRELUDerivative
.visible .entry LeakyRELUDerivative(
	.param .u64 .ptr .align 1 LeakyRELUDerivative_param_0,
	.param .u64 .ptr .align 1 LeakyRELUDerivative_param_1,
	.param .u32 LeakyRELUDerivative_param_2,
	.param .u32 LeakyRELUDerivative_param_3,
	.param .u32 LeakyRELUDerivative_param_4
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [LeakyRELUDerivative_param_0];
	ld.param.u64 	%rd4, [LeakyRELUDerivative_param_1];
	ld.param.u32 	%r6, [LeakyRELUDerivative_param_2];
	ld.param.u32 	%r7, [LeakyRELUDerivative_param_3];
	ld.param.u32 	%r8, [LeakyRELUDerivative_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB18_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB18_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	setp.le.ftz.f32 	%p2, %f1, 0f00000000;
	selp.f32 	%f2, 0f3C23D70A, 0f3F800000, %p2;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f2;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p3, %r14, %r6;
	@%p3 bra 	$L__BB18_2;
$L__BB18_3:
	ret;

}
	// .globl	Reverse
.visible .entry Reverse(
	.param .u64 .ptr .align 1 Reverse_param_0,
	.param .u64 .ptr .align 1 Reverse_param_1,
	.param .u32 Reverse_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Reverse_param_0];
	ld.param.u64 	%rd4, [Reverse_param_1];
	ld.param.u32 	%r6, [Reverse_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r12, %r1, %r7, %r8;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB19_3;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r9;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB19_2:
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	not.b32 	%r10, %r12;
	add.s32 	%r11, %r6, %r10;
	mul.wide.u32 	%rd7, %r11, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f1;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p2, %r12, %r6;
	@%p2 bra 	$L__BB19_2;
$L__BB19_3:
	ret;

}
	// .globl	SumRows
.visible .entry SumRows(
	.param .u64 .ptr .align 1 SumRows_param_0,
	.param .u64 .ptr .align 1 SumRows_param_1,
	.param .u32 SumRows_param_2,
	.param .u32 SumRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd4, [SumRows_param_0];
	ld.param.u64 	%rd5, [SumRows_param_1];
	ld.param.u32 	%r10, [SumRows_param_2];
	ld.param.u32 	%r11, [SumRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB20_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
$L__BB20_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB20_5;
	mul.wide.u32 	%rd6, %r20, 4;
	add.s64 	%rd3, %rd2, %rd6;
	mov.u32 	%r21, %r3;
$L__BB20_4:
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	atom.global.add.f32 	%f2, [%rd3], %f1;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB20_4;
$L__BB20_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB20_2;
$L__BB20_6:
	ret;

}
	// .globl	SumColumns
.visible .entry SumColumns(
	.param .u64 .ptr .align 1 SumColumns_param_0,
	.param .u64 .ptr .align 1 SumColumns_param_1,
	.param .u32 SumColumns_param_2,
	.param .u32 SumColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [SumColumns_param_0];
	ld.param.u64 	%rd4, [SumColumns_param_1];
	ld.param.u32 	%r10, [SumColumns_param_2];
	ld.param.u32 	%r11, [SumColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB21_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB21_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB21_5;
	mov.u32 	%r21, %r3;
$L__BB21_4:
	mul.wide.u32 	%rd5, %r21, 4;
	add.s64 	%rd6, %rd2, %rd5;
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	atom.global.add.f32 	%f2, [%rd6], %f1;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB21_4;
$L__BB21_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB21_2;
$L__BB21_6:
	ret;

}
	// .globl	MemSet
.visible .entry MemSet(
	.param .u64 .ptr .align 1 MemSet_param_0,
	.param .f32 MemSet_param_1,
	.param .u32 MemSet_param_2,
	.param .u32 MemSet_param_3,
	.param .u32 MemSet_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [MemSet_param_0];
	ld.param.f32 	%f1, [MemSet_param_1];
	ld.param.u32 	%r6, [MemSet_param_2];
	ld.param.u32 	%r7, [MemSet_param_3];
	ld.param.u32 	%r8, [MemSet_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r13, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r13, %r6;
	@%p1 bra 	$L__BB22_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB22_2:
	mad.lo.s32 	%r12, %r13, %r8, %r7;
	mul.wide.u32 	%rd3, %r12, 4;
	add.s64 	%rd4, %rd1, %rd3;
	st.global.f32 	[%rd4], %f1;
	add.s32 	%r13, %r13, %r3;
	setp.lt.u32 	%p2, %r13, %r6;
	@%p2 bra 	$L__BB22_2;
$L__BB22_3:
	ret;

}
	// .globl	MemCpy
.visible .entry MemCpy(
	.param .u64 .ptr .align 1 MemCpy_param_0,
	.param .u64 .ptr .align 1 MemCpy_param_1,
	.param .u32 MemCpy_param_2,
	.param .u32 MemCpy_param_3,
	.param .u32 MemCpy_param_4,
	.param .u32 MemCpy_param_5,
	.param .u32 MemCpy_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [MemCpy_param_0];
	ld.param.u64 	%rd4, [MemCpy_param_1];
	ld.param.u32 	%r6, [MemCpy_param_2];
	ld.param.u32 	%r7, [MemCpy_param_3];
	ld.param.u32 	%r8, [MemCpy_param_4];
	ld.param.u32 	%r9, [MemCpy_param_5];
	ld.param.u32 	%r10, [MemCpy_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	mov.u32 	%r12, %tid.x;
	mad.lo.s32 	%r16, %r1, %r11, %r12;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB23_3;
	mov.u32 	%r13, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r13;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd3;
$L__BB23_2:
	mad.lo.s32 	%r14, %r16, %r10, %r8;
	mul.wide.u32 	%rd5, %r14, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mad.lo.s32 	%r15, %r16, %r9, %r7;
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f1;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB23_2;
$L__BB23_3:
	ret;

}
	// .globl	FindMinAndMax
.visible .entry FindMinAndMax(
	.param .u64 .ptr .align 1 FindMinAndMax_param_0,
	.param .u32 FindMinAndMax_param_1,
	.param .u64 .ptr .align 1 FindMinAndMax_param_2,
	.param .u64 .ptr .align 1 FindMinAndMax_param_3,
	.param .u32 FindMinAndMax_param_4
)
{
	.reg .pred 	%p<46>;
	.reg .b32 	%r<49>;
	.reg .f32 	%f<103>;
	.reg .b64 	%rd<12>;
	// demoted variable
	.shared .align 4 .b8 _ZZ13FindMinAndMaxE5block[4096];
	ld.param.u64 	%rd1, [FindMinAndMax_param_0];
	ld.param.u32 	%r20, [FindMinAndMax_param_1];
	ld.param.u64 	%rd2, [FindMinAndMax_param_2];
	ld.param.u64 	%rd3, [FindMinAndMax_param_3];
	ld.param.u32 	%r21, [FindMinAndMax_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r3, %r22, %r2, %r1;
	setp.le.u32 	%p1, %r20, %r3;
	mov.f32 	%f86, 0f00000000;
	@%p1 bra 	$L__BB24_2;
	cvta.to.global.u64 	%rd4, %rd1;
	mul.lo.s32 	%r23, %r21, %r3;
	mul.wide.u32 	%rd5, %r23, 4;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.f32 	%f86, [%rd6];
$L__BB24_2:
	shl.b32 	%r24, %r1, 2;
	mov.u32 	%r25, _ZZ13FindMinAndMaxE5block;
	add.s32 	%r26, %r25, %r24;
	st.shared.f32 	[%r26], %f86;
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB24_16;
	sub.s32 	%r4, %r20, %r3;
	setp.eq.s32 	%p3, %r4, 0;
	mov.f32 	%f102, 0f00800000;
	mov.f32 	%f101, 0f7F7FFFFF;
	@%p3 bra 	$L__BB24_15;
	min.u32 	%r5, %r4, 1024;
	and.b32  	%r6, %r5, 7;
	setp.lt.u32 	%p4, %r4, 8;
	mov.f32 	%f101, 0f7F7FFFFF;
	mov.f32 	%f102, 0f00800000;
	mov.b32 	%r46, 0;
	@%p4 bra 	$L__BB24_7;
	and.b32  	%r30, %r5, 2040;
	neg.s32 	%r43, %r30;
	mov.f32 	%f101, 0f7F7FFFFF;
	mov.f32 	%f102, 0f00800000;
	mov.b32 	%r46, 0;
	mov.b32 	%r44, 16;
$L__BB24_6:
	.pragma "nounroll";
	add.s32 	%r32, %r25, %r44;
	ld.shared.f32 	%f35, [%r32+-16];
	setp.eq.s32 	%p5, %r44, 16;
	setp.gt.ftz.f32 	%p6, %f35, %f102;
	selp.f32 	%f37, %f35, %f102, %p6;
	selp.f32 	%f38, %f35, %f37, %p5;
	setp.lt.ftz.f32 	%p7, %f35, %f101;
	selp.f32 	%f39, %f35, %f101, %p7;
	selp.f32 	%f40, %f35, %f39, %p5;
	ld.shared.f32 	%f41, [%r32+-12];
	setp.gt.ftz.f32 	%p8, %f41, %f38;
	selp.f32 	%f42, %f41, %f38, %p8;
	setp.lt.ftz.f32 	%p9, %f41, %f40;
	selp.f32 	%f43, %f41, %f40, %p9;
	ld.shared.f32 	%f44, [%r32+-8];
	setp.gt.ftz.f32 	%p10, %f44, %f42;
	selp.f32 	%f45, %f44, %f42, %p10;
	setp.lt.ftz.f32 	%p11, %f44, %f43;
	selp.f32 	%f46, %f44, %f43, %p11;
	ld.shared.f32 	%f47, [%r32+-4];
	setp.gt.ftz.f32 	%p12, %f47, %f45;
	selp.f32 	%f48, %f47, %f45, %p12;
	setp.lt.ftz.f32 	%p13, %f47, %f46;
	selp.f32 	%f49, %f47, %f46, %p13;
	ld.shared.f32 	%f50, [%r32];
	setp.gt.ftz.f32 	%p14, %f50, %f48;
	selp.f32 	%f51, %f50, %f48, %p14;
	setp.lt.ftz.f32 	%p15, %f50, %f49;
	selp.f32 	%f52, %f50, %f49, %p15;
	ld.shared.f32 	%f53, [%r32+4];
	setp.gt.ftz.f32 	%p16, %f53, %f51;
	selp.f32 	%f54, %f53, %f51, %p16;
	setp.lt.ftz.f32 	%p17, %f53, %f52;
	selp.f32 	%f55, %f53, %f52, %p17;
	ld.shared.f32 	%f56, [%r32+8];
	setp.gt.ftz.f32 	%p18, %f56, %f54;
	selp.f32 	%f57, %f56, %f54, %p18;
	setp.lt.ftz.f32 	%p19, %f56, %f55;
	selp.f32 	%f58, %f56, %f55, %p19;
	ld.shared.f32 	%f59, [%r32+12];
	setp.gt.ftz.f32 	%p20, %f59, %f57;
	selp.f32 	%f102, %f59, %f57, %p20;
	setp.lt.ftz.f32 	%p21, %f59, %f58;
	selp.f32 	%f101, %f59, %f58, %p21;
	add.s32 	%r46, %r46, 8;
	add.s32 	%r44, %r44, 32;
	add.s32 	%r43, %r43, 8;
	setp.ne.s32 	%p22, %r43, 0;
	@%p22 bra 	$L__BB24_6;
$L__BB24_7:
	setp.eq.s32 	%p23, %r6, 0;
	@%p23 bra 	$L__BB24_15;
	and.b32  	%r15, %r5, 3;
	setp.lt.u32 	%p24, %r6, 4;
	@%p24 bra 	$L__BB24_10;
	shl.b32 	%r33, %r46, 2;
	add.s32 	%r35, %r25, %r33;
	ld.shared.f32 	%f61, [%r35];
	setp.eq.s32 	%p25, %r46, 0;
	setp.gt.ftz.f32 	%p26, %f61, %f102;
	selp.f32 	%f63, %f61, %f102, %p26;
	selp.f32 	%f64, %f61, %f63, %p25;
	setp.lt.ftz.f32 	%p27, %f61, %f101;
	selp.f32 	%f65, %f61, %f101, %p27;
	selp.f32 	%f66, %f61, %f65, %p25;
	ld.shared.f32 	%f67, [%r35+4];
	setp.gt.ftz.f32 	%p28, %f67, %f64;
	selp.f32 	%f68, %f67, %f64, %p28;
	setp.lt.ftz.f32 	%p29, %f67, %f66;
	selp.f32 	%f69, %f67, %f66, %p29;
	ld.shared.f32 	%f70, [%r35+8];
	setp.gt.ftz.f32 	%p30, %f70, %f68;
	selp.f32 	%f71, %f70, %f68, %p30;
	setp.lt.ftz.f32 	%p31, %f70, %f69;
	selp.f32 	%f72, %f70, %f69, %p31;
	ld.shared.f32 	%f73, [%r35+12];
	setp.gt.ftz.f32 	%p32, %f73, %f71;
	selp.f32 	%f102, %f73, %f71, %p32;
	setp.lt.ftz.f32 	%p33, %f73, %f72;
	selp.f32 	%f101, %f73, %f72, %p33;
	add.s32 	%r46, %r46, 4;
$L__BB24_10:
	setp.eq.s32 	%p34, %r15, 0;
	@%p34 bra 	$L__BB24_15;
	setp.eq.s32 	%p35, %r15, 1;
	@%p35 bra 	$L__BB24_13;
	shl.b32 	%r36, %r46, 2;
	add.s32 	%r38, %r25, %r36;
	ld.shared.f32 	%f75, [%r38];
	setp.eq.s32 	%p36, %r46, 0;
	setp.gt.ftz.f32 	%p37, %f75, %f102;
	selp.f32 	%f77, %f75, %f102, %p37;
	selp.f32 	%f78, %f75, %f77, %p36;
	setp.lt.ftz.f32 	%p38, %f75, %f101;
	selp.f32 	%f79, %f75, %f101, %p38;
	selp.f32 	%f80, %f75, %f79, %p36;
	ld.shared.f32 	%f81, [%r38+4];
	setp.gt.ftz.f32 	%p39, %f81, %f78;
	selp.f32 	%f102, %f81, %f78, %p39;
	setp.lt.ftz.f32 	%p40, %f81, %f80;
	selp.f32 	%f101, %f81, %f80, %p40;
	add.s32 	%r46, %r46, 2;
$L__BB24_13:
	and.b32  	%r39, %r5, 1;
	setp.eq.b32 	%p41, %r39, 1;
	not.pred 	%p42, %p41;
	@%p42 bra 	$L__BB24_15;
	shl.b32 	%r40, %r46, 2;
	add.s32 	%r42, %r25, %r40;
	ld.shared.f32 	%f82, [%r42];
	setp.eq.s32 	%p43, %r46, 0;
	setp.gt.ftz.f32 	%p44, %f82, %f102;
	selp.f32 	%f84, %f82, %f102, %p44;
	selp.f32 	%f102, %f82, %f84, %p43;
	setp.lt.ftz.f32 	%p45, %f82, %f101;
	selp.f32 	%f85, %f82, %f101, %p45;
	selp.f32 	%f101, %f82, %f85, %p43;
$L__BB24_15:
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.u32 	%rd8, %r2, 4;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.f32 	[%rd9], %f101;
	cvta.to.global.u64 	%rd10, %rd3;
	add.s64 	%rd11, %rd10, %rd8;
	st.global.f32 	[%rd11], %f102;
$L__BB24_16:
	ret;

}
	// .globl	FindStdDev
.visible .entry FindStdDev(
	.param .u64 .ptr .align 1 FindStdDev_param_0,
	.param .u32 FindStdDev_param_1,
	.param .f32 FindStdDev_param_2,
	.param .u64 .ptr .align 1 FindStdDev_param_3,
	.param .u32 FindStdDev_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<49>;
	.reg .f32 	%f<70>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ10FindStdDevE5block[4096];
	ld.param.u64 	%rd1, [FindStdDev_param_0];
	ld.param.u32 	%r20, [FindStdDev_param_1];
	ld.param.f32 	%f13, [FindStdDev_param_2];
	ld.param.u64 	%rd2, [FindStdDev_param_3];
	ld.param.u32 	%r21, [FindStdDev_param_4];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r22, %ntid.x;
	mad.lo.s32 	%r3, %r22, %r2, %r1;
	setp.ge.u32 	%p1, %r3, %r20;
	@%p1 bra 	$L__BB25_2;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.lo.s32 	%r23, %r21, %r3;
	mul.wide.u32 	%rd4, %r23, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f14, [%rd5];
	shl.b32 	%r24, %r1, 2;
	mov.u32 	%r25, _ZZ10FindStdDevE5block;
	add.s32 	%r26, %r25, %r24;
	st.shared.f32 	[%r26], %f14;
$L__BB25_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB25_16;
	shl.b32 	%r27, %r2, 10;
	sub.s32 	%r4, %r20, %r27;
	setp.eq.s32 	%p3, %r4, 0;
	mov.f32 	%f69, 0f00000000;
	@%p3 bra 	$L__BB25_15;
	min.u32 	%r5, %r4, 1024;
	and.b32  	%r6, %r5, 7;
	setp.lt.u32 	%p4, %r4, 8;
	mov.f32 	%f69, 0f00000000;
	mov.b32 	%r46, 0;
	@%p4 bra 	$L__BB25_7;
	mov.u32 	%r31, _ZZ10FindStdDevE5block;
	add.s32 	%r43, %r31, 16;
	and.b32  	%r32, %r5, 2040;
	neg.s32 	%r44, %r32;
	mov.f32 	%f69, 0f00000000;
	mov.b32 	%r46, 0;
$L__BB25_6:
	.pragma "nounroll";
	ld.shared.f32 	%f19, [%r43+-16];
	sub.ftz.f32 	%f20, %f19, %f13;
	fma.rn.ftz.f32 	%f21, %f20, %f20, %f69;
	ld.shared.f32 	%f22, [%r43+-12];
	sub.ftz.f32 	%f23, %f22, %f13;
	fma.rn.ftz.f32 	%f24, %f23, %f23, %f21;
	ld.shared.f32 	%f25, [%r43+-8];
	sub.ftz.f32 	%f26, %f25, %f13;
	fma.rn.ftz.f32 	%f27, %f26, %f26, %f24;
	ld.shared.f32 	%f28, [%r43+-4];
	sub.ftz.f32 	%f29, %f28, %f13;
	fma.rn.ftz.f32 	%f30, %f29, %f29, %f27;
	ld.shared.f32 	%f31, [%r43];
	sub.ftz.f32 	%f32, %f31, %f13;
	fma.rn.ftz.f32 	%f33, %f32, %f32, %f30;
	ld.shared.f32 	%f34, [%r43+4];
	sub.ftz.f32 	%f35, %f34, %f13;
	fma.rn.ftz.f32 	%f36, %f35, %f35, %f33;
	ld.shared.f32 	%f37, [%r43+8];
	sub.ftz.f32 	%f38, %f37, %f13;
	fma.rn.ftz.f32 	%f39, %f38, %f38, %f36;
	ld.shared.f32 	%f40, [%r43+12];
	sub.ftz.f32 	%f41, %f40, %f13;
	fma.rn.ftz.f32 	%f69, %f41, %f41, %f39;
	add.s32 	%r46, %r46, 8;
	add.s32 	%r44, %r44, 8;
	add.s32 	%r43, %r43, 32;
	setp.ne.s32 	%p5, %r44, 0;
	@%p5 bra 	$L__BB25_6;
$L__BB25_7:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB25_15;
	and.b32  	%r15, %r5, 3;
	setp.lt.u32 	%p7, %r6, 4;
	@%p7 bra 	$L__BB25_10;
	shl.b32 	%r33, %r46, 2;
	mov.u32 	%r34, _ZZ10FindStdDevE5block;
	add.s32 	%r35, %r34, %r33;
	ld.shared.f32 	%f43, [%r35];
	sub.ftz.f32 	%f44, %f43, %f13;
	fma.rn.ftz.f32 	%f45, %f44, %f44, %f69;
	ld.shared.f32 	%f46, [%r35+4];
	sub.ftz.f32 	%f47, %f46, %f13;
	fma.rn.ftz.f32 	%f48, %f47, %f47, %f45;
	ld.shared.f32 	%f49, [%r35+8];
	sub.ftz.f32 	%f50, %f49, %f13;
	fma.rn.ftz.f32 	%f51, %f50, %f50, %f48;
	ld.shared.f32 	%f52, [%r35+12];
	sub.ftz.f32 	%f53, %f52, %f13;
	fma.rn.ftz.f32 	%f69, %f53, %f53, %f51;
	add.s32 	%r46, %r46, 4;
$L__BB25_10:
	setp.eq.s32 	%p8, %r15, 0;
	@%p8 bra 	$L__BB25_15;
	setp.eq.s32 	%p9, %r15, 1;
	@%p9 bra 	$L__BB25_13;
	shl.b32 	%r36, %r46, 2;
	mov.u32 	%r37, _ZZ10FindStdDevE5block;
	add.s32 	%r38, %r37, %r36;
	ld.shared.f32 	%f55, [%r38];
	sub.ftz.f32 	%f56, %f55, %f13;
	fma.rn.ftz.f32 	%f57, %f56, %f56, %f69;
	ld.shared.f32 	%f58, [%r38+4];
	sub.ftz.f32 	%f59, %f58, %f13;
	fma.rn.ftz.f32 	%f69, %f59, %f59, %f57;
	add.s32 	%r46, %r46, 2;
$L__BB25_13:
	and.b32  	%r39, %r5, 1;
	setp.eq.b32 	%p10, %r39, 1;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB25_15;
	shl.b32 	%r40, %r46, 2;
	mov.u32 	%r41, _ZZ10FindStdDevE5block;
	add.s32 	%r42, %r41, %r40;
	ld.shared.f32 	%f60, [%r42];
	sub.ftz.f32 	%f61, %f60, %f13;
	fma.rn.ftz.f32 	%f69, %f61, %f61, %f69;
$L__BB25_15:
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f69;
$L__BB25_16:
	ret;

}
	// .globl	Constrain
.visible .entry Constrain(
	.param .u64 .ptr .align 1 Constrain_param_0,
	.param .u32 Constrain_param_1,
	.param .f32 Constrain_param_2,
	.param .f32 Constrain_param_3,
	.param .u32 Constrain_param_4
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<7>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd3, [Constrain_param_0];
	ld.param.u32 	%r6, [Constrain_param_1];
	ld.param.f32 	%f2, [Constrain_param_2];
	ld.param.f32 	%f3, [Constrain_param_3];
	ld.param.u32 	%r7, [Constrain_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB26_5;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd3;
$L__BB26_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd4, %r11, 4;
	add.s64 	%rd2, %rd1, %rd4;
	ld.global.f32 	%f4, [%rd2];
	setp.lt.ftz.f32 	%p2, %f4, %f2;
	setp.eq.ftz.f32 	%p3, %f4, 0fFF800000;
	or.pred  	%p4, %p2, %p3;
	setp.gt.ftz.f32 	%p5, %f4, %f3;
	setp.eq.ftz.f32 	%p6, %f4, 0f7F800000;
	or.pred  	%p7, %p5, %p6;
	selp.f32 	%f5, %f3, %f2, %p7;
	or.pred  	%p8, %p4, %p7;
	abs.ftz.f32 	%f6, %f4;
	setp.nan.ftz.f32 	%p9, %f6, %f6;
	selp.f32 	%f1, 0f00000000, %f5, %p9;
	or.pred  	%p10, %p9, %p8;
	not.pred 	%p11, %p10;
	@%p11 bra 	$L__BB26_4;
	st.global.f32 	[%rd2], %f1;
$L__BB26_4:
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p12, %r12, %r6;
	@%p12 bra 	$L__BB26_2;
$L__BB26_5:
	ret;

}
	// .globl	RoundInPlace
.visible .entry RoundInPlace(
	.param .u64 .ptr .align 1 RoundInPlace_param_0,
	.param .u32 RoundInPlace_param_1,
	.param .f32 RoundInPlace_param_2,
	.param .f32 RoundInPlace_param_3,
	.param .f32 RoundInPlace_param_4,
	.param .u32 RoundInPlace_param_5
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [RoundInPlace_param_0];
	ld.param.u32 	%r7, [RoundInPlace_param_1];
	ld.param.f32 	%f1, [RoundInPlace_param_2];
	ld.param.f32 	%f2, [RoundInPlace_param_3];
	ld.param.f32 	%f3, [RoundInPlace_param_4];
	ld.param.u32 	%r8, [RoundInPlace_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r13, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r13, %r7;
	@%p1 bra 	$L__BB27_3;
	mul.lo.s32 	%r3, %r8, %r8;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r4, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB27_2:
	mul.lo.s32 	%r12, %r3, %r13;
	mul.wide.u32 	%rd3, %r12, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f4, [%rd4];
	setp.ltu.ftz.f32 	%p2, %f4, %f3;
	selp.f32 	%f5, %f1, %f2, %p2;
	st.global.f32 	[%rd4], %f5;
	add.s32 	%r13, %r13, %r4;
	setp.lt.u32 	%p3, %r13, %r7;
	@%p3 bra 	$L__BB27_2;
$L__BB27_3:
	ret;

}
	// .globl	Pow
.visible .entry Pow(
	.param .u64 .ptr .align 1 Pow_param_0,
	.param .u64 .ptr .align 1 Pow_param_1,
	.param .u32 Pow_param_2,
	.param .f32 Pow_param_3,
	.param .u32 Pow_param_4,
	.param .u32 Pow_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Pow_param_0];
	ld.param.u64 	%rd4, [Pow_param_1];
	ld.param.u32 	%r6, [Pow_param_2];
	ld.param.f32 	%f1, [Pow_param_3];
	ld.param.u32 	%r7, [Pow_param_4];
	ld.param.u32 	%r8, [Pow_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB28_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB28_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	lg2.approx.ftz.f32 	%f3, %f2;
	mul.ftz.f32 	%f4, %f1, %f3;
	ex2.approx.ftz.f32 	%f5, %f4;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f5;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB28_2;
$L__BB28_3:
	ret;

}
	// .globl	Diagonal
.visible .entry Diagonal(
	.param .u64 .ptr .align 1 Diagonal_param_0,
	.param .u64 .ptr .align 1 Diagonal_param_1,
	.param .u32 Diagonal_param_2,
	.param .u32 Diagonal_param_3,
	.param .u32 Diagonal_param_4,
	.param .u32 Diagonal_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Diagonal_param_0];
	ld.param.u64 	%rd4, [Diagonal_param_1];
	ld.param.u32 	%r9, [Diagonal_param_2];
	ld.param.u32 	%r12, [Diagonal_param_3];
	ld.param.u32 	%r10, [Diagonal_param_4];
	ld.param.u32 	%r11, [Diagonal_param_5];
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r19, %r2, %r13, %r14;
	min.u32 	%r15, %r9, %r12;
	setp.le.u32 	%p1, %r15, %r19;
	@%p1 bra 	$L__BB29_3;
	mul.lo.s32 	%r4, %r10, %r9;
	mov.u32 	%r16, %nctaid.x;
	mul.lo.s32 	%r5, %r2, %r16;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB29_2:
	mad.lo.s32 	%r17, %r4, %r19, %r19;
	mul.wide.u32 	%rd5, %r17, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.lo.s32 	%r18, %r19, %r11;
	mul.wide.u32 	%rd7, %r18, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f1;
	add.s32 	%r19, %r19, %r5;
	setp.lt.u32 	%p2, %r19, %r15;
	@%p2 bra 	$L__BB29_2;
$L__BB29_3:
	ret;

}
	// .globl	L1Regularisation
.visible .entry L1Regularisation(
	.param .u64 .ptr .align 1 L1Regularisation_param_0,
	.param .u32 L1Regularisation_param_1,
	.param .f32 L1Regularisation_param_2,
	.param .u32 L1Regularisation_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<7>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [L1Regularisation_param_0];
	ld.param.u32 	%r6, [L1Regularisation_param_1];
	ld.param.f32 	%f1, [L1Regularisation_param_2];
	ld.param.u32 	%r7, [L1Regularisation_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB30_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB30_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd3, %r11, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f2, [%rd4];
	setp.gt.ftz.f32 	%p2, %f2, 0f00000000;
	setp.lt.ftz.f32 	%p3, %f2, 0f00000000;
	selp.f32 	%f3, 0fBF800000, 0f00000000, %p3;
	selp.f32 	%f4, 0f3F800000, %f3, %p2;
	mul.ftz.f32 	%f5, %f1, %f4;
	sub.ftz.f32 	%f6, %f2, %f5;
	st.global.f32 	[%rd4], %f6;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p4, %r12, %r6;
	@%p4 bra 	$L__BB30_2;
$L__BB30_3:
	ret;

}
	// .globl	PointwiseDivideRows
.visible .entry PointwiseDivideRows(
	.param .u64 .ptr .align 1 PointwiseDivideRows_param_0,
	.param .u64 .ptr .align 1 PointwiseDivideRows_param_1,
	.param .u32 PointwiseDivideRows_param_2,
	.param .u32 PointwiseDivideRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [PointwiseDivideRows_param_0];
	ld.param.u64 	%rd4, [PointwiseDivideRows_param_1];
	ld.param.u32 	%r10, [PointwiseDivideRows_param_2];
	ld.param.u32 	%r11, [PointwiseDivideRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB31_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB31_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB31_5;
	mul.wide.u32 	%rd5, %r20, 4;
	add.s64 	%rd6, %rd2, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mov.u32 	%r21, %r3;
$L__BB31_4:
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd7, %r19, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f2, [%rd8];
	div.approx.ftz.f32 	%f3, %f2, %f1;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB31_4;
$L__BB31_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB31_2;
$L__BB31_6:
	ret;

}
	// .globl	PointwiseDivideColumns
.visible .entry PointwiseDivideColumns(
	.param .u64 .ptr .align 1 PointwiseDivideColumns_param_0,
	.param .u64 .ptr .align 1 PointwiseDivideColumns_param_1,
	.param .u32 PointwiseDivideColumns_param_2,
	.param .u32 PointwiseDivideColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [PointwiseDivideColumns_param_0];
	ld.param.u64 	%rd4, [PointwiseDivideColumns_param_1];
	ld.param.u32 	%r10, [PointwiseDivideColumns_param_2];
	ld.param.u32 	%r11, [PointwiseDivideColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB32_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB32_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB32_5;
	mov.u32 	%r21, %r3;
$L__BB32_4:
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd5, %r19, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f1, [%rd6];
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd2, %rd7;
	ld.global.nc.f32 	%f2, [%rd8];
	div.approx.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd6], %f3;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB32_4;
$L__BB32_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB32_2;
$L__BB32_6:
	ret;

}
	// .globl	SplitRows
.visible .entry SplitRows(
	.param .u64 .ptr .align 1 SplitRows_param_0,
	.param .u64 .ptr .align 1 SplitRows_param_1,
	.param .u64 .ptr .align 1 SplitRows_param_2,
	.param .u32 SplitRows_param_3,
	.param .u32 SplitRows_param_4,
	.param .u32 SplitRows_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<25>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<14>;

	ld.param.u64 	%rd5, [SplitRows_param_0];
	ld.param.u64 	%rd6, [SplitRows_param_1];
	ld.param.u64 	%rd7, [SplitRows_param_2];
	ld.param.u32 	%r10, [SplitRows_param_3];
	ld.param.u32 	%r11, [SplitRows_param_4];
	ld.param.u32 	%r12, [SplitRows_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r23, %r1, %r13, %r14;
	setp.ge.u32 	%p1, %r23, %r10;
	@%p1 bra 	$L__BB33_9;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r15, %r16, %r17;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r4, %r15, %r18;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r19;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB33_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB33_8;
	mov.u32 	%r24, %r3;
$L__BB33_4:
	mad.lo.s32 	%r20, %r24, %r10, %r23;
	cvt.u64.u32 	%rd4, %r20;
	mul.wide.u32 	%rd8, %r20, 4;
	add.s64 	%rd9, %rd1, %rd8;
	ld.global.nc.f32 	%f1, [%rd9];
	setp.lt.u32 	%p3, %r24, %r12;
	@%p3 bra 	$L__BB33_6;
	sub.s32 	%r21, %r24, %r12;
	mad.lo.s32 	%r22, %r21, %r10, %r23;
	mul.wide.u32 	%rd10, %r22, 4;
	add.s64 	%rd11, %rd2, %rd10;
	st.global.f32 	[%rd11], %f1;
	bra.uni 	$L__BB33_7;
$L__BB33_6:
	shl.b64 	%rd12, %rd4, 2;
	add.s64 	%rd13, %rd3, %rd12;
	st.global.f32 	[%rd13], %f1;
$L__BB33_7:
	add.s32 	%r24, %r24, %r4;
	setp.lt.u32 	%p4, %r24, %r11;
	@%p4 bra 	$L__BB33_4;
$L__BB33_8:
	add.s32 	%r23, %r23, %r5;
	setp.lt.u32 	%p5, %r23, %r10;
	@%p5 bra 	$L__BB33_2;
$L__BB33_9:
	ret;

}
	// .globl	SplitColumns
.visible .entry SplitColumns(
	.param .u64 .ptr .align 1 SplitColumns_param_0,
	.param .u64 .ptr .align 1 SplitColumns_param_1,
	.param .u64 .ptr .align 1 SplitColumns_param_2,
	.param .u32 SplitColumns_param_3,
	.param .u32 SplitColumns_param_4,
	.param .u32 SplitColumns_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<27>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [SplitColumns_param_0];
	ld.param.u64 	%rd5, [SplitColumns_param_1];
	ld.param.u64 	%rd6, [SplitColumns_param_2];
	ld.param.u32 	%r12, [SplitColumns_param_3];
	ld.param.u32 	%r13, [SplitColumns_param_4];
	ld.param.u32 	%r14, [SplitColumns_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r25, %r1, %r15, %r16;
	setp.ge.u32 	%p1, %r25, %r12;
	@%p1 bra 	$L__BB34_9;
	mov.u32 	%r17, %ntid.y;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %tid.y;
	mad.lo.s32 	%r3, %r17, %r18, %r19;
	sub.s32 	%r4, %r12, %r14;
	mov.u32 	%r20, %nctaid.y;
	mul.lo.s32 	%r5, %r17, %r20;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r21;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd6;
	cvta.to.global.u64 	%rd3, %rd5;
$L__BB34_2:
	setp.ge.u32 	%p2, %r3, %r13;
	@%p2 bra 	$L__BB34_8;
	sub.s32 	%r8, %r25, %r14;
	mov.u32 	%r26, %r3;
$L__BB34_4:
	setp.lt.u32 	%p3, %r25, %r14;
	mad.lo.s32 	%r22, %r26, %r12, %r25;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	@%p3 bra 	$L__BB34_6;
	mad.lo.s32 	%r23, %r26, %r4, %r8;
	mul.wide.u32 	%rd9, %r23, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.f32 	[%rd10], %f1;
	bra.uni 	$L__BB34_7;
$L__BB34_6:
	mad.lo.s32 	%r24, %r26, %r14, %r25;
	mul.wide.u32 	%rd11, %r24, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f32 	[%rd12], %f1;
$L__BB34_7:
	add.s32 	%r26, %r26, %r5;
	setp.lt.u32 	%p4, %r26, %r13;
	@%p4 bra 	$L__BB34_4;
$L__BB34_8:
	add.s32 	%r25, %r25, %r6;
	setp.lt.u32 	%p5, %r25, %r12;
	@%p5 bra 	$L__BB34_2;
$L__BB34_9:
	ret;

}
	// .globl	ConcatColumns
.visible .entry ConcatColumns(
	.param .u64 .ptr .align 1 ConcatColumns_param_0,
	.param .u64 .ptr .align 1 ConcatColumns_param_1,
	.param .u64 .ptr .align 1 ConcatColumns_param_2,
	.param .u32 ConcatColumns_param_3,
	.param .u32 ConcatColumns_param_4,
	.param .u32 ConcatColumns_param_5,
	.param .u32 ConcatColumns_param_6
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<27>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [ConcatColumns_param_0];
	ld.param.u64 	%rd5, [ConcatColumns_param_1];
	ld.param.u64 	%rd6, [ConcatColumns_param_2];
	ld.param.u32 	%r11, [ConcatColumns_param_3];
	ld.param.u32 	%r12, [ConcatColumns_param_4];
	ld.param.u32 	%r13, [ConcatColumns_param_5];
	ld.param.u32 	%r14, [ConcatColumns_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r15, %ctaid.x;
	mov.u32 	%r16, %tid.x;
	mad.lo.s32 	%r25, %r1, %r15, %r16;
	setp.ge.u32 	%p1, %r25, %r11;
	@%p1 bra 	$L__BB35_9;
	mov.u32 	%r17, %ntid.y;
	mov.u32 	%r18, %ctaid.y;
	mov.u32 	%r19, %tid.y;
	mad.lo.s32 	%r3, %r17, %r18, %r19;
	mov.u32 	%r20, %nctaid.y;
	mul.lo.s32 	%r4, %r17, %r20;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r21;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB35_2:
	setp.ge.u32 	%p2, %r3, %r12;
	@%p2 bra 	$L__BB35_8;
	sub.s32 	%r7, %r25, %r13;
	mov.u32 	%r26, %r3;
$L__BB35_4:
	setp.lt.u32 	%p3, %r25, %r13;
	@%p3 bra 	$L__BB35_6;
	mad.lo.s32 	%r22, %r26, %r14, %r7;
	mul.wide.u32 	%rd7, %r22, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f4, [%rd8];
	bra.uni 	$L__BB35_7;
$L__BB35_6:
	mad.lo.s32 	%r23, %r26, %r13, %r25;
	mul.wide.u32 	%rd9, %r23, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f4, [%rd10];
$L__BB35_7:
	mad.lo.s32 	%r24, %r26, %r11, %r25;
	mul.wide.u32 	%rd11, %r24, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r26, %r26, %r4;
	setp.lt.u32 	%p4, %r26, %r12;
	@%p4 bra 	$L__BB35_4;
$L__BB35_8:
	add.s32 	%r25, %r25, %r5;
	setp.lt.u32 	%p5, %r25, %r11;
	@%p5 bra 	$L__BB35_2;
$L__BB35_9:
	ret;

}
	// .globl	ConcatRows
.visible .entry ConcatRows(
	.param .u64 .ptr .align 1 ConcatRows_param_0,
	.param .u64 .ptr .align 1 ConcatRows_param_1,
	.param .u64 .ptr .align 1 ConcatRows_param_2,
	.param .u32 ConcatRows_param_3,
	.param .u32 ConcatRows_param_4,
	.param .u32 ConcatRows_param_5
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<26>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [ConcatRows_param_0];
	ld.param.u64 	%rd5, [ConcatRows_param_1];
	ld.param.u64 	%rd6, [ConcatRows_param_2];
	ld.param.u32 	%r10, [ConcatRows_param_3];
	ld.param.u32 	%r11, [ConcatRows_param_4];
	ld.param.u32 	%r12, [ConcatRows_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r13, %ctaid.x;
	mov.u32 	%r14, %tid.x;
	mad.lo.s32 	%r24, %r1, %r13, %r14;
	setp.ge.u32 	%p1, %r24, %r10;
	@%p1 bra 	$L__BB36_9;
	mov.u32 	%r15, %ntid.y;
	mov.u32 	%r16, %ctaid.y;
	mov.u32 	%r17, %tid.y;
	mad.lo.s32 	%r3, %r15, %r16, %r17;
	mov.u32 	%r18, %nctaid.y;
	mul.lo.s32 	%r4, %r15, %r18;
	mov.u32 	%r19, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r19;
	cvta.to.global.u64 	%rd1, %rd5;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB36_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB36_8;
	mov.u32 	%r25, %r3;
$L__BB36_4:
	setp.lt.u32 	%p3, %r25, %r12;
	@%p3 bra 	$L__BB36_6;
	sub.s32 	%r20, %r25, %r12;
	mad.lo.s32 	%r21, %r20, %r10, %r24;
	mul.wide.u32 	%rd7, %r21, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f4, [%rd8];
	bra.uni 	$L__BB36_7;
$L__BB36_6:
	mad.lo.s32 	%r22, %r25, %r10, %r24;
	mul.wide.u32 	%rd9, %r22, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f4, [%rd10];
$L__BB36_7:
	mad.lo.s32 	%r23, %r25, %r10, %r24;
	mul.wide.u32 	%rd11, %r23, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r25, %r25, %r4;
	setp.lt.u32 	%p4, %r25, %r11;
	@%p4 bra 	$L__BB36_4;
$L__BB36_8:
	add.s32 	%r24, %r24, %r5;
	setp.lt.u32 	%p5, %r24, %r10;
	@%p5 bra 	$L__BB36_2;
$L__BB36_9:
	ret;

}
	// .globl	EuclideanDistance
.visible .entry EuclideanDistance(
	.param .u64 .ptr .align 1 EuclideanDistance_param_0,
	.param .u64 .ptr .align 1 EuclideanDistance_param_1,
	.param .u64 .ptr .align 1 EuclideanDistance_param_2,
	.param .u32 EuclideanDistance_param_3,
	.param .u32 EuclideanDistance_param_4,
	.param .u32 EuclideanDistance_param_5,
	.param .u32 EuclideanDistance_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [EuclideanDistance_param_0];
	ld.param.u64 	%rd5, [EuclideanDistance_param_1];
	ld.param.u64 	%rd6, [EuclideanDistance_param_2];
	ld.param.u32 	%r6, [EuclideanDistance_param_3];
	ld.param.u32 	%r7, [EuclideanDistance_param_4];
	ld.param.u32 	%r8, [EuclideanDistance_param_5];
	ld.param.u32 	%r9, [EuclideanDistance_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r16, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB37_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB37_2:
	mul.lo.s32 	%r13, %r16, %r7;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	mul.lo.s32 	%r14, %r16, %r8;
	mul.wide.u32 	%rd9, %r14, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f2, [%rd10];
	sub.ftz.f32 	%f3, %f1, %f2;
	mul.ftz.f32 	%f4, %f3, %f3;
	mul.lo.s32 	%r15, %r16, %r9;
	mul.wide.u32 	%rd11, %r15, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB37_2;
$L__BB37_3:
	ret;

}
	// .globl	ManhattanDistance
.visible .entry ManhattanDistance(
	.param .u64 .ptr .align 1 ManhattanDistance_param_0,
	.param .u64 .ptr .align 1 ManhattanDistance_param_1,
	.param .u64 .ptr .align 1 ManhattanDistance_param_2,
	.param .u32 ManhattanDistance_param_3,
	.param .u32 ManhattanDistance_param_4,
	.param .u32 ManhattanDistance_param_5,
	.param .u32 ManhattanDistance_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [ManhattanDistance_param_0];
	ld.param.u64 	%rd5, [ManhattanDistance_param_1];
	ld.param.u64 	%rd6, [ManhattanDistance_param_2];
	ld.param.u32 	%r6, [ManhattanDistance_param_3];
	ld.param.u32 	%r7, [ManhattanDistance_param_4];
	ld.param.u32 	%r8, [ManhattanDistance_param_5];
	ld.param.u32 	%r9, [ManhattanDistance_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r10, %ctaid.x;
	mov.u32 	%r11, %tid.x;
	mad.lo.s32 	%r16, %r1, %r10, %r11;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB38_3;
	mov.u32 	%r12, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r12;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB38_2:
	mul.lo.s32 	%r13, %r16, %r7;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f1, [%rd8];
	mul.lo.s32 	%r14, %r16, %r8;
	mul.wide.u32 	%rd9, %r14, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f2, [%rd10];
	sub.ftz.f32 	%f3, %f1, %f2;
	abs.ftz.f32 	%f4, %f3;
	mul.lo.s32 	%r15, %r16, %r9;
	mul.wide.u32 	%rd11, %r15, 4;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.f32 	[%rd12], %f4;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB38_2;
$L__BB38_3:
	ret;

}
	// .globl	CosineDistance
.visible .entry CosineDistance(
	.param .u64 .ptr .align 1 CosineDistance_param_0,
	.param .u64 .ptr .align 1 CosineDistance_param_1,
	.param .u64 .ptr .align 1 CosineDistance_param_2,
	.param .u64 .ptr .align 1 CosineDistance_param_3,
	.param .u64 .ptr .align 1 CosineDistance_param_4,
	.param .u32 CosineDistance_param_5,
	.param .u32 CosineDistance_param_6,
	.param .u32 CosineDistance_param_7
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<15>;

	ld.param.u64 	%rd6, [CosineDistance_param_0];
	ld.param.u64 	%rd7, [CosineDistance_param_1];
	ld.param.u64 	%rd8, [CosineDistance_param_2];
	ld.param.u64 	%rd9, [CosineDistance_param_3];
	ld.param.u64 	%rd10, [CosineDistance_param_4];
	ld.param.u32 	%r6, [CosineDistance_param_5];
	ld.param.u32 	%r7, [CosineDistance_param_6];
	ld.param.u32 	%r8, [CosineDistance_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB39_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd9;
	cvta.to.global.u64 	%rd5, %rd10;
$L__BB39_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd11, %r12, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd13, %r13, 4;
	add.s64 	%rd14, %rd2, %rd13;
	ld.global.nc.f32 	%f2, [%rd14];
	mul.ftz.f32 	%f3, %f1, %f1;
	atom.global.add.f32 	%f4, [%rd3], %f3;
	mul.ftz.f32 	%f5, %f1, %f2;
	atom.global.add.f32 	%f6, [%rd4], %f5;
	mul.ftz.f32 	%f7, %f2, %f2;
	atom.global.add.f32 	%f8, [%rd5], %f7;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB39_2;
$L__BB39_3:
	ret;

}
	// .globl	Abs
.visible .entry Abs(
	.param .u64 .ptr .align 1 Abs_param_0,
	.param .u64 .ptr .align 1 Abs_param_1,
	.param .u32 Abs_param_2,
	.param .u32 Abs_param_3,
	.param .u32 Abs_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Abs_param_0];
	ld.param.u64 	%rd4, [Abs_param_1];
	ld.param.u32 	%r6, [Abs_param_2];
	ld.param.u32 	%r7, [Abs_param_3];
	ld.param.u32 	%r8, [Abs_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB40_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB40_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	abs.ftz.f32 	%f2, %f1;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f2;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB40_2;
$L__BB40_3:
	ret;

}
	// .globl	Log
.visible .entry Log(
	.param .u64 .ptr .align 1 Log_param_0,
	.param .u64 .ptr .align 1 Log_param_1,
	.param .u32 Log_param_2,
	.param .u32 Log_param_3,
	.param .u32 Log_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Log_param_0];
	ld.param.u64 	%rd4, [Log_param_1];
	ld.param.u32 	%r6, [Log_param_2];
	ld.param.u32 	%r7, [Log_param_3];
	ld.param.u32 	%r8, [Log_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB41_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB41_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	lg2.approx.ftz.f32 	%f2, %f1;
	mul.ftz.f32 	%f3, %f2, 0f3F317218;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB41_2;
$L__BB41_3:
	ret;

}
	// .globl	Exp
.visible .entry Exp(
	.param .u64 .ptr .align 1 Exp_param_0,
	.param .u64 .ptr .align 1 Exp_param_1,
	.param .u32 Exp_param_2,
	.param .u32 Exp_param_3,
	.param .u32 Exp_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [Exp_param_0];
	ld.param.u64 	%rd4, [Exp_param_1];
	ld.param.u32 	%r6, [Exp_param_2];
	ld.param.u32 	%r7, [Exp_param_3];
	ld.param.u32 	%r8, [Exp_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r14, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r14, %r6;
	@%p1 bra 	$L__BB42_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB42_2:
	mul.lo.s32 	%r12, %r14, %r7;
	mul.wide.u32 	%rd5, %r12, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f1, [%rd6];
	mul.ftz.f32 	%f2, %f1, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f3, %f2;
	mul.lo.s32 	%r13, %r14, %r8;
	mul.wide.u32 	%rd7, %r13, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f3;
	add.s32 	%r14, %r14, %r3;
	setp.lt.u32 	%p2, %r14, %r6;
	@%p2 bra 	$L__BB42_2;
$L__BB42_3:
	ret;

}
	// .globl	Normalise
.visible .entry Normalise(
	.param .u64 .ptr .align 1 Normalise_param_0,
	.param .u32 Normalise_param_1,
	.param .f32 Normalise_param_2,
	.param .f32 Normalise_param_3,
	.param .u32 Normalise_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [Normalise_param_0];
	ld.param.u32 	%r6, [Normalise_param_1];
	ld.param.f32 	%f1, [Normalise_param_2];
	ld.param.f32 	%f2, [Normalise_param_3];
	ld.param.u32 	%r7, [Normalise_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB43_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB43_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd3, %r11, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f3, [%rd4];
	sub.ftz.f32 	%f4, %f3, %f1;
	div.approx.ftz.f32 	%f5, %f4, %f2;
	st.global.f32 	[%rd4], %f5;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p2, %r12, %r6;
	@%p2 bra 	$L__BB43_2;
$L__BB43_3:
	ret;

}
	// .globl	SoftmaxVector
.visible .entry SoftmaxVector(
	.param .u64 .ptr .align 1 SoftmaxVector_param_0,
	.param .u64 .ptr .align 1 SoftmaxVector_param_1,
	.param .u32 SoftmaxVector_param_2,
	.param .f32 SoftmaxVector_param_3,
	.param .u32 SoftmaxVector_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd3, [SoftmaxVector_param_0];
	ld.param.u64 	%rd4, [SoftmaxVector_param_1];
	ld.param.u32 	%r6, [SoftmaxVector_param_2];
	ld.param.f32 	%f1, [SoftmaxVector_param_3];
	ld.param.u32 	%r7, [SoftmaxVector_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB44_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB44_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd5, %r11, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.nc.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	mul.ftz.f32 	%f4, %f3, 0f3FB8AA3B;
	ex2.approx.ftz.f32 	%f5, %f4;
	mul.wide.u32 	%rd7, %r12, 4;
	add.s64 	%rd8, %rd2, %rd7;
	st.global.f32 	[%rd8], %f5;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p2, %r12, %r6;
	@%p2 bra 	$L__BB44_2;
$L__BB44_3:
	ret;

}
	// .globl	VectorAddInPlace
.visible .entry VectorAddInPlace(
	.param .u64 .ptr .align 1 VectorAddInPlace_param_0,
	.param .u32 VectorAddInPlace_param_1,
	.param .f32 VectorAddInPlace_param_2,
	.param .u32 VectorAddInPlace_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<13>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<5>;

	ld.param.u64 	%rd2, [VectorAddInPlace_param_0];
	ld.param.u32 	%r6, [VectorAddInPlace_param_1];
	ld.param.f32 	%f1, [VectorAddInPlace_param_2];
	ld.param.u32 	%r7, [VectorAddInPlace_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r12, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r12, %r6;
	@%p1 bra 	$L__BB45_3;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
	cvta.to.global.u64 	%rd1, %rd2;
$L__BB45_2:
	mul.lo.s32 	%r11, %r12, %r7;
	mul.wide.u32 	%rd3, %r11, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f2, [%rd4];
	add.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd4], %f3;
	add.s32 	%r12, %r12, %r3;
	setp.lt.u32 	%p2, %r12, %r6;
	@%p2 bra 	$L__BB45_2;
$L__BB45_3:
	ret;

}
	// .globl	VectorCopyRandom
.visible .entry VectorCopyRandom(
	.param .u64 .ptr .align 1 VectorCopyRandom_param_0,
	.param .u64 .ptr .align 1 VectorCopyRandom_param_1,
	.param .u64 .ptr .align 1 VectorCopyRandom_param_2,
	.param .u32 VectorCopyRandom_param_3,
	.param .u32 VectorCopyRandom_param_4,
	.param .u32 VectorCopyRandom_param_5
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<16>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [VectorCopyRandom_param_0];
	ld.param.u64 	%rd5, [VectorCopyRandom_param_1];
	ld.param.u64 	%rd6, [VectorCopyRandom_param_2];
	ld.param.u32 	%r6, [VectorCopyRandom_param_3];
	ld.param.u32 	%r7, [VectorCopyRandom_param_4];
	ld.param.u32 	%r8, [VectorCopyRandom_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r9, %ctaid.x;
	mov.u32 	%r10, %tid.x;
	mad.lo.s32 	%r15, %r1, %r9, %r10;
	setp.ge.u32 	%p1, %r15, %r6;
	@%p1 bra 	$L__BB46_3;
	mov.u32 	%r11, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r11;
	cvta.to.global.u64 	%rd1, %rd6;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
$L__BB46_2:
	mul.wide.u32 	%rd7, %r15, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.u32 	%r12, [%rd8];
	mul.lo.s32 	%r13, %r12, %r7;
	mul.wide.u32 	%rd9, %r13, 4;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	mul.lo.s32 	%r14, %r15, %r8;
	mul.wide.u32 	%rd11, %r14, 4;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.f32 	%f2, [%rd12];
	add.ftz.f32 	%f3, %f1, %f2;
	st.global.f32 	[%rd12], %f3;
	add.s32 	%r15, %r15, %r3;
	setp.lt.u32 	%p2, %r15, %r6;
	@%p2 bra 	$L__BB46_2;
$L__BB46_3:
	ret;

}
	// .globl	CopyToMatrixRows
.visible .entry CopyToMatrixRows(
	.param .u64 .ptr .align 1 CopyToMatrixRows_param_0,
	.param .u64 .ptr .align 1 CopyToMatrixRows_param_1,
	.param .u32 CopyToMatrixRows_param_2,
	.param .u32 CopyToMatrixRows_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd4, [CopyToMatrixRows_param_0];
	ld.param.u64 	%rd5, [CopyToMatrixRows_param_1];
	ld.param.u32 	%r10, [CopyToMatrixRows_param_2];
	ld.param.u32 	%r11, [CopyToMatrixRows_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB47_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
$L__BB47_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB47_5;
	mul.wide.u32 	%rd6, %r20, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.u64 	%rd8, [%rd7];
	cvta.to.global.u64 	%rd3, %rd8;
	mov.u32 	%r21, %r3;
$L__BB47_4:
	mul.wide.u32 	%rd9, %r21, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd11, %r19, 4;
	add.s64 	%rd12, %rd2, %rd11;
	st.global.f32 	[%rd12], %f1;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB47_4;
$L__BB47_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB47_2;
$L__BB47_6:
	ret;

}
	// .globl	CopyToMatrixColumns
.visible .entry CopyToMatrixColumns(
	.param .u64 .ptr .align 1 CopyToMatrixColumns_param_0,
	.param .u64 .ptr .align 1 CopyToMatrixColumns_param_1,
	.param .u32 CopyToMatrixColumns_param_2,
	.param .u32 CopyToMatrixColumns_param_3
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<22>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<14>;

	ld.param.u64 	%rd4, [CopyToMatrixColumns_param_0];
	ld.param.u64 	%rd5, [CopyToMatrixColumns_param_1];
	ld.param.u32 	%r10, [CopyToMatrixColumns_param_2];
	ld.param.u32 	%r11, [CopyToMatrixColumns_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r20, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r20, %r10;
	@%p1 bra 	$L__BB48_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
$L__BB48_2:
	setp.ge.u32 	%p2, %r3, %r11;
	@%p2 bra 	$L__BB48_5;
	cvt.u64.u32 	%rd3, %r20;
	shl.b64 	%rd10, %rd3, 2;
	mov.u32 	%r21, %r3;
$L__BB48_4:
	mul.wide.u32 	%rd6, %r21, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.nc.u64 	%rd8, [%rd7];
	cvta.to.global.u64 	%rd9, %rd8;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.nc.f32 	%f1, [%rd11];
	mad.lo.s32 	%r19, %r21, %r10, %r20;
	mul.wide.u32 	%rd12, %r19, 4;
	add.s64 	%rd13, %rd2, %rd12;
	st.global.f32 	[%rd13], %f1;
	add.s32 	%r21, %r21, %r4;
	setp.lt.u32 	%p3, %r21, %r11;
	@%p3 bra 	$L__BB48_4;
$L__BB48_5:
	add.s32 	%r20, %r20, %r5;
	setp.lt.u32 	%p4, %r20, %r10;
	@%p4 bra 	$L__BB48_2;
$L__BB48_6:
	ret;

}
	// .globl	TensorAddPadding
.visible .entry TensorAddPadding(
	.param .u32 TensorAddPadding_param_0,
	.param .u64 .ptr .align 1 TensorAddPadding_param_1,
	.param .u64 .ptr .align 1 TensorAddPadding_param_2,
	.param .u32 TensorAddPadding_param_3,
	.param .u32 TensorAddPadding_param_4,
	.param .u32 TensorAddPadding_param_5,
	.param .u32 TensorAddPadding_param_6,
	.param .u32 TensorAddPadding_param_7,
	.param .u32 TensorAddPadding_param_8,
	.param .u32 TensorAddPadding_param_9
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<40>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<19>;

	ld.param.u32 	%r16, [TensorAddPadding_param_0];
	ld.param.u64 	%rd3, [TensorAddPadding_param_1];
	ld.param.u64 	%rd4, [TensorAddPadding_param_2];
	ld.param.u32 	%r17, [TensorAddPadding_param_3];
	ld.param.u32 	%r18, [TensorAddPadding_param_4];
	ld.param.u32 	%r19, [TensorAddPadding_param_5];
	ld.param.u32 	%r20, [TensorAddPadding_param_7];
	ld.param.u32 	%r21, [TensorAddPadding_param_8];
	ld.param.u32 	%r22, [TensorAddPadding_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %tid.x;
	mad.lo.s32 	%r39, %r1, %r23, %r24;
	setp.ge.u32 	%p1, %r39, %r16;
	@%p1 bra 	$L__BB49_5;
	sub.s32 	%r3, %r20, %r22;
	sub.s32 	%r4, %r21, %r22;
	mul.lo.s32 	%r5, %r18, %r17;
	mul.lo.s32 	%r6, %r5, %r19;
	mul.lo.s32 	%r7, %r21, %r20;
	mul.lo.s32 	%r8, %r7, %r19;
	mov.u32 	%r25, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r25;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB49_2:
	div.u32 	%r26, %r39, %r20;
	mul.lo.s32 	%r27, %r26, %r20;
	sub.s32 	%r11, %r39, %r27;
	div.u32 	%r28, %r26, %r21;
	mul.lo.s32 	%r29, %r28, %r21;
	sub.s32 	%r12, %r26, %r29;
	div.u32 	%r14, %r28, %r19;
	mul.lo.s32 	%r30, %r14, %r19;
	sub.s32 	%r13, %r28, %r30;
	setp.ge.u32 	%p2, %r11, %r22;
	setp.lt.u32 	%p3, %r11, %r3;
	and.pred  	%p4, %p2, %p3;
	setp.ge.u32 	%p5, %r12, %r22;
	setp.lt.u32 	%p6, %r12, %r4;
	and.pred  	%p7, %p5, %p6;
	and.pred  	%p9, %p4, %p7;
	mov.f32 	%f4, 0f00000000;
	not.pred 	%p10, %p9;
	@%p10 bra 	$L__BB49_4;
	mul.lo.s32 	%r31, %r6, %r14;
	cvt.u64.u32 	%rd5, %r31;
	mul.lo.s32 	%r32, %r13, %r5;
	cvt.u64.u32 	%rd6, %r32;
	sub.s32 	%r33, %r12, %r22;
	sub.s32 	%r34, %r11, %r22;
	mad.lo.s32 	%r35, %r33, %r17, %r34;
	cvt.u64.u32 	%rd7, %r35;
	add.s64 	%rd8, %rd7, %rd6;
	add.s64 	%rd9, %rd8, %rd5;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f4, [%rd11];
$L__BB49_4:
	mul.lo.s32 	%r36, %r8, %r14;
	cvt.u64.u32 	%rd12, %r36;
	mul.lo.s32 	%r37, %r13, %r7;
	cvt.u64.u32 	%rd13, %r37;
	mad.lo.s32 	%r38, %r12, %r20, %r11;
	cvt.u64.u32 	%rd14, %r38;
	add.s64 	%rd15, %rd13, %rd14;
	add.s64 	%rd16, %rd15, %rd12;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	st.global.f32 	[%rd18], %f4;
	add.s32 	%r39, %r39, %r9;
	setp.lt.u32 	%p11, %r39, %r16;
	@%p11 bra 	$L__BB49_2;
$L__BB49_5:
	ret;

}
	// .globl	TensorRemovePadding
.visible .entry TensorRemovePadding(
	.param .u32 TensorRemovePadding_param_0,
	.param .u64 .ptr .align 1 TensorRemovePadding_param_1,
	.param .u64 .ptr .align 1 TensorRemovePadding_param_2,
	.param .u32 TensorRemovePadding_param_3,
	.param .u32 TensorRemovePadding_param_4,
	.param .u32 TensorRemovePadding_param_5,
	.param .u32 TensorRemovePadding_param_6,
	.param .u32 TensorRemovePadding_param_7,
	.param .u32 TensorRemovePadding_param_8,
	.param .u32 TensorRemovePadding_param_9
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<39>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<19>;

	ld.param.u32 	%r15, [TensorRemovePadding_param_0];
	ld.param.u64 	%rd3, [TensorRemovePadding_param_1];
	ld.param.u64 	%rd4, [TensorRemovePadding_param_2];
	ld.param.u32 	%r16, [TensorRemovePadding_param_3];
	ld.param.u32 	%r17, [TensorRemovePadding_param_4];
	ld.param.u32 	%r18, [TensorRemovePadding_param_5];
	ld.param.u32 	%r19, [TensorRemovePadding_param_7];
	ld.param.u32 	%r20, [TensorRemovePadding_param_8];
	ld.param.u32 	%r21, [TensorRemovePadding_param_9];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r22, %ctaid.x;
	mov.u32 	%r23, %tid.x;
	mad.lo.s32 	%r38, %r1, %r22, %r23;
	setp.ge.u32 	%p1, %r38, %r15;
	@%p1 bra 	$L__BB50_5;
	sub.s32 	%r3, %r16, %r21;
	sub.s32 	%r4, %r17, %r21;
	mul.lo.s32 	%r5, %r17, %r16;
	mul.lo.s32 	%r6, %r5, %r18;
	mul.lo.s32 	%r7, %r20, %r19;
	mul.lo.s32 	%r8, %r7, %r18;
	mov.u32 	%r24, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r24;
	cvta.to.global.u64 	%rd1, %rd3;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB50_2:
	div.u32 	%r12, %r38, %r16;
	mul.lo.s32 	%r25, %r12, %r16;
	sub.s32 	%r11, %r38, %r25;
	rem.u32 	%r13, %r12, %r17;
	setp.ge.u32 	%p2, %r11, %r21;
	setp.lt.u32 	%p3, %r11, %r3;
	and.pred  	%p4, %p2, %p3;
	setp.ge.u32 	%p5, %r13, %r21;
	setp.lt.u32 	%p6, %r13, %r4;
	and.pred  	%p7, %p5, %p6;
	and.pred  	%p9, %p4, %p7;
	not.pred 	%p10, %p9;
	@%p10 bra 	$L__BB50_4;
	div.u32 	%r26, %r12, %r17;
	div.u32 	%r27, %r26, %r18;
	mul.lo.s32 	%r28, %r27, %r18;
	sub.s32 	%r29, %r26, %r28;
	mul.lo.s32 	%r30, %r6, %r27;
	cvt.u64.u32 	%rd5, %r30;
	mul.lo.s32 	%r31, %r29, %r5;
	cvt.u64.u32 	%rd6, %r31;
	mad.lo.s32 	%r32, %r13, %r16, %r11;
	cvt.u64.u32 	%rd7, %r32;
	add.s64 	%rd8, %rd6, %rd7;
	add.s64 	%rd9, %rd8, %rd5;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd1, %rd10;
	ld.global.nc.f32 	%f1, [%rd11];
	mul.lo.s32 	%r33, %r8, %r27;
	cvt.u64.u32 	%rd12, %r33;
	mul.lo.s32 	%r34, %r29, %r7;
	cvt.u64.u32 	%rd13, %r34;
	sub.s32 	%r35, %r13, %r21;
	sub.s32 	%r36, %r11, %r21;
	mad.lo.s32 	%r37, %r35, %r19, %r36;
	cvt.u64.u32 	%rd14, %r37;
	add.s64 	%rd15, %rd13, %rd14;
	add.s64 	%rd16, %rd15, %rd12;
	shl.b64 	%rd17, %rd16, 2;
	add.s64 	%rd18, %rd2, %rd17;
	st.global.f32 	[%rd18], %f1;
$L__BB50_4:
	add.s32 	%r38, %r38, %r9;
	setp.lt.u32 	%p11, %r38, %r15;
	@%p11 bra 	$L__BB50_2;
$L__BB50_5:
	ret;

}
	// .globl	TensorIm2Col
.visible .entry TensorIm2Col(
	.param .u32 TensorIm2Col_param_0,
	.param .u64 .ptr .align 1 TensorIm2Col_param_1,
	.param .u64 .ptr .align 1 TensorIm2Col_param_2,
	.param .u64 .ptr .align 1 TensorIm2Col_param_3,
	.param .u64 .ptr .align 1 TensorIm2Col_param_4,
	.param .u32 TensorIm2Col_param_5,
	.param .u32 TensorIm2Col_param_6,
	.param .u32 TensorIm2Col_param_7,
	.param .u32 TensorIm2Col_param_8,
	.param .u32 TensorIm2Col_param_9,
	.param .u32 TensorIm2Col_param_10,
	.param .u32 TensorIm2Col_param_11,
	.param .u32 TensorIm2Col_param_12,
	.param .u32 TensorIm2Col_param_13,
	.param .u32 TensorIm2Col_param_14,
	.param .u32 TensorIm2Col_param_15
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<47>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<24>;

	ld.param.u32 	%r9, [TensorIm2Col_param_0];
	ld.param.u64 	%rd5, [TensorIm2Col_param_1];
	ld.param.u64 	%rd6, [TensorIm2Col_param_2];
	ld.param.u64 	%rd7, [TensorIm2Col_param_3];
	ld.param.u64 	%rd8, [TensorIm2Col_param_4];
	ld.param.u32 	%r10, [TensorIm2Col_param_5];
	ld.param.u32 	%r11, [TensorIm2Col_param_6];
	ld.param.u32 	%r12, [TensorIm2Col_param_7];
	ld.param.u32 	%r13, [TensorIm2Col_param_9];
	ld.param.u32 	%r14, [TensorIm2Col_param_10];
	ld.param.u32 	%r17, [TensorIm2Col_param_13];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r46, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r46, %r9;
	@%p1 bra 	$L__BB51_3;
	mul.lo.s32 	%r3, %r14, %r13;
	mul.lo.s32 	%r4, %r11, %r10;
	mul.lo.s32 	%r5, %r4, %r12;
	mov.u32 	%r20, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r20;
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd5;
	cvta.to.global.u64 	%rd4, %rd6;
$L__BB51_2:
	ld.param.u32 	%r45, [TensorIm2Col_param_12];
	ld.param.u32 	%r44, [TensorIm2Col_param_11];
	div.u32 	%r21, %r46, %r45;
	mul.lo.s32 	%r22, %r21, %r45;
	sub.s32 	%r23, %r46, %r22;
	div.u32 	%r24, %r21, %r17;
	mul.lo.s32 	%r25, %r24, %r17;
	sub.s32 	%r26, %r21, %r25;
	div.u32 	%r27, %r24, %r12;
	mul.lo.s32 	%r28, %r27, %r12;
	sub.s32 	%r29, %r24, %r28;
	div.u32 	%r30, %r27, %r44;
	mul.lo.s32 	%r31, %r30, %r44;
	sub.s32 	%r32, %r27, %r31;
	mul.wide.u32 	%rd9, %r32, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	cvt.rzi.ftz.u32.f32 	%r33, %f1;
	add.s64 	%rd11, %rd2, %rd9;
	ld.global.nc.f32 	%f2, [%rd11];
	cvt.rzi.ftz.u32.f32 	%r34, %f2;
	mad.lo.s32 	%r35, %r29, %r45, %r23;
	mad.lo.s32 	%r36, %r35, %r17, %r26;
	mul.lo.s32 	%r37, %r3, %r30;
	cvt.u64.u32 	%rd12, %r37;
	mul.lo.s32 	%r38, %r5, %r30;
	cvt.u64.u32 	%rd13, %r38;
	mul.lo.s32 	%r39, %r29, %r4;
	cvt.u64.u32 	%rd14, %r39;
	add.s64 	%rd15, %rd13, %rd14;
	add.s32 	%r40, %r33, %r23;
	add.s32 	%r41, %r34, %r26;
	mad.lo.s32 	%r42, %r40, %r10, %r41;
	cvt.u64.u32 	%rd16, %r42;
	add.s64 	%rd17, %rd15, %rd16;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.f32 	%f3, [%rd19];
	mad.lo.s32 	%r43, %r36, %r13, %r32;
	cvt.u64.u32 	%rd20, %r43;
	add.s64 	%rd21, %rd20, %rd12;
	shl.b64 	%rd22, %rd21, 2;
	add.s64 	%rd23, %rd4, %rd22;
	st.global.f32 	[%rd23], %f3;
	add.s32 	%r46, %r46, %r6;
	setp.lt.u32 	%p2, %r46, %r9;
	@%p2 bra 	$L__BB51_2;
$L__BB51_3:
	ret;

}
	// .globl	TensorReverseIm2Col
.visible .entry TensorReverseIm2Col(
	.param .u32 TensorReverseIm2Col_param_0,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_1,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_2,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_3,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_4,
	.param .u64 .ptr .align 1 TensorReverseIm2Col_param_5,
	.param .u32 TensorReverseIm2Col_param_6,
	.param .u32 TensorReverseIm2Col_param_7,
	.param .u32 TensorReverseIm2Col_param_8,
	.param .u32 TensorReverseIm2Col_param_9,
	.param .u32 TensorReverseIm2Col_param_10,
	.param .u32 TensorReverseIm2Col_param_11,
	.param .u32 TensorReverseIm2Col_param_12,
	.param .u32 TensorReverseIm2Col_param_13,
	.param .u32 TensorReverseIm2Col_param_14,
	.param .u32 TensorReverseIm2Col_param_15,
	.param .u32 TensorReverseIm2Col_param_16,
	.param .u32 TensorReverseIm2Col_param_17
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<63>;
	.reg .f32 	%f<7>;
	.reg .b64 	%rd<37>;

	ld.param.u32 	%r20, [TensorReverseIm2Col_param_0];
	ld.param.u64 	%rd4, [TensorReverseIm2Col_param_1];
	ld.param.u64 	%rd5, [TensorReverseIm2Col_param_2];
	ld.param.u64 	%rd6, [TensorReverseIm2Col_param_3];
	ld.param.u32 	%r21, [TensorReverseIm2Col_param_6];
	ld.param.u32 	%r22, [TensorReverseIm2Col_param_7];
	ld.param.u32 	%r23, [TensorReverseIm2Col_param_8];
	ld.param.u32 	%r25, [TensorReverseIm2Col_param_11];
	ld.param.u32 	%r26, [TensorReverseIm2Col_param_12];
	ld.param.u32 	%r29, [TensorReverseIm2Col_param_15];
	ld.param.u32 	%r30, [TensorReverseIm2Col_param_16];
	ld.param.u32 	%r31, [TensorReverseIm2Col_param_17];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	mad.lo.s32 	%r62, %r1, %r32, %r33;
	setp.ge.u32 	%p1, %r62, %r20;
	@%p1 bra 	$L__BB52_5;
	mul.lo.s32 	%r3, %r22, %r21;
	mul.lo.s32 	%r4, %r3, %r23;
	mul.lo.s32 	%r5, %r26, %r25;
	mul.lo.s32 	%r6, %r5, %r31;
	mul.lo.s32 	%r7, %r30, %r29;
	mul.lo.s32 	%r8, %r7, %r31;
	mov.u32 	%r34, %nctaid.x;
	mul.lo.s32 	%r9, %r1, %r34;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB52_2:
	ld.param.u32 	%r61, [TensorReverseIm2Col_param_14];
	ld.param.u32 	%r60, [TensorReverseIm2Col_param_13];
	ld.param.u32 	%r59, [TensorReverseIm2Col_param_10];
	ld.param.u64 	%rd36, [TensorReverseIm2Col_param_5];
	ld.param.u64 	%rd35, [TensorReverseIm2Col_param_4];
	div.u32 	%r11, %r62, %r31;
	div.u32 	%r12, %r11, %r25;
	div.u32 	%r35, %r12, %r26;
	div.u32 	%r36, %r35, %r59;
	mul.lo.s32 	%r37, %r36, %r59;
	sub.s32 	%r38, %r35, %r37;
	div.u32 	%r14, %r36, %r23;
	mul.lo.s32 	%r39, %r14, %r23;
	sub.s32 	%r13, %r36, %r39;
	cvta.to.global.u64 	%rd9, %rd35;
	mul.wide.u32 	%rd10, %r38, 4;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.nc.f32 	%f1, [%rd11];
	cvt.rzi.ftz.u32.f32 	%r15, %f1;
	cvta.to.global.u64 	%rd12, %rd36;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.nc.f32 	%f2, [%rd13];
	cvt.rzi.ftz.u32.f32 	%r16, %f2;
	div.u32 	%r17, %r15, %r60;
	div.u32 	%r40, %r16, %r61;
	setp.ge.u32 	%p2, %r17, %r22;
	setp.ge.u32 	%p3, %r40, %r21;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB52_4;
	rem.u32 	%r41, %r62, %r31;
	rem.u32 	%r42, %r11, %r25;
	rem.u32 	%r43, %r12, %r26;
	mul.lo.s32 	%r44, %r5, %r41;
	cvt.u64.u32 	%rd14, %r44;
	mul.lo.s32 	%r45, %r6, %r13;
	cvt.u64.u32 	%rd15, %r45;
	mad.lo.s32 	%r46, %r17, %r21, %r40;
	cvt.u64.u32 	%rd16, %r46;
	mul.lo.s32 	%r47, %r3, %r13;
	cvt.u64.u32 	%rd17, %r47;
	mul.lo.s32 	%r48, %r4, %r14;
	cvt.u64.u32 	%rd18, %r48;
	add.s64 	%rd19, %rd18, %rd17;
	add.s64 	%rd20, %rd19, %rd16;
	shl.b64 	%rd21, %rd20, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.f32 	%f3, [%rd22];
	not.b32 	%r49, %r42;
	add.s32 	%r50, %r25, %r49;
	not.b32 	%r51, %r43;
	add.s32 	%r52, %r26, %r51;
	mad.lo.s32 	%r53, %r50, %r26, %r52;
	add.s32 	%r54, %r15, %r42;
	add.s32 	%r55, %r16, %r43;
	mad.lo.s32 	%r56, %r54, %r29, %r55;
	cvt.u64.u32 	%rd23, %r53;
	add.s64 	%rd24, %rd23, %rd14;
	add.s64 	%rd25, %rd24, %rd15;
	shl.b64 	%rd26, %rd25, 2;
	add.s64 	%rd27, %rd2, %rd26;
	ld.global.nc.f32 	%f4, [%rd27];
	mul.ftz.f32 	%f5, %f3, %f4;
	cvt.u64.u32 	%rd28, %r56;
	mul.lo.s32 	%r57, %r7, %r41;
	cvt.u64.u32 	%rd29, %r57;
	mul.lo.s32 	%r58, %r8, %r14;
	cvt.u64.u32 	%rd30, %r58;
	add.s64 	%rd31, %rd30, %rd29;
	add.s64 	%rd32, %rd31, %rd28;
	shl.b64 	%rd33, %rd32, 2;
	add.s64 	%rd34, %rd3, %rd33;
	atom.global.add.f32 	%f6, [%rd34], %f5;
$L__BB52_4:
	add.s32 	%r62, %r62, %r9;
	setp.lt.u32 	%p5, %r62, %r20;
	@%p5 bra 	$L__BB52_2;
$L__BB52_5:
	ret;

}
	// .globl	SoftmaxDerivative
.visible .entry SoftmaxDerivative(
	.param .u64 .ptr .align 1 SoftmaxDerivative_param_0,
	.param .u64 .ptr .align 1 SoftmaxDerivative_param_1,
	.param .u32 SoftmaxDerivative_param_2,
	.param .u32 SoftmaxDerivative_param_3
)
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<24>;
	.reg .f32 	%f<12>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd5, [SoftmaxDerivative_param_0];
	ld.param.u64 	%rd4, [SoftmaxDerivative_param_1];
	ld.param.u32 	%r10, [SoftmaxDerivative_param_2];
	ld.param.u32 	%r11, [SoftmaxDerivative_param_3];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r22, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r22, %r10;
	@%p1 bra 	$L__BB53_9;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd2, %rd4;
$L__BB53_2:
	setp.ge.u32 	%p2, %r3, %r10;
	@%p2 bra 	$L__BB53_8;
	mul.lo.s32 	%r19, %r22, %r11;
	mul.wide.u32 	%rd6, %r19, 4;
	add.s64 	%rd3, %rd1, %rd6;
	mov.u32 	%r23, %r3;
$L__BB53_4:
	setp.ne.s32 	%p3, %r22, %r23;
	@%p3 bra 	$L__BB53_6;
	ld.global.nc.f32 	%f10, [%rd3];
	mov.f32 	%f8, 0f3F800000;
	sub.ftz.f32 	%f11, %f8, %f10;
	bra.uni 	$L__BB53_7;
$L__BB53_6:
	ld.global.nc.f32 	%f7, [%rd3];
	neg.ftz.f32 	%f11, %f7;
	mul.lo.s32 	%r20, %r23, %r11;
	mul.wide.u32 	%rd7, %r20, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.nc.f32 	%f10, [%rd8];
$L__BB53_7:
	mul.ftz.f32 	%f9, %f10, %f11;
	mad.lo.s32 	%r21, %r23, %r10, %r22;
	mul.wide.u32 	%rd9, %r21, 4;
	add.s64 	%rd10, %rd2, %rd9;
	st.global.f32 	[%rd10], %f9;
	add.s32 	%r23, %r23, %r4;
	setp.lt.u32 	%p4, %r23, %r10;
	@%p4 bra 	$L__BB53_4;
$L__BB53_8:
	add.s32 	%r22, %r22, %r5;
	setp.lt.u32 	%p5, %r22, %r10;
	@%p5 bra 	$L__BB53_2;
$L__BB53_9:
	ret;

}
	// .globl	RotateInPlace
.visible .entry RotateInPlace(
	.param .u64 .ptr .align 1 RotateInPlace_param_0,
	.param .u32 RotateInPlace_param_1,
	.param .u32 RotateInPlace_param_2,
	.param .u32 RotateInPlace_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<7>;

	ld.param.u64 	%rd2, [RotateInPlace_param_0];
	ld.param.u32 	%r6, [RotateInPlace_param_1];
	ld.param.u32 	%r7, [RotateInPlace_param_3];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r16, %r1, %r8, %r9;
	setp.ge.u32 	%p1, %r16, %r6;
	@%p1 bra 	$L__BB54_3;
	cvta.to.global.u64 	%rd1, %rd2;
	mov.u32 	%r10, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r10;
$L__BB54_2:
	rem.u32 	%r11, %r16, %r7;
	sub.s32 	%r12, %r16, %r11;
	not.b32 	%r13, %r11;
	add.s32 	%r14, %r7, %r13;
	add.s32 	%r15, %r14, %r12;
	mul.wide.u32 	%rd3, %r15, 4;
	add.s64 	%rd4, %rd1, %rd3;
	ld.global.f32 	%f1, [%rd4];
	mul.wide.u32 	%rd5, %r16, 4;
	add.s64 	%rd6, %rd1, %rd5;
	ld.global.f32 	%f2, [%rd6];
	st.global.f32 	[%rd4], %f2;
	st.global.f32 	[%rd6], %f1;
	add.s32 	%r16, %r16, %r3;
	setp.lt.u32 	%p2, %r16, %r6;
	@%p2 bra 	$L__BB54_2;
$L__BB54_3:
	ret;

}
	// .globl	TensorMaxPool
.visible .entry TensorMaxPool(
	.param .u32 TensorMaxPool_param_0,
	.param .u64 .ptr .align 1 TensorMaxPool_param_1,
	.param .u64 .ptr .align 1 TensorMaxPool_param_2,
	.param .u64 .ptr .align 1 TensorMaxPool_param_3,
	.param .u64 .ptr .align 1 TensorMaxPool_param_4,
	.param .u64 .ptr .align 1 TensorMaxPool_param_5,
	.param .u32 TensorMaxPool_param_6,
	.param .u32 TensorMaxPool_param_7,
	.param .u32 TensorMaxPool_param_8,
	.param .u32 TensorMaxPool_param_9,
	.param .u32 TensorMaxPool_param_10,
	.param .u32 TensorMaxPool_param_11,
	.param .u32 TensorMaxPool_param_12,
	.param .u32 TensorMaxPool_param_13,
	.param .u32 TensorMaxPool_param_14,
	.param .u32 TensorMaxPool_param_15,
	.param .u32 TensorMaxPool_param_16,
	.param .u32 TensorMaxPool_param_17
)
{
	.reg .pred 	%p<121>;
	.reg .b32 	%r<333>;
	.reg .f32 	%f<170>;
	.reg .b64 	%rd<190>;

	ld.param.u32 	%r100, [TensorMaxPool_param_0];
	ld.param.u64 	%rd9, [TensorMaxPool_param_1];
	ld.param.u64 	%rd10, [TensorMaxPool_param_2];
	ld.param.u64 	%rd8, [TensorMaxPool_param_3];
	ld.param.u64 	%rd11, [TensorMaxPool_param_4];
	ld.param.u64 	%rd12, [TensorMaxPool_param_5];
	ld.param.u32 	%r101, [TensorMaxPool_param_6];
	ld.param.u32 	%r103, [TensorMaxPool_param_8];
	ld.param.u32 	%r104, [TensorMaxPool_param_9];
	ld.param.u32 	%r105, [TensorMaxPool_param_11];
	ld.param.u32 	%r106, [TensorMaxPool_param_12];
	ld.param.u32 	%r108, [TensorMaxPool_param_14];
	ld.param.u32 	%r109, [TensorMaxPool_param_15];
	ld.param.u32 	%r110, [TensorMaxPool_param_16];
	ld.param.u32 	%r111, [TensorMaxPool_param_17];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd10;
	cvta.to.global.u64 	%rd3, %rd8;
	cvta.to.global.u64 	%rd4, %rd12;
	cvta.to.global.u64 	%rd5, %rd11;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r112, %ctaid.x;
	mov.u32 	%r113, %tid.x;
	mad.lo.s32 	%r2, %r1, %r112, %r113;
	setp.ge.u32 	%p1, %r2, %r100;
	@%p1 bra 	$L__BB55_41;
	ld.param.u32 	%r284, [TensorMaxPool_param_13];
	ld.param.u32 	%r281, [TensorMaxPool_param_7];
	mul.lo.s32 	%r3, %r106, %r105;
	mul.lo.s32 	%r4, %r103, %r281;
	setp.ne.s32 	%p2, %r284, 0;
	mov.u32 	%r114, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r114;
	@%p2 bra 	$L__BB55_5;
	setp.eq.s32 	%p118, %r111, 0;
	@%p118 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_4;
$L__BB55_3:
	div.u32 	%r270, %r2, %r101;
	mul.lo.s32 	%r271, %r270, %r101;
	sub.s32 	%r272, %r2, %r271;
	mul.wide.u32 	%rd181, %r272, 4;
	add.s64 	%rd182, %rd5, %rd181;
	ld.global.nc.f32 	%f150, [%rd182];
	cvt.rzi.ftz.u32.f32 	%r274, %f150;
	add.s64 	%rd183, %rd4, %rd181;
	ld.global.nc.f32 	%f151, [%rd183];
	cvt.rzi.ftz.u32.f32 	%r275, %f151;
	div.u32 	%r276, %r274, %r109;
	div.u32 	%r277, %r275, %r110;
	mul.lo.s32 	%r278, %r3, %r270;
	cvt.u64.u32 	%rd184, %r278;
	mad.lo.s32 	%r279, %r276, %r105, %r277;
	cvt.u64.u32 	%rd185, %r279;
	add.s64 	%rd186, %rd184, %rd185;
	shl.b64 	%rd187, %rd186, 2;
	add.s64 	%rd188, %rd2, %rd187;
	mov.b32 	%r280, 0;
	st.global.u32 	[%rd188], %r280;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p120, %r2, %r100;
	@%p120 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_41;
$L__BB55_4:
	div.u32 	%r258, %r2, %r101;
	mul.lo.s32 	%r259, %r258, %r101;
	sub.s32 	%r260, %r2, %r259;
	mul.wide.u32 	%rd172, %r260, 4;
	add.s64 	%rd173, %rd5, %rd172;
	ld.global.nc.f32 	%f148, [%rd173];
	cvt.rzi.ftz.u32.f32 	%r262, %f148;
	add.s64 	%rd174, %rd4, %rd172;
	ld.global.nc.f32 	%f149, [%rd174];
	cvt.rzi.ftz.u32.f32 	%r263, %f149;
	div.u32 	%r264, %r262, %r109;
	div.u32 	%r265, %r263, %r110;
	mul.lo.s32 	%r266, %r3, %r258;
	cvt.u64.u32 	%rd175, %r266;
	mad.lo.s32 	%r267, %r264, %r105, %r265;
	cvt.u64.u32 	%rd176, %r267;
	add.s64 	%rd177, %rd175, %rd176;
	shl.b64 	%rd178, %rd177, 2;
	add.s64 	%rd179, %rd3, %rd178;
	mov.b32 	%r268, -1082130432;
	st.global.u32 	[%rd179], %r268;
	add.s64 	%rd180, %rd2, %rd178;
	mov.b32 	%r269, 0;
	st.global.u32 	[%rd180], %r269;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p119, %r2, %r100;
	@%p119 bra 	$L__BB55_4;
	bra.uni 	$L__BB55_41;
$L__BB55_5:
	setp.ne.s32 	%p3, %r108, 0;
	@%p3 bra 	$L__BB55_9;
	setp.eq.s32 	%p115, %r111, 0;
	@%p115 bra 	$L__BB55_7;
	bra.uni 	$L__BB55_8;
$L__BB55_7:
	div.u32 	%r247, %r2, %r101;
	mul.lo.s32 	%r248, %r247, %r101;
	sub.s32 	%r249, %r2, %r248;
	mul.wide.u32 	%rd164, %r249, 4;
	add.s64 	%rd165, %rd5, %rd164;
	ld.global.nc.f32 	%f146, [%rd165];
	cvt.rzi.ftz.u32.f32 	%r251, %f146;
	add.s64 	%rd166, %rd4, %rd164;
	ld.global.nc.f32 	%f147, [%rd166];
	cvt.rzi.ftz.u32.f32 	%r252, %f147;
	div.u32 	%r253, %r251, %r109;
	div.u32 	%r254, %r252, %r110;
	mul.lo.s32 	%r255, %r3, %r247;
	cvt.u64.u32 	%rd167, %r255;
	mad.lo.s32 	%r256, %r253, %r105, %r254;
	cvt.u64.u32 	%rd168, %r256;
	add.s64 	%rd169, %rd167, %rd168;
	shl.b64 	%rd170, %rd169, 2;
	add.s64 	%rd171, %rd2, %rd170;
	mov.b32 	%r257, 0;
	st.global.u32 	[%rd171], %r257;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p117, %r2, %r100;
	@%p117 bra 	$L__BB55_7;
	bra.uni 	$L__BB55_41;
$L__BB55_8:
	div.u32 	%r235, %r2, %r101;
	mul.lo.s32 	%r236, %r235, %r101;
	sub.s32 	%r237, %r2, %r236;
	mul.wide.u32 	%rd155, %r237, 4;
	add.s64 	%rd156, %rd5, %rd155;
	ld.global.nc.f32 	%f144, [%rd156];
	cvt.rzi.ftz.u32.f32 	%r239, %f144;
	add.s64 	%rd157, %rd4, %rd155;
	ld.global.nc.f32 	%f145, [%rd157];
	cvt.rzi.ftz.u32.f32 	%r240, %f145;
	div.u32 	%r241, %r239, %r109;
	div.u32 	%r242, %r240, %r110;
	mul.lo.s32 	%r243, %r3, %r235;
	cvt.u64.u32 	%rd158, %r243;
	mad.lo.s32 	%r244, %r241, %r105, %r242;
	cvt.u64.u32 	%rd159, %r244;
	add.s64 	%rd160, %rd158, %rd159;
	shl.b64 	%rd161, %rd160, 2;
	add.s64 	%rd162, %rd3, %rd161;
	mov.b32 	%r245, -1082130432;
	st.global.u32 	[%rd162], %r245;
	add.s64 	%rd163, %rd2, %rd161;
	mov.b32 	%r246, 0;
	st.global.u32 	[%rd163], %r246;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p116, %r2, %r100;
	@%p116 bra 	$L__BB55_8;
	bra.uni 	$L__BB55_41;
$L__BB55_9:
	setp.ne.s32 	%p4, %r111, 0;
	@%p4 bra 	$L__BB55_26;
	and.b32  	%r14, %r108, 7;
	mul.lo.s32 	%r181, %r4, %r104;
$L__BB55_11:
	div.u32 	%r177, %r2, %r101;
	mul.lo.s32 	%r178, %r177, %r101;
	sub.s32 	%r179, %r2, %r178;
	div.u32 	%r17, %r177, %r104;
	mul.lo.s32 	%r180, %r17, %r104;
	sub.s32 	%r16, %r177, %r180;
	mul.wide.u32 	%rd85, %r179, 4;
	add.s64 	%rd86, %rd5, %rd85;
	ld.global.nc.f32 	%f87, [%rd86];
	cvt.rzi.ftz.u32.f32 	%r18, %f87;
	add.s64 	%rd87, %rd4, %rd85;
	ld.global.nc.f32 	%f88, [%rd87];
	cvt.rzi.ftz.u32.f32 	%r19, %f88;
	mul.lo.s32 	%r182, %r181, %r17;
	cvt.u64.u32 	%rd88, %r182;
	mul.lo.s32 	%r183, %r4, %r16;
	cvt.u64.u32 	%rd89, %r183;
	add.s64 	%rd6, %rd88, %rd89;
	mov.f32 	%f160, 0f00000000;
	mov.b32 	%r312, -1;
	mov.b32 	%r292, 0;
	mov.u32 	%r293, %r292;
$L__BB55_12:
	ld.param.u32 	%r283, [TensorMaxPool_param_7];
	setp.lt.u32 	%p60, %r108, 8;
	add.s32 	%r186, %r292, %r18;
	mad.lo.s32 	%r23, %r186, %r283, %r19;
	mov.b32 	%r305, 0;
	mov.u32 	%r306, %r293;
	@%p60 bra 	$L__BB55_16;
	and.b32  	%r188, %r108, -8;
	neg.s32 	%r295, %r188;
	mov.b32 	%r297, 3;
	mov.u32 	%r296, %r23;
	mov.u32 	%r298, %r293;
$L__BB55_14:
	.pragma "nounroll";
	mov.u32 	%r28, %r298;
	cvt.u64.u32 	%rd90, %r296;
	add.s64 	%rd91, %rd6, %rd90;
	shl.b64 	%rd92, %rd91, 2;
	add.s64 	%rd93, %rd1, %rd92;
	ld.global.nc.f32 	%f90, [%rd93];
	setp.lt.s32 	%p61, %r312, 0;
	setp.gt.ftz.f32 	%p62, %f90, %f160;
	or.pred  	%p63, %p61, %p62;
	selp.f32 	%f92, %f90, %f160, %p63;
	selp.b32 	%r189, %r28, %r312, %p63;
	add.s32 	%r190, %r296, 1;
	cvt.u64.u32 	%rd94, %r190;
	add.s64 	%rd95, %rd6, %rd94;
	shl.b64 	%rd96, %rd95, 2;
	add.s64 	%rd97, %rd1, %rd96;
	ld.global.nc.f32 	%f94, [%rd97];
	setp.lt.s32 	%p64, %r189, 0;
	setp.gt.ftz.f32 	%p65, %f94, %f92;
	or.pred  	%p66, %p64, %p65;
	selp.f32 	%f96, %f94, %f92, %p66;
	add.s32 	%r191, %r28, 1;
	selp.b32 	%r192, %r191, %r189, %p66;
	add.s32 	%r193, %r296, 2;
	cvt.u64.u32 	%rd98, %r193;
	add.s64 	%rd99, %rd6, %rd98;
	shl.b64 	%rd100, %rd99, 2;
	add.s64 	%rd101, %rd1, %rd100;
	ld.global.nc.f32 	%f98, [%rd101];
	setp.lt.s32 	%p67, %r192, 0;
	setp.gt.ftz.f32 	%p68, %f98, %f96;
	or.pred  	%p69, %p67, %p68;
	selp.f32 	%f100, %f98, %f96, %p69;
	add.s32 	%r194, %r28, 2;
	selp.b32 	%r195, %r194, %r192, %p69;
	add.s32 	%r196, %r296, 3;
	cvt.u64.u32 	%rd102, %r196;
	add.s64 	%rd103, %rd6, %rd102;
	shl.b64 	%rd104, %rd103, 2;
	add.s64 	%rd105, %rd1, %rd104;
	ld.global.nc.f32 	%f102, [%rd105];
	setp.lt.s32 	%p70, %r195, 0;
	setp.gt.ftz.f32 	%p71, %f102, %f100;
	or.pred  	%p72, %p70, %p71;
	selp.f32 	%f104, %f102, %f100, %p72;
	add.s32 	%r197, %r28, 3;
	selp.b32 	%r198, %r197, %r195, %p72;
	add.s32 	%r199, %r296, 4;
	cvt.u64.u32 	%rd106, %r199;
	add.s64 	%rd107, %rd6, %rd106;
	shl.b64 	%rd108, %rd107, 2;
	add.s64 	%rd109, %rd1, %rd108;
	ld.global.nc.f32 	%f106, [%rd109];
	setp.lt.s32 	%p73, %r198, 0;
	setp.gt.ftz.f32 	%p74, %f106, %f104;
	or.pred  	%p75, %p73, %p74;
	selp.f32 	%f108, %f106, %f104, %p75;
	add.s32 	%r200, %r28, 4;
	selp.b32 	%r201, %r200, %r198, %p75;
	add.s32 	%r202, %r296, 5;
	cvt.u64.u32 	%rd110, %r202;
	add.s64 	%rd111, %rd6, %rd110;
	shl.b64 	%rd112, %rd111, 2;
	add.s64 	%rd113, %rd1, %rd112;
	ld.global.nc.f32 	%f110, [%rd113];
	setp.lt.s32 	%p76, %r201, 0;
	setp.gt.ftz.f32 	%p77, %f110, %f108;
	or.pred  	%p78, %p76, %p77;
	selp.f32 	%f112, %f110, %f108, %p78;
	add.s32 	%r203, %r28, 5;
	selp.b32 	%r204, %r203, %r201, %p78;
	add.s32 	%r205, %r296, 6;
	cvt.u64.u32 	%rd114, %r205;
	add.s64 	%rd115, %rd6, %rd114;
	shl.b64 	%rd116, %rd115, 2;
	add.s64 	%rd117, %rd1, %rd116;
	ld.global.nc.f32 	%f114, [%rd117];
	setp.lt.s32 	%p79, %r204, 0;
	setp.gt.ftz.f32 	%p80, %f114, %f112;
	or.pred  	%p81, %p79, %p80;
	selp.f32 	%f116, %f114, %f112, %p81;
	add.s32 	%r206, %r28, 6;
	selp.b32 	%r207, %r206, %r204, %p81;
	add.s32 	%r208, %r296, 7;
	cvt.u64.u32 	%rd118, %r208;
	add.s64 	%rd119, %rd6, %rd118;
	shl.b64 	%rd120, %rd119, 2;
	add.s64 	%rd121, %rd1, %rd120;
	ld.global.nc.f32 	%f118, [%rd121];
	setp.lt.s32 	%p82, %r207, 0;
	setp.gt.ftz.f32 	%p83, %f118, %f116;
	or.pred  	%p84, %p82, %p83;
	selp.f32 	%f160, %f118, %f116, %p84;
	add.s32 	%r209, %r28, 7;
	selp.b32 	%r312, %r209, %r207, %p84;
	add.s32 	%r297, %r297, 8;
	add.s32 	%r296, %r296, 8;
	add.s32 	%r295, %r295, 8;
	setp.ne.s32 	%p85, %r295, 0;
	add.s32 	%r298, %r28, 8;
	@%p85 bra 	$L__BB55_14;
	add.s32 	%r305, %r297, -3;
	add.s32 	%r306, %r28, 8;
$L__BB55_16:
	setp.eq.s32 	%p86, %r14, 0;
	@%p86 bra 	$L__BB55_24;
	add.s32 	%r211, %r14, -1;
	setp.lt.u32 	%p87, %r211, 3;
	@%p87 bra 	$L__BB55_19;
	add.s32 	%r212, %r23, %r305;
	cvt.u64.u32 	%rd122, %r212;
	add.s64 	%rd123, %rd6, %rd122;
	shl.b64 	%rd124, %rd123, 2;
	add.s64 	%rd125, %rd1, %rd124;
	ld.global.nc.f32 	%f121, [%rd125];
	setp.lt.s32 	%p88, %r312, 0;
	setp.gt.ftz.f32 	%p89, %f121, %f160;
	or.pred  	%p90, %p88, %p89;
	selp.f32 	%f123, %f121, %f160, %p90;
	selp.b32 	%r213, %r306, %r312, %p90;
	add.s32 	%r214, %r306, 1;
	add.s32 	%r215, %r212, 1;
	cvt.u64.u32 	%rd126, %r215;
	add.s64 	%rd127, %rd6, %rd126;
	shl.b64 	%rd128, %rd127, 2;
	add.s64 	%rd129, %rd1, %rd128;
	ld.global.nc.f32 	%f125, [%rd129];
	setp.lt.s32 	%p91, %r213, 0;
	setp.gt.ftz.f32 	%p92, %f125, %f123;
	or.pred  	%p93, %p91, %p92;
	selp.f32 	%f127, %f125, %f123, %p93;
	selp.b32 	%r216, %r214, %r213, %p93;
	add.s32 	%r217, %r306, 2;
	add.s32 	%r218, %r212, 2;
	cvt.u64.u32 	%rd130, %r218;
	add.s64 	%rd131, %rd6, %rd130;
	shl.b64 	%rd132, %rd131, 2;
	add.s64 	%rd133, %rd1, %rd132;
	ld.global.nc.f32 	%f129, [%rd133];
	setp.lt.s32 	%p94, %r216, 0;
	setp.gt.ftz.f32 	%p95, %f129, %f127;
	or.pred  	%p96, %p94, %p95;
	selp.f32 	%f131, %f129, %f127, %p96;
	selp.b32 	%r219, %r217, %r216, %p96;
	add.s32 	%r220, %r306, 3;
	add.s32 	%r221, %r212, 3;
	cvt.u64.u32 	%rd134, %r221;
	add.s64 	%rd135, %rd6, %rd134;
	shl.b64 	%rd136, %rd135, 2;
	add.s64 	%rd137, %rd1, %rd136;
	ld.global.nc.f32 	%f133, [%rd137];
	setp.lt.s32 	%p97, %r219, 0;
	setp.gt.ftz.f32 	%p98, %f133, %f131;
	or.pred  	%p99, %p97, %p98;
	selp.f32 	%f160, %f133, %f131, %p99;
	selp.b32 	%r312, %r220, %r219, %p99;
	add.s32 	%r306, %r306, 4;
	add.s32 	%r305, %r305, 4;
$L__BB55_19:
	and.b32  	%r223, %r108, 3;
	setp.eq.s32 	%p100, %r223, 0;
	@%p100 bra 	$L__BB55_24;
	setp.eq.s32 	%p101, %r223, 1;
	@%p101 bra 	$L__BB55_22;
	add.s32 	%r224, %r23, %r305;
	cvt.u64.u32 	%rd138, %r224;
	add.s64 	%rd139, %rd6, %rd138;
	shl.b64 	%rd140, %rd139, 2;
	add.s64 	%rd141, %rd1, %rd140;
	ld.global.nc.f32 	%f136, [%rd141];
	setp.lt.s32 	%p102, %r312, 0;
	setp.gt.ftz.f32 	%p103, %f136, %f160;
	or.pred  	%p104, %p102, %p103;
	selp.f32 	%f138, %f136, %f160, %p104;
	selp.b32 	%r225, %r306, %r312, %p104;
	add.s32 	%r226, %r306, 1;
	add.s32 	%r227, %r224, 1;
	cvt.u64.u32 	%rd142, %r227;
	add.s64 	%rd143, %rd6, %rd142;
	shl.b64 	%rd144, %rd143, 2;
	add.s64 	%rd145, %rd1, %rd144;
	ld.global.nc.f32 	%f140, [%rd145];
	setp.lt.s32 	%p105, %r225, 0;
	setp.gt.ftz.f32 	%p106, %f140, %f138;
	or.pred  	%p107, %p105, %p106;
	selp.f32 	%f160, %f140, %f138, %p107;
	selp.b32 	%r312, %r226, %r225, %p107;
	add.s32 	%r306, %r306, 2;
	add.s32 	%r305, %r305, 2;
$L__BB55_22:
	and.b32  	%r228, %r108, 1;
	setp.eq.b32 	%p108, %r228, 1;
	not.pred 	%p109, %p108;
	@%p109 bra 	$L__BB55_24;
	add.s32 	%r229, %r23, %r305;
	cvt.u64.u32 	%rd146, %r229;
	add.s64 	%rd147, %rd6, %rd146;
	shl.b64 	%rd148, %rd147, 2;
	add.s64 	%rd149, %rd1, %rd148;
	ld.global.nc.f32 	%f142, [%rd149];
	setp.lt.s32 	%p110, %r312, 0;
	setp.gt.ftz.f32 	%p111, %f142, %f160;
	or.pred  	%p112, %p110, %p111;
	selp.f32 	%f160, %f142, %f160, %p112;
	selp.b32 	%r312, %r306, %r312, %p112;
$L__BB55_24:
	ld.param.u32 	%r286, [TensorMaxPool_param_13];
	add.s32 	%r292, %r292, 1;
	setp.ne.s32 	%p113, %r292, %r286;
	add.s32 	%r293, %r108, %r293;
	@%p113 bra 	$L__BB55_12;
	div.u32 	%r230, %r18, %r109;
	div.u32 	%r231, %r19, %r110;
	mad.lo.s32 	%r232, %r230, %r105, %r231;
	cvt.u64.u32 	%rd150, %r232;
	mad.lo.s32 	%r233, %r17, %r104, %r16;
	mul.lo.s32 	%r234, %r3, %r233;
	cvt.u64.u32 	%rd151, %r234;
	add.s64 	%rd152, %rd151, %rd150;
	shl.b64 	%rd153, %rd152, 2;
	add.s64 	%rd154, %rd2, %rd153;
	st.global.f32 	[%rd154], %f160;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p114, %r2, %r100;
	@%p114 bra 	$L__BB55_11;
	bra.uni 	$L__BB55_41;
$L__BB55_26:
	and.b32  	%r60, %r108, 7;
	and.b32  	%r61, %r108, -8;
	mul.lo.s32 	%r121, %r4, %r104;
$L__BB55_27:
	div.u32 	%r117, %r2, %r101;
	mul.lo.s32 	%r118, %r117, %r101;
	sub.s32 	%r119, %r2, %r118;
	div.u32 	%r64, %r117, %r104;
	mul.lo.s32 	%r120, %r64, %r104;
	sub.s32 	%r63, %r117, %r120;
	mul.wide.u32 	%rd13, %r119, 4;
	add.s64 	%rd14, %rd5, %rd13;
	ld.global.nc.f32 	%f28, [%rd14];
	cvt.rzi.ftz.u32.f32 	%r65, %f28;
	add.s64 	%rd15, %rd4, %rd13;
	ld.global.nc.f32 	%f29, [%rd15];
	cvt.rzi.ftz.u32.f32 	%r66, %f29;
	mul.lo.s32 	%r122, %r121, %r64;
	cvt.u64.u32 	%rd16, %r122;
	mul.lo.s32 	%r123, %r4, %r63;
	cvt.u64.u32 	%rd17, %r123;
	add.s64 	%rd7, %rd16, %rd17;
	mov.f32 	%f169, 0f00000000;
	mov.b32 	%r332, -1;
	mov.b32 	%r314, 0;
	mov.u32 	%r315, %r314;
$L__BB55_28:
	ld.param.u32 	%r282, [TensorMaxPool_param_7];
	setp.lt.u32 	%p5, %r108, 8;
	add.s32 	%r126, %r314, %r65;
	mad.lo.s32 	%r70, %r126, %r282, %r66;
	mov.b32 	%r325, 0;
	mov.u32 	%r322, %r315;
	@%p5 bra 	$L__BB55_31;
	mov.b32 	%r317, 0;
	mov.u32 	%r322, %r315;
$L__BB55_30:
	.pragma "nounroll";
	add.s32 	%r128, %r70, %r317;
	cvt.u64.u32 	%rd18, %r128;
	add.s64 	%rd19, %rd7, %rd18;
	shl.b64 	%rd20, %rd19, 2;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f32 	%f31, [%rd21];
	setp.lt.s32 	%p6, %r332, 0;
	setp.gt.ftz.f32 	%p7, %f31, %f169;
	or.pred  	%p8, %p6, %p7;
	selp.f32 	%f33, %f31, %f169, %p8;
	selp.b32 	%r129, %r322, %r332, %p8;
	add.s32 	%r130, %r322, 1;
	add.s32 	%r131, %r128, 1;
	cvt.u64.u32 	%rd22, %r131;
	add.s64 	%rd23, %rd7, %rd22;
	shl.b64 	%rd24, %rd23, 2;
	add.s64 	%rd25, %rd1, %rd24;
	ld.global.nc.f32 	%f35, [%rd25];
	setp.lt.s32 	%p9, %r129, 0;
	setp.gt.ftz.f32 	%p10, %f35, %f33;
	or.pred  	%p11, %p9, %p10;
	selp.f32 	%f37, %f35, %f33, %p11;
	selp.b32 	%r132, %r130, %r129, %p11;
	add.s32 	%r133, %r322, 2;
	add.s32 	%r134, %r128, 2;
	cvt.u64.u32 	%rd26, %r134;
	add.s64 	%rd27, %rd7, %rd26;
	shl.b64 	%rd28, %rd27, 2;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.nc.f32 	%f39, [%rd29];
	setp.lt.s32 	%p12, %r132, 0;
	setp.gt.ftz.f32 	%p13, %f39, %f37;
	or.pred  	%p14, %p12, %p13;
	selp.f32 	%f41, %f39, %f37, %p14;
	selp.b32 	%r135, %r133, %r132, %p14;
	add.s32 	%r136, %r322, 3;
	add.s32 	%r137, %r128, 3;
	cvt.u64.u32 	%rd30, %r137;
	add.s64 	%rd31, %rd7, %rd30;
	shl.b64 	%rd32, %rd31, 2;
	add.s64 	%rd33, %rd1, %rd32;
	ld.global.nc.f32 	%f43, [%rd33];
	setp.lt.s32 	%p15, %r135, 0;
	setp.gt.ftz.f32 	%p16, %f43, %f41;
	or.pred  	%p17, %p15, %p16;
	selp.f32 	%f45, %f43, %f41, %p17;
	selp.b32 	%r138, %r136, %r135, %p17;
	add.s32 	%r139, %r322, 4;
	add.s32 	%r140, %r128, 4;
	cvt.u64.u32 	%rd34, %r140;
	add.s64 	%rd35, %rd7, %rd34;
	shl.b64 	%rd36, %rd35, 2;
	add.s64 	%rd37, %rd1, %rd36;
	ld.global.nc.f32 	%f47, [%rd37];
	setp.lt.s32 	%p18, %r138, 0;
	setp.gt.ftz.f32 	%p19, %f47, %f45;
	or.pred  	%p20, %p18, %p19;
	selp.f32 	%f49, %f47, %f45, %p20;
	selp.b32 	%r141, %r139, %r138, %p20;
	add.s32 	%r142, %r322, 5;
	add.s32 	%r143, %r128, 5;
	cvt.u64.u32 	%rd38, %r143;
	add.s64 	%rd39, %rd7, %rd38;
	shl.b64 	%rd40, %rd39, 2;
	add.s64 	%rd41, %rd1, %rd40;
	ld.global.nc.f32 	%f51, [%rd41];
	setp.lt.s32 	%p21, %r141, 0;
	setp.gt.ftz.f32 	%p22, %f51, %f49;
	or.pred  	%p23, %p21, %p22;
	selp.f32 	%f53, %f51, %f49, %p23;
	selp.b32 	%r144, %r142, %r141, %p23;
	add.s32 	%r145, %r322, 6;
	add.s32 	%r146, %r128, 6;
	cvt.u64.u32 	%rd42, %r146;
	add.s64 	%rd43, %rd7, %rd42;
	shl.b64 	%rd44, %rd43, 2;
	add.s64 	%rd45, %rd1, %rd44;
	ld.global.nc.f32 	%f55, [%rd45];
	setp.lt.s32 	%p24, %r144, 0;
	setp.gt.ftz.f32 	%p25, %f55, %f53;
	or.pred  	%p26, %p24, %p25;
	selp.f32 	%f57, %f55, %f53, %p26;
	selp.b32 	%r147, %r145, %r144, %p26;
	add.s32 	%r148, %r322, 7;
	add.s32 	%r149, %r128, 7;
	cvt.u64.u32 	%rd46, %r149;
	add.s64 	%rd47, %rd7, %rd46;
	shl.b64 	%rd48, %rd47, 2;
	add.s64 	%rd49, %rd1, %rd48;
	ld.global.nc.f32 	%f59, [%rd49];
	setp.lt.s32 	%p27, %r147, 0;
	setp.gt.ftz.f32 	%p28, %f59, %f57;
	or.pred  	%p29, %p27, %p28;
	selp.f32 	%f169, %f59, %f57, %p29;
	selp.b32 	%r332, %r148, %r147, %p29;
	add.s32 	%r322, %r322, 8;
	add.s32 	%r317, %r317, 8;
	setp.ne.s32 	%p30, %r317, %r61;
	mov.u32 	%r325, %r61;
	@%p30 bra 	$L__BB55_30;
$L__BB55_31:
	setp.eq.s32 	%p31, %r60, 0;
	@%p31 bra 	$L__BB55_39;
	add.s32 	%r151, %r60, -1;
	setp.lt.u32 	%p32, %r151, 3;
	@%p32 bra 	$L__BB55_34;
	add.s32 	%r152, %r70, %r325;
	cvt.u64.u32 	%rd50, %r152;
	add.s64 	%rd51, %rd7, %rd50;
	shl.b64 	%rd52, %rd51, 2;
	add.s64 	%rd53, %rd1, %rd52;
	ld.global.nc.f32 	%f62, [%rd53];
	setp.lt.s32 	%p33, %r332, 0;
	setp.gt.ftz.f32 	%p34, %f62, %f169;
	or.pred  	%p35, %p33, %p34;
	selp.f32 	%f64, %f62, %f169, %p35;
	selp.b32 	%r153, %r322, %r332, %p35;
	add.s32 	%r154, %r322, 1;
	add.s32 	%r155, %r152, 1;
	cvt.u64.u32 	%rd54, %r155;
	add.s64 	%rd55, %rd7, %rd54;
	shl.b64 	%rd56, %rd55, 2;
	add.s64 	%rd57, %rd1, %rd56;
	ld.global.nc.f32 	%f66, [%rd57];
	setp.lt.s32 	%p36, %r153, 0;
	setp.gt.ftz.f32 	%p37, %f66, %f64;
	or.pred  	%p38, %p36, %p37;
	selp.f32 	%f68, %f66, %f64, %p38;
	selp.b32 	%r156, %r154, %r153, %p38;
	add.s32 	%r157, %r322, 2;
	add.s32 	%r158, %r152, 2;
	cvt.u64.u32 	%rd58, %r158;
	add.s64 	%rd59, %rd7, %rd58;
	shl.b64 	%rd60, %rd59, 2;
	add.s64 	%rd61, %rd1, %rd60;
	ld.global.nc.f32 	%f70, [%rd61];
	setp.lt.s32 	%p39, %r156, 0;
	setp.gt.ftz.f32 	%p40, %f70, %f68;
	or.pred  	%p41, %p39, %p40;
	selp.f32 	%f72, %f70, %f68, %p41;
	selp.b32 	%r159, %r157, %r156, %p41;
	add.s32 	%r160, %r322, 3;
	add.s32 	%r161, %r152, 3;
	cvt.u64.u32 	%rd62, %r161;
	add.s64 	%rd63, %rd7, %rd62;
	shl.b64 	%rd64, %rd63, 2;
	add.s64 	%rd65, %rd1, %rd64;
	ld.global.nc.f32 	%f74, [%rd65];
	setp.lt.s32 	%p42, %r159, 0;
	setp.gt.ftz.f32 	%p43, %f74, %f72;
	or.pred  	%p44, %p42, %p43;
	selp.f32 	%f169, %f74, %f72, %p44;
	selp.b32 	%r332, %r160, %r159, %p44;
	add.s32 	%r322, %r322, 4;
	add.s32 	%r325, %r325, 4;
$L__BB55_34:
	and.b32  	%r163, %r108, 3;
	setp.eq.s32 	%p45, %r163, 0;
	@%p45 bra 	$L__BB55_39;
	setp.eq.s32 	%p46, %r163, 1;
	@%p46 bra 	$L__BB55_37;
	add.s32 	%r164, %r70, %r325;
	cvt.u64.u32 	%rd66, %r164;
	add.s64 	%rd67, %rd7, %rd66;
	shl.b64 	%rd68, %rd67, 2;
	add.s64 	%rd69, %rd1, %rd68;
	ld.global.nc.f32 	%f77, [%rd69];
	setp.lt.s32 	%p47, %r332, 0;
	setp.gt.ftz.f32 	%p48, %f77, %f169;
	or.pred  	%p49, %p47, %p48;
	selp.f32 	%f79, %f77, %f169, %p49;
	selp.b32 	%r165, %r322, %r332, %p49;
	add.s32 	%r166, %r322, 1;
	add.s32 	%r167, %r164, 1;
	cvt.u64.u32 	%rd70, %r167;
	add.s64 	%rd71, %rd7, %rd70;
	shl.b64 	%rd72, %rd71, 2;
	add.s64 	%rd73, %rd1, %rd72;
	ld.global.nc.f32 	%f81, [%rd73];
	setp.lt.s32 	%p50, %r165, 0;
	setp.gt.ftz.f32 	%p51, %f81, %f79;
	or.pred  	%p52, %p50, %p51;
	selp.f32 	%f169, %f81, %f79, %p52;
	selp.b32 	%r332, %r166, %r165, %p52;
	add.s32 	%r322, %r322, 2;
	add.s32 	%r325, %r325, 2;
$L__BB55_37:
	and.b32  	%r168, %r108, 1;
	setp.eq.b32 	%p53, %r168, 1;
	not.pred 	%p54, %p53;
	@%p54 bra 	$L__BB55_39;
	add.s32 	%r169, %r70, %r325;
	cvt.u64.u32 	%rd74, %r169;
	add.s64 	%rd75, %rd7, %rd74;
	shl.b64 	%rd76, %rd75, 2;
	add.s64 	%rd77, %rd1, %rd76;
	ld.global.nc.f32 	%f83, [%rd77];
	setp.lt.s32 	%p55, %r332, 0;
	setp.gt.ftz.f32 	%p56, %f83, %f169;
	or.pred  	%p57, %p55, %p56;
	selp.f32 	%f169, %f83, %f169, %p57;
	selp.b32 	%r332, %r322, %r332, %p57;
$L__BB55_39:
	ld.param.u32 	%r285, [TensorMaxPool_param_13];
	add.s32 	%r314, %r314, 1;
	setp.ne.s32 	%p58, %r314, %r285;
	add.s32 	%r315, %r108, %r315;
	@%p58 bra 	$L__BB55_28;
	ld.param.u64 	%rd189, [TensorMaxPool_param_3];
	cvt.rn.f32.s32 	%f85, %r332;
	div.u32 	%r170, %r65, %r109;
	div.u32 	%r171, %r66, %r110;
	mad.lo.s32 	%r172, %r170, %r105, %r171;
	cvt.u64.u32 	%rd78, %r172;
	mad.lo.s32 	%r173, %r64, %r104, %r63;
	mul.lo.s32 	%r174, %r3, %r173;
	cvt.u64.u32 	%rd79, %r174;
	add.s64 	%rd80, %rd79, %rd78;
	cvta.to.global.u64 	%rd81, %rd189;
	shl.b64 	%rd82, %rd80, 2;
	add.s64 	%rd83, %rd81, %rd82;
	st.global.f32 	[%rd83], %f85;
	add.s64 	%rd84, %rd2, %rd82;
	st.global.f32 	[%rd84], %f169;
	add.s32 	%r2, %r2, %r5;
	setp.lt.u32 	%p59, %r2, %r100;
	@%p59 bra 	$L__BB55_27;
$L__BB55_41:
	ret;

}
	// .globl	TensorReverseMaxPool
.visible .entry TensorReverseMaxPool(
	.param .u32 TensorReverseMaxPool_param_0,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_1,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_2,
	.param .u64 .ptr .align 1 TensorReverseMaxPool_param_3,
	.param .u32 TensorReverseMaxPool_param_4,
	.param .u32 TensorReverseMaxPool_param_5,
	.param .u32 TensorReverseMaxPool_param_6,
	.param .u32 TensorReverseMaxPool_param_7,
	.param .u32 TensorReverseMaxPool_param_8,
	.param .u32 TensorReverseMaxPool_param_9,
	.param .u32 TensorReverseMaxPool_param_10,
	.param .u32 TensorReverseMaxPool_param_11,
	.param .u32 TensorReverseMaxPool_param_12,
	.param .u32 TensorReverseMaxPool_param_13
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<45>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<20>;

	ld.param.u32 	%r9, [TensorReverseMaxPool_param_0];
	ld.param.u64 	%rd4, [TensorReverseMaxPool_param_1];
	ld.param.u64 	%rd5, [TensorReverseMaxPool_param_2];
	ld.param.u64 	%rd6, [TensorReverseMaxPool_param_3];
	ld.param.u32 	%r10, [TensorReverseMaxPool_param_4];
	ld.param.u32 	%r11, [TensorReverseMaxPool_param_5];
	ld.param.u32 	%r12, [TensorReverseMaxPool_param_6];
	ld.param.u32 	%r13, [TensorReverseMaxPool_param_8];
	ld.param.u32 	%r14, [TensorReverseMaxPool_param_9];
	ld.param.u32 	%r15, [TensorReverseMaxPool_param_11];
	ld.param.u32 	%r16, [TensorReverseMaxPool_param_12];
	ld.param.u32 	%r17, [TensorReverseMaxPool_param_13];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r18, %ctaid.x;
	mov.u32 	%r19, %tid.x;
	mad.lo.s32 	%r44, %r1, %r18, %r19;
	setp.ge.u32 	%p1, %r44, %r9;
	@%p1 bra 	$L__BB56_3;
	mul.lo.s32 	%r3, %r11, %r10;
	mul.lo.s32 	%r20, %r13, %r12;
	mul.lo.s32 	%r4, %r20, %r14;
	mul.lo.s32 	%r5, %r14, %r13;
	mov.u32 	%r21, %nctaid.x;
	mul.lo.s32 	%r6, %r1, %r21;
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
$L__BB56_2:
	div.u32 	%r22, %r44, %r10;
	mul.lo.s32 	%r23, %r22, %r10;
	sub.s32 	%r24, %r44, %r23;
	div.u32 	%r25, %r22, %r11;
	mul.lo.s32 	%r26, %r25, %r11;
	sub.s32 	%r27, %r22, %r26;
	div.u32 	%r28, %r25, %r12;
	mul.lo.s32 	%r29, %r28, %r12;
	sub.s32 	%r30, %r25, %r29;
	mad.lo.s32 	%r31, %r28, %r12, %r30;
	mul.lo.s32 	%r32, %r3, %r31;
	cvt.u64.u32 	%rd7, %r32;
	mul.lo.s32 	%r33, %r4, %r28;
	cvt.u64.u32 	%rd8, %r33;
	mul.lo.s32 	%r34, %r5, %r30;
	cvt.u64.u32 	%rd9, %r34;
	add.s64 	%rd10, %rd8, %rd9;
	mad.lo.s32 	%r35, %r27, %r10, %r24;
	cvt.u64.u32 	%rd11, %r35;
	add.s64 	%rd12, %rd7, %rd11;
	shl.b64 	%rd13, %rd12, 2;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.f32 	%f1, [%rd14];
	add.s64 	%rd15, %rd2, %rd13;
	ld.global.nc.f32 	%f2, [%rd15];
	cvt.rzi.ftz.s32.f32 	%r36, %f2;
	max.s32 	%r37, %r36, 0;
	div.u32 	%r38, %r37, %r15;
	mad.lo.s32 	%r39, %r27, %r16, %r38;
	mul.lo.s32 	%r40, %r38, %r15;
	sub.s32 	%r41, %r37, %r40;
	mad.lo.s32 	%r42, %r24, %r17, %r41;
	mad.lo.s32 	%r43, %r39, %r13, %r42;
	cvt.u64.u32 	%rd16, %r43;
	add.s64 	%rd17, %rd10, %rd16;
	shl.b64 	%rd18, %rd17, 2;
	add.s64 	%rd19, %rd3, %rd18;
	st.global.f32 	[%rd19], %f1;
	add.s32 	%r44, %r44, %r6;
	setp.lt.u32 	%p2, %r44, %r9;
	@%p2 bra 	$L__BB56_2;
$L__BB56_3:
	ret;

}
	// .globl	CalculateMultiDistances
.visible .entry CalculateMultiDistances(
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_0,
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_1,
	.param .u64 .ptr .align 1 CalculateMultiDistances_param_2,
	.param .u32 CalculateMultiDistances_param_3,
	.param .u32 CalculateMultiDistances_param_4,
	.param .u32 CalculateMultiDistances_param_5,
	.param .u32 CalculateMultiDistances_param_6
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<53>;
	.reg .f32 	%f<12>;
	.reg .b64 	%rd<34>;

	ld.param.u64 	%rd7, [CalculateMultiDistances_param_0];
	ld.param.u64 	%rd8, [CalculateMultiDistances_param_1];
	ld.param.u64 	%rd9, [CalculateMultiDistances_param_2];
	ld.param.u32 	%r28, [CalculateMultiDistances_param_3];
	ld.param.u32 	%r29, [CalculateMultiDistances_param_4];
	ld.param.u32 	%r30, [CalculateMultiDistances_param_5];
	ld.param.u32 	%r31, [CalculateMultiDistances_param_6];
	cvta.to.global.u64 	%rd1, %rd9;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd7;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r32, %ctaid.x;
	mov.u32 	%r33, %tid.x;
	mad.lo.s32 	%r45, %r1, %r32, %r33;
	setp.ge.u32 	%p1, %r45, %r30;
	@%p1 bra 	$L__BB57_18;
	mov.u32 	%r34, %ntid.y;
	mov.u32 	%r35, %ctaid.y;
	mov.u32 	%r36, %tid.y;
	mad.lo.s32 	%r3, %r34, %r35, %r36;
	mov.u32 	%r37, %ntid.z;
	mov.u32 	%r38, %ctaid.z;
	mov.u32 	%r39, %tid.z;
	mad.lo.s32 	%r4, %r37, %r38, %r39;
	mov.u32 	%r40, %nctaid.z;
	mul.lo.s32 	%r5, %r37, %r40;
	mov.u32 	%r41, %nctaid.y;
	mul.lo.s32 	%r6, %r34, %r41;
	mov.u32 	%r42, %nctaid.x;
	mul.lo.s32 	%r7, %r1, %r42;
$L__BB57_2:
	setp.ge.u32 	%p2, %r3, %r29;
	@%p2 bra 	$L__BB57_17;
	cvt.u64.u32 	%rd4, %r45;
	setp.eq.s32 	%p3, %r31, 0;
	mov.u32 	%r46, %r3;
	@%p3 bra 	$L__BB57_9;
	setp.ne.s32 	%p4, %r31, 2;
	mov.u32 	%r48, %r3;
	mov.u32 	%r51, %r3;
	@%p4 bra 	$L__BB57_13;
$L__BB57_5:
	setp.ge.u32 	%p5, %r4, %r28;
	@%p5 bra 	$L__BB57_8;
	mul.wide.u32 	%rd10, %r48, 8;
	add.s64 	%rd11, %rd3, %rd10;
	ld.global.nc.u64 	%rd12, [%rd11];
	shl.b64 	%rd13, %rd4, 2;
	add.s64 	%rd6, %rd12, %rd13;
	mad.lo.s32 	%r49, %r48, %r28, %r4;
	mov.u32 	%r50, %r4;
$L__BB57_7:
	mul.wide.u32 	%rd14, %r50, 8;
	add.s64 	%rd15, %rd2, %rd14;
	ld.global.nc.u64 	%rd16, [%rd15];
	add.s64 	%rd18, %rd16, %rd13;
	ld.f32 	%f1, [%rd18];
	ld.f32 	%f2, [%rd6];
	sub.ftz.f32 	%f3, %f2, %f1;
	abs.ftz.f32 	%f4, %f3;
	mul.wide.u32 	%rd19, %r49, 4;
	add.s64 	%rd20, %rd1, %rd19;
	atom.global.add.f32 	%f5, [%rd20], %f4;
	add.s32 	%r50, %r50, %r5;
	add.s32 	%r49, %r49, %r5;
	setp.lt.u32 	%p6, %r50, %r28;
	@%p6 bra 	$L__BB57_7;
$L__BB57_8:
	add.s32 	%r48, %r48, %r6;
	setp.lt.u32 	%p7, %r48, %r29;
	@%p7 bra 	$L__BB57_5;
	bra.uni 	$L__BB57_17;
$L__BB57_9:
	setp.ge.u32 	%p8, %r4, %r28;
	@%p8 bra 	$L__BB57_12;
	mul.wide.u32 	%rd21, %r46, 8;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.nc.u64 	%rd23, [%rd22];
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd5, %rd23, %rd24;
	mul.lo.s32 	%r11, %r46, %r28;
	mov.u32 	%r47, %r4;
$L__BB57_11:
	mul.wide.u32 	%rd25, %r47, 8;
	add.s64 	%rd26, %rd2, %rd25;
	ld.global.nc.u64 	%rd27, [%rd26];
	add.s64 	%rd29, %rd27, %rd24;
	ld.f32 	%f6, [%rd29];
	ld.f32 	%f7, [%rd5];
	sub.ftz.f32 	%f8, %f7, %f6;
	mul.ftz.f32 	%f9, %f8, %f8;
	add.s32 	%r43, %r47, %r11;
	mul.wide.u32 	%rd30, %r43, 4;
	add.s64 	%rd31, %rd1, %rd30;
	atom.global.add.f32 	%f10, [%rd31], %f9;
	add.s32 	%r47, %r47, %r5;
	setp.lt.u32 	%p9, %r47, %r28;
	@%p9 bra 	$L__BB57_11;
$L__BB57_12:
	add.s32 	%r46, %r46, %r6;
	setp.lt.u32 	%p10, %r46, %r29;
	@%p10 bra 	$L__BB57_9;
	bra.uni 	$L__BB57_17;
$L__BB57_13:
	setp.ge.u32 	%p11, %r4, %r28;
	@%p11 bra 	$L__BB57_16;
	mul.lo.s32 	%r23, %r51, %r28;
	mov.u32 	%r52, %r4;
$L__BB57_15:
	add.s32 	%r44, %r52, %r23;
	mul.wide.u32 	%rd32, %r44, 4;
	add.s64 	%rd33, %rd1, %rd32;
	atom.global.add.f32 	%f11, [%rd33], 0f00000000;
	add.s32 	%r52, %r52, %r5;
	setp.lt.u32 	%p12, %r52, %r28;
	@%p12 bra 	$L__BB57_15;
$L__BB57_16:
	add.s32 	%r51, %r51, %r6;
	setp.lt.u32 	%p13, %r51, %r29;
	@%p13 bra 	$L__BB57_13;
$L__BB57_17:
	add.s32 	%r45, %r45, %r7;
	setp.lt.u32 	%p14, %r45, %r30;
	@%p14 bra 	$L__BB57_2;
$L__BB57_18:
	ret;

}
	// .globl	CalculateDistances
.visible .entry CalculateDistances(
	.param .u64 .ptr .align 1 CalculateDistances_param_0,
	.param .u64 .ptr .align 1 CalculateDistances_param_1,
	.param .u64 .ptr .align 1 CalculateDistances_param_2,
	.param .u32 CalculateDistances_param_3,
	.param .u32 CalculateDistances_param_4,
	.param .u32 CalculateDistances_param_5
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<35>;
	.reg .f32 	%f<12>;
	.reg .b64 	%rd<29>;

	ld.param.u64 	%rd6, [CalculateDistances_param_0];
	ld.param.u64 	%rd7, [CalculateDistances_param_1];
	ld.param.u64 	%rd8, [CalculateDistances_param_2];
	ld.param.u32 	%r18, [CalculateDistances_param_3];
	ld.param.u32 	%r19, [CalculateDistances_param_4];
	ld.param.u32 	%r20, [CalculateDistances_param_5];
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd7;
	cvta.to.global.u64 	%rd3, %rd6;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r21, %ctaid.x;
	mov.u32 	%r22, %tid.x;
	mad.lo.s32 	%r31, %r1, %r21, %r22;
	setp.ge.u32 	%p1, %r31, %r19;
	@%p1 bra 	$L__BB58_15;
	mov.u32 	%r24, %ntid.y;
	mov.u32 	%r25, %ctaid.y;
	mov.u32 	%r26, %tid.y;
	mad.lo.s32 	%r3, %r24, %r25, %r26;
	mov.u32 	%r27, %nctaid.y;
	mul.lo.s32 	%r4, %r24, %r27;
	mov.u32 	%r28, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r28;
	setp.eq.s32 	%p2, %r20, 0;
	@%p2 bra 	$L__BB58_7;
	setp.ne.s32 	%p3, %r20, 2;
	@%p3 bra 	$L__BB58_11;
$L__BB58_3:
	setp.ge.u32 	%p4, %r3, %r18;
	@%p4 bra 	$L__BB58_6;
	cvt.u64.u32 	%rd5, %r31;
	mul.wide.u32 	%rd9, %r31, 4;
	add.s64 	%rd10, %rd3, %rd9;
	ld.global.nc.f32 	%f2, [%rd10];
	mov.u32 	%r32, %r3;
$L__BB58_5:
	mul.wide.u32 	%rd11, %r32, 8;
	add.s64 	%rd12, %rd2, %rd11;
	ld.global.nc.u64 	%rd13, [%rd12];
	shl.b64 	%rd14, %rd5, 2;
	add.s64 	%rd15, %rd13, %rd14;
	ld.f32 	%f3, [%rd15];
	sub.ftz.f32 	%f4, %f2, %f3;
	abs.ftz.f32 	%f5, %f4;
	mul.wide.u32 	%rd16, %r32, 4;
	add.s64 	%rd17, %rd1, %rd16;
	atom.global.add.f32 	%f6, [%rd17], %f5;
	add.s32 	%r32, %r32, %r4;
	setp.lt.u32 	%p5, %r32, %r18;
	@%p5 bra 	$L__BB58_5;
$L__BB58_6:
	add.s32 	%r31, %r31, %r5;
	setp.lt.u32 	%p6, %r31, %r19;
	@%p6 bra 	$L__BB58_3;
	bra.uni 	$L__BB58_15;
$L__BB58_7:
	setp.ge.u32 	%p7, %r3, %r18;
	@%p7 bra 	$L__BB58_10;
	cvt.u64.u32 	%rd4, %r31;
	mul.wide.u32 	%rd18, %r31, 4;
	add.s64 	%rd19, %rd3, %rd18;
	ld.global.nc.f32 	%f1, [%rd19];
	mov.u32 	%r30, %r3;
$L__BB58_9:
	mul.wide.u32 	%rd20, %r30, 8;
	add.s64 	%rd21, %rd2, %rd20;
	ld.global.nc.u64 	%rd22, [%rd21];
	shl.b64 	%rd23, %rd4, 2;
	add.s64 	%rd24, %rd22, %rd23;
	ld.f32 	%f7, [%rd24];
	sub.ftz.f32 	%f8, %f1, %f7;
	mul.ftz.f32 	%f9, %f8, %f8;
	mul.wide.u32 	%rd25, %r30, 4;
	add.s64 	%rd26, %rd1, %rd25;
	atom.global.add.f32 	%f10, [%rd26], %f9;
	add.s32 	%r30, %r30, %r4;
	setp.lt.u32 	%p8, %r30, %r18;
	@%p8 bra 	$L__BB58_9;
$L__BB58_10:
	add.s32 	%r31, %r31, %r5;
	setp.lt.u32 	%p9, %r31, %r19;
	@%p9 bra 	$L__BB58_7;
	bra.uni 	$L__BB58_15;
$L__BB58_11:
	setp.ge.u32 	%p10, %r3, %r18;
	@%p10 bra 	$L__BB58_14;
	mov.u32 	%r34, %r3;
$L__BB58_13:
	mul.wide.u32 	%rd27, %r34, 4;
	add.s64 	%rd28, %rd1, %rd27;
	atom.global.add.f32 	%f11, [%rd28], 0f00000000;
	add.s32 	%r34, %r34, %r4;
	setp.lt.u32 	%p11, %r34, %r18;
	@%p11 bra 	$L__BB58_13;
$L__BB58_14:
	add.s32 	%r31, %r31, %r5;
	setp.lt.u32 	%p12, %r31, %r19;
	@%p12 bra 	$L__BB58_11;
$L__BB58_15:
	ret;

}
	// .globl	CosineMultiDistance
.visible .entry CosineMultiDistance(
	.param .u64 .ptr .align 1 CosineMultiDistance_param_0,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_1,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_2,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_3,
	.param .u64 .ptr .align 1 CosineMultiDistance_param_4,
	.param .u32 CosineMultiDistance_param_5,
	.param .u32 CosineMultiDistance_param_6,
	.param .u32 CosineMultiDistance_param_7
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<35>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<27>;

	ld.param.u64 	%rd8, [CosineMultiDistance_param_1];
	ld.param.u64 	%rd9, [CosineMultiDistance_param_2];
	ld.param.u64 	%rd10, [CosineMultiDistance_param_3];
	ld.param.u64 	%rd11, [CosineMultiDistance_param_4];
	ld.param.u32 	%r17, [CosineMultiDistance_param_5];
	ld.param.u32 	%r18, [CosineMultiDistance_param_6];
	ld.param.u32 	%r19, [CosineMultiDistance_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r20, %ctaid.x;
	mov.u32 	%r21, %tid.x;
	mad.lo.s32 	%r31, %r1, %r20, %r21;
	setp.ge.u32 	%p1, %r31, %r19;
	@%p1 bra 	$L__BB59_9;
	mov.u32 	%r22, %ntid.y;
	mov.u32 	%r23, %ctaid.y;
	mov.u32 	%r24, %tid.y;
	mad.lo.s32 	%r3, %r22, %r23, %r24;
	mov.u32 	%r25, %ntid.z;
	mov.u32 	%r26, %ctaid.z;
	mov.u32 	%r27, %tid.z;
	mad.lo.s32 	%r4, %r25, %r26, %r27;
	mov.u32 	%r28, %nctaid.z;
	mul.lo.s32 	%r5, %r25, %r28;
	mov.u32 	%r29, %nctaid.y;
	mul.lo.s32 	%r6, %r22, %r29;
	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r7, %r1, %r30;
	cvta.to.global.u64 	%rd1, %rd8;
	cvta.to.global.u64 	%rd2, %rd9;
	cvta.to.global.u64 	%rd3, %rd10;
	cvta.to.global.u64 	%rd4, %rd11;
$L__BB59_2:
	setp.ge.u32 	%p2, %r3, %r18;
	@%p2 bra 	$L__BB59_8;
	cvt.u64.u32 	%rd5, %r31;
	shl.b64 	%rd16, %rd5, 2;
	mov.u32 	%r32, %r3;
$L__BB59_4:
	setp.ge.u32 	%p3, %r4, %r17;
	@%p3 bra 	$L__BB59_7;
	ld.param.u64 	%rd26, [CosineMultiDistance_param_0];
	cvta.to.global.u64 	%rd12, %rd26;
	mul.wide.u32 	%rd13, %r32, 8;
	add.s64 	%rd14, %rd12, %rd13;
	ld.global.nc.u64 	%rd15, [%rd14];
	add.s64 	%rd6, %rd15, %rd16;
	mad.lo.s32 	%r33, %r32, %r17, %r4;
	mov.u32 	%r34, %r4;
$L__BB59_6:
	ld.f32 	%f1, [%rd6];
	mul.wide.u32 	%rd17, %r34, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.u64 	%rd19, [%rd18];
	add.s64 	%rd21, %rd19, %rd16;
	ld.f32 	%f2, [%rd21];
	mul.wide.u32 	%rd22, %r33, 4;
	add.s64 	%rd23, %rd2, %rd22;
	mul.ftz.f32 	%f3, %f1, %f1;
	atom.global.add.f32 	%f4, [%rd23], %f3;
	add.s64 	%rd24, %rd3, %rd22;
	mul.ftz.f32 	%f5, %f1, %f2;
	atom.global.add.f32 	%f6, [%rd24], %f5;
	add.s64 	%rd25, %rd4, %rd22;
	mul.ftz.f32 	%f7, %f2, %f2;
	atom.global.add.f32 	%f8, [%rd25], %f7;
	add.s32 	%r34, %r34, %r5;
	add.s32 	%r33, %r33, %r5;
	setp.lt.u32 	%p4, %r34, %r17;
	@%p4 bra 	$L__BB59_6;
$L__BB59_7:
	add.s32 	%r32, %r32, %r6;
	setp.lt.u32 	%p5, %r32, %r18;
	@%p5 bra 	$L__BB59_4;
$L__BB59_8:
	add.s32 	%r31, %r31, %r7;
	setp.lt.u32 	%p6, %r31, %r19;
	@%p6 bra 	$L__BB59_2;
$L__BB59_9:
	ret;

}
	// .globl	CosineDistances
.visible .entry CosineDistances(
	.param .u64 .ptr .align 1 CosineDistances_param_0,
	.param .u64 .ptr .align 1 CosineDistances_param_1,
	.param .u64 .ptr .align 1 CosineDistances_param_2,
	.param .u64 .ptr .align 1 CosineDistances_param_3,
	.param .u64 .ptr .align 1 CosineDistances_param_4,
	.param .u32 CosineDistances_param_5,
	.param .u32 CosineDistances_param_6
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<21>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<23>;

	ld.param.u64 	%rd7, [CosineDistances_param_0];
	ld.param.u64 	%rd8, [CosineDistances_param_1];
	ld.param.u64 	%rd9, [CosineDistances_param_2];
	ld.param.u64 	%rd10, [CosineDistances_param_3];
	ld.param.u64 	%rd11, [CosineDistances_param_4];
	ld.param.u32 	%r10, [CosineDistances_param_5];
	ld.param.u32 	%r11, [CosineDistances_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r12, %ctaid.x;
	mov.u32 	%r13, %tid.x;
	mad.lo.s32 	%r19, %r1, %r12, %r13;
	setp.ge.u32 	%p1, %r19, %r11;
	@%p1 bra 	$L__BB60_6;
	mov.u32 	%r14, %ntid.y;
	mov.u32 	%r15, %ctaid.y;
	mov.u32 	%r16, %tid.y;
	mad.lo.s32 	%r3, %r14, %r15, %r16;
	mov.u32 	%r17, %nctaid.y;
	mul.lo.s32 	%r4, %r14, %r17;
	mov.u32 	%r18, %nctaid.x;
	mul.lo.s32 	%r5, %r1, %r18;
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd2, %rd8;
	cvta.to.global.u64 	%rd3, %rd9;
	cvta.to.global.u64 	%rd4, %rd10;
	cvta.to.global.u64 	%rd5, %rd11;
$L__BB60_2:
	setp.ge.u32 	%p2, %r3, %r10;
	@%p2 bra 	$L__BB60_5;
	cvt.u64.u32 	%rd6, %r19;
	mul.wide.u32 	%rd12, %r19, 4;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.f32 	%f1, [%rd13];
	mul.ftz.f32 	%f2, %f1, %f1;
	shl.b64 	%rd17, %rd6, 2;
	mov.u32 	%r20, %r3;
$L__BB60_4:
	mul.wide.u32 	%rd14, %r20, 8;
	add.s64 	%rd15, %rd2, %rd14;
	ld.global.nc.u64 	%rd16, [%rd15];
	add.s64 	%rd18, %rd16, %rd17;
	ld.f32 	%f3, [%rd18];
	mul.wide.u32 	%rd19, %r20, 4;
	add.s64 	%rd20, %rd3, %rd19;
	atom.global.add.f32 	%f4, [%rd20], %f2;
	add.s64 	%rd21, %rd4, %rd19;
	mul.ftz.f32 	%f5, %f1, %f3;
	atom.global.add.f32 	%f6, [%rd21], %f5;
	add.s64 	%rd22, %rd5, %rd19;
	mul.ftz.f32 	%f7, %f3, %f3;
	atom.global.add.f32 	%f8, [%rd22], %f7;
	add.s32 	%r20, %r20, %r4;
	setp.lt.u32 	%p3, %r20, %r10;
	@%p3 bra 	$L__BB60_4;
$L__BB60_5:
	add.s32 	%r19, %r19, %r5;
	setp.lt.u32 	%p4, %r19, %r11;
	@%p4 bra 	$L__BB60_2;
$L__BB60_6:
	ret;

}
	// .globl	SumValues
.visible .entry SumValues(
	.param .u64 .ptr .align 1 SumValues_param_0,
	.param .u32 SumValues_param_1,
	.param .u64 .ptr .align 1 SumValues_param_2,
	.param .u32 SumValues_param_3
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<56>;
	.reg .f32 	%f<84>;
	.reg .b64 	%rd<9>;
	// demoted variable
	.shared .align 4 .b8 _ZZ9SumValuesE5block[4096];
	ld.param.u64 	%rd1, [SumValues_param_0];
	ld.param.u32 	%r27, [SumValues_param_1];
	ld.param.u64 	%rd2, [SumValues_param_2];
	ld.param.u32 	%r28, [SumValues_param_3];
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r29, %ntid.x;
	mad.lo.s32 	%r3, %r29, %r2, %r1;
	setp.ge.u32 	%p1, %r3, %r27;
	@%p1 bra 	$L__BB61_2;
	cvta.to.global.u64 	%rd3, %rd1;
	mul.lo.s32 	%r30, %r28, %r3;
	mul.wide.u32 	%rd4, %r30, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.nc.f32 	%f14, [%rd5];
	shl.b32 	%r31, %r1, 2;
	mov.u32 	%r32, _ZZ9SumValuesE5block;
	add.s32 	%r33, %r32, %r31;
	st.shared.f32 	[%r33], %f14;
$L__BB61_2:
	bar.sync 	0;
	setp.ne.s32 	%p2, %r1, 0;
	@%p2 bra 	$L__BB61_17;
	shl.b32 	%r34, %r2, 10;
	sub.s32 	%r4, %r27, %r34;
	setp.eq.s32 	%p3, %r4, 0;
	mov.f32 	%f83, 0f00000000;
	@%p3 bra 	$L__BB61_16;
	min.u32 	%r5, %r4, 1024;
	and.b32  	%r6, %r5, 15;
	setp.lt.u32 	%p4, %r4, 16;
	mov.f32 	%f83, 0f00000000;
	mov.b32 	%r51, 0;
	@%p4 bra 	$L__BB61_7;
	mov.u32 	%r38, _ZZ9SumValuesE5block;
	add.s32 	%r48, %r38, 32;
	and.b32  	%r39, %r5, 2032;
	neg.s32 	%r49, %r39;
	mov.f32 	%f83, 0f00000000;
	mov.b32 	%r51, 0;
$L__BB61_6:
	.pragma "nounroll";
	ld.shared.f32 	%f19, [%r48+-32];
	add.ftz.f32 	%f20, %f83, %f19;
	ld.shared.f32 	%f21, [%r48+-28];
	add.ftz.f32 	%f22, %f20, %f21;
	ld.shared.f32 	%f23, [%r48+-24];
	add.ftz.f32 	%f24, %f22, %f23;
	ld.shared.f32 	%f25, [%r48+-20];
	add.ftz.f32 	%f26, %f24, %f25;
	ld.shared.f32 	%f27, [%r48+-16];
	add.ftz.f32 	%f28, %f26, %f27;
	ld.shared.f32 	%f29, [%r48+-12];
	add.ftz.f32 	%f30, %f28, %f29;
	ld.shared.f32 	%f31, [%r48+-8];
	add.ftz.f32 	%f32, %f30, %f31;
	ld.shared.f32 	%f33, [%r48+-4];
	add.ftz.f32 	%f34, %f32, %f33;
	ld.shared.f32 	%f35, [%r48];
	add.ftz.f32 	%f36, %f34, %f35;
	ld.shared.f32 	%f37, [%r48+4];
	add.ftz.f32 	%f38, %f36, %f37;
	ld.shared.f32 	%f39, [%r48+8];
	add.ftz.f32 	%f40, %f38, %f39;
	ld.shared.f32 	%f41, [%r48+12];
	add.ftz.f32 	%f42, %f40, %f41;
	ld.shared.f32 	%f43, [%r48+16];
	add.ftz.f32 	%f44, %f42, %f43;
	ld.shared.f32 	%f45, [%r48+20];
	add.ftz.f32 	%f46, %f44, %f45;
	ld.shared.f32 	%f47, [%r48+24];
	add.ftz.f32 	%f48, %f46, %f47;
	ld.shared.f32 	%f49, [%r48+28];
	add.ftz.f32 	%f83, %f48, %f49;
	add.s32 	%r51, %r51, 16;
	add.s32 	%r49, %r49, 16;
	add.s32 	%r48, %r48, 64;
	setp.ne.s32 	%p5, %r49, 0;
	@%p5 bra 	$L__BB61_6;
$L__BB61_7:
	setp.eq.s32 	%p6, %r6, 0;
	@%p6 bra 	$L__BB61_16;
	and.b32  	%r15, %r5, 7;
	setp.lt.u32 	%p7, %r6, 8;
	@%p7 bra 	$L__BB61_10;
	shl.b32 	%r40, %r51, 2;
	mov.u32 	%r41, _ZZ9SumValuesE5block;
	add.s32 	%r42, %r41, %r40;
	ld.shared.f32 	%f51, [%r42];
	add.ftz.f32 	%f52, %f83, %f51;
	ld.shared.f32 	%f53, [%r42+4];
	add.ftz.f32 	%f54, %f52, %f53;
	ld.shared.f32 	%f55, [%r42+8];
	add.ftz.f32 	%f56, %f54, %f55;
	ld.shared.f32 	%f57, [%r42+12];
	add.ftz.f32 	%f58, %f56, %f57;
	ld.shared.f32 	%f59, [%r42+16];
	add.ftz.f32 	%f60, %f58, %f59;
	ld.shared.f32 	%f61, [%r42+20];
	add.ftz.f32 	%f62, %f60, %f61;
	ld.shared.f32 	%f63, [%r42+24];
	add.ftz.f32 	%f64, %f62, %f63;
	ld.shared.f32 	%f65, [%r42+28];
	add.ftz.f32 	%f83, %f64, %f65;
	add.s32 	%r51, %r51, 8;
$L__BB61_10:
	setp.eq.s32 	%p8, %r15, 0;
	@%p8 bra 	$L__BB61_16;
	and.b32  	%r18, %r5, 3;
	setp.lt.u32 	%p9, %r15, 4;
	@%p9 bra 	$L__BB61_13;
	shl.b32 	%r43, %r51, 2;
	mov.u32 	%r44, _ZZ9SumValuesE5block;
	add.s32 	%r45, %r44, %r43;
	ld.shared.f32 	%f67, [%r45];
	add.ftz.f32 	%f68, %f83, %f67;
	ld.shared.f32 	%f69, [%r45+4];
	add.ftz.f32 	%f70, %f68, %f69;
	ld.shared.f32 	%f71, [%r45+8];
	add.ftz.f32 	%f72, %f70, %f71;
	ld.shared.f32 	%f73, [%r45+12];
	add.ftz.f32 	%f83, %f72, %f73;
	add.s32 	%r51, %r51, 4;
$L__BB61_13:
	setp.eq.s32 	%p10, %r18, 0;
	@%p10 bra 	$L__BB61_16;
	shl.b32 	%r46, %r51, 2;
	mov.u32 	%r47, _ZZ9SumValuesE5block;
	add.s32 	%r55, %r47, %r46;
	neg.s32 	%r54, %r18;
$L__BB61_15:
	.pragma "nounroll";
	ld.shared.f32 	%f74, [%r55];
	add.ftz.f32 	%f83, %f83, %f74;
	add.s32 	%r55, %r55, 4;
	add.s32 	%r54, %r54, 1;
	setp.ne.s32 	%p11, %r54, 0;
	@%p11 bra 	$L__BB61_15;
$L__BB61_16:
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r2, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f83;
$L__BB61_17:
	ret;

}
